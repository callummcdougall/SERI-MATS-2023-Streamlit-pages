{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big question for this project - how often does it happen that 10.7 doesn't do copy suppression, in situations where we think it should have?\n",
    "\n",
    "I could do a scatter plot, but for now I'll just do a simple confusion plot.\n",
    "\n",
    "What are the conditions for when we think copy-suppression should take place?\n",
    "\n",
    "1. The unembedding for a certain source token is larger than a certain threshold (so that it turns off attention to BOS)\n",
    "2. The unembdding for this source token is larger than the unembeddings for all other source tokens (so that it takes all the attention)\n",
    "\n",
    "There's only one hyperparameter here - `X`, the threshold.\n",
    "\n",
    "I'll split points by whether this is true or not, and I'll also split points by whether they're attended to more than anything else (including BOS). I'll create a table of this. I hope that the table won't have many diagonal elements.\n",
    "\n",
    "*(Note - when I say \"BOS\" in this document, that's a shorthand for the token at position zero, since the head doesn't seem to care what its identity is.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pattern --no-dependencies\n",
    "# %pip install nltk\n",
    "# %pip install protobuf==3.20.0\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens.rs.callum2.cspa.cspa_functions import (\n",
    "    FUNCTION_STR_TOKS,\n",
    "    # project,\n",
    "    get_cspa_results,\n",
    "    get_cspa_results_batched,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    parse_str,\n",
    "    parse_str_toks_for_printing,\n",
    "    process_webtext,\n",
    "    ST_HTML_PATH,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_plots import (\n",
    "    generate_scatter,\n",
    "    generate_loss_based_scatter,\n",
    "    add_cspa_to_streamlit_page,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_result_mean,\n",
    "    get_model_results,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    generate_4_html_plots,\n",
    "    CSS,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_semantic_similarity import (\n",
    "    get_equivalency_toks,\n",
    "    get_related_words,\n",
    "    concat_lists,\n",
    "    make_list_correct_length,\n",
    "    create_full_semantic_similarity_dict,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\",\n",
    "    # fold value bias?\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = torch.Size([80, 61])\n",
      "\n",
      "First prompt:\n",
      "<|endoftext|>Oh boy was this damn hard to crack.\n",
      "\n",
      "Ok, I believe before it was established before that Aperture Science headquarters are in Cleveland, OH.\n",
      "\n",
      "Source: HL2EP2\n",
      "\n",
      "Though, this has been found.\n",
      "\n",
      "Source: Portal 2\n",
      "\n",
      "It can be assumed\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 80 # 80 for viz\n",
    "SEQ_LEN = 61 # 61 for viz\n",
    "batch_idx = 36\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(BATCH_SIZE, SEQ_LEN, model=model, seed=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_semantic_dict = pickle.load(open(ST_HTML_PATH.parent.parent / \"cspa/cspa_semantic_dict_full.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy suppression condition\n",
    "\n",
    "First, I'll ignore semantically related tokens, and just look at raw tokens.\n",
    "\n",
    "Rather than a table, I actually think I'll do 2 violin plots, one for \"should be CS\" and one for \"shouldn't be CS\" (the values in each violin plot are attention). I can do this and a table pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_table(\n",
    "    toks: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    threshold_logit_lens: Optional[float],\n",
    "    threshold_cs_classification: Optional[float],\n",
    "    minibatch_size: Optional[int] = None,\n",
    "    head: Tuple[int, int] = (10, 7),\n",
    "    filter_for_BOS_not_largest: bool = False,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    '''\n",
    "    Do a hooked forward pass on the tokens, getting the attention probs and the logit lens.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        threshold_logit_lens\n",
    "            The logits must be above this threshold for the point to be classified as \"we expect CS here\". This is\n",
    "            interpreted as the value we must be higher than in order to override attn to BOS. If None, we don't apply\n",
    "            any thresholding here (whatever the max logit lens src token is, we expect CS there).\n",
    "\n",
    "        threshold_cs_classification\n",
    "            The attn prob must be above this threshold for the point to be classified as \"CS happening here\". If None,\n",
    "            we don't apply any thresholding here (whatever the max attn prob is, we expect CS there).\n",
    "\n",
    "    '''\n",
    "    layer, head_idx = head\n",
    "    batch_size, seq_len = toks.shape\n",
    "    model.reset_hooks()\n",
    "    FUNCTION_TOKS = model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze()\n",
    "\n",
    "    # Create external storage, which we concatenate to\n",
    "    external_storage = {\n",
    "        \"pattern\": t.empty((0, seq_len, seq_len), device=toks.device, dtype=t.float),\n",
    "        \"logit_lens_is_above_threshold\": t.empty((0, seq_len, seq_len), device=toks.device, dtype=t.bool),\n",
    "    }\n",
    "\n",
    "\n",
    "    # ! First we define hook functions to do most of the work for us\n",
    "\n",
    "    def hook_fn_cache_attn_probs(pattern: Float[Tensor, \"batch nheads seqQ seqK\"], hook: HookPoint):\n",
    "        '''\n",
    "        Caches attention probs for a single head.\n",
    "        '''\n",
    "        external_storage[\"pattern\"] = t.concat([\n",
    "            external_storage[\"pattern\"],\n",
    "            pattern[:, head_idx]\n",
    "        ])\n",
    "\n",
    "\n",
    "    def hook_fn_compute_logit_lens(resid_pre: Float[Tensor, \"batch seqK d_model\"], hook: HookPoint, _toks: Int[Tensor, \"batch seqK\"]):\n",
    "        '''\n",
    "        Computes logit lens at the residual stream before the head, and figures out which (b, sQ, sK) should\n",
    "        have copy suppression activated and which shouldn't.\n",
    "        '''\n",
    "        _batch_size, _seq_len = _toks.shape\n",
    "\n",
    "        logit_lens_for_src = einops.einsum(\n",
    "            resid_pre,\n",
    "            model.W_U.T[_toks],\n",
    "            \"batch seqQ d_model, batch seqK d_model -> batch seqQ seqK\"\n",
    "        )\n",
    "        # logit_lens_for_src[b, sQ, sK] = logits for src token sK at destination position (b, sQ)\n",
    "\n",
    "        # We need to apply causal mask\n",
    "        seqQ = einops.repeat(t.arange(seq_len, device=_toks.device), \"seqQ -> 1 seqQ 1\")\n",
    "        seqK = einops.repeat(t.arange(seq_len, device=_toks.device), \"seqK -> 1 1 seqK\")\n",
    "        logit_lens_for_src.masked_fill_(seqQ < seqK, float(\"-inf\"))\n",
    "        # We mask the attention from any token to BOS, and function words\n",
    "        logit_lens_for_src[:, :, 0] = float(\"-inf\")\n",
    "        is_fn_word = (_toks[:, :, None] == FUNCTION_TOKS[None, None, :]).any(dim=-1)\n",
    "        is_fn_word = einops.repeat(is_fn_word, \"batch seqK -> batch seqQ seqK\", seqQ=_seq_len)\n",
    "        logit_lens_for_src = t.where(is_fn_word, float(\"-inf\"), logit_lens_for_src)\n",
    "        \n",
    "        if threshold_logit_lens is None:\n",
    "            # We also want to mask all but the best tokens (because attention is limited, and we only care about the src\n",
    "            # token we think should be getting the most attention)\n",
    "            top_src_token_logit_values = logit_lens_for_src.max(dim=-1, keepdim=True).values\n",
    "            is_top_src_token = logit_lens_for_src + 1e-8 >= top_src_token_logit_values\n",
    "            logit_lens_for_src.masked_fill_(~is_top_src_token, float(\"-inf\"))\n",
    "            logit_lens_is_above_threshold: Bool[Tensor, \"batch seqQ seqK\"] = logit_lens_for_src > 1e-6\n",
    "        else:\n",
    "            # In this case, we filter for attention being above some value\n",
    "            logit_lens_for_src.masked_fill_(~is_top_src_token, float(\"-inf\"))\n",
    "            logit_lens_is_above_threshold: Bool[Tensor, \"batch seqQ seqK\"] = logit_lens_for_src > threshold_logit_lens\n",
    "\n",
    "        # Finally, we store these indices in our results dict\n",
    "        external_storage[\"logit_lens_is_above_threshold\"] = t.concat([\n",
    "            external_storage[\"logit_lens_is_above_threshold\"],\n",
    "            logit_lens_is_above_threshold\n",
    "        ])\n",
    "\n",
    "    \n",
    "    # ! Next we run a fwd pass, to activate these hook fns\n",
    "\n",
    "    toks_for_fwd_pass = (toks,) if (minibatch_size is None) else toks.split(minibatch_size, dim=0)\n",
    "\n",
    "    for _toks in toks_for_fwd_pass:\n",
    "        model.run_with_hooks(\n",
    "            _toks,\n",
    "            return_type = None,\n",
    "            fwd_hooks = [\n",
    "                (utils.get_act_name(\"pattern\", layer), hook_fn_cache_attn_probs),\n",
    "                (utils.get_act_name(\"resid_pre\", layer), partial(hook_fn_compute_logit_lens, _toks=_toks)),\n",
    "            ]\n",
    "        )\n",
    "        model.reset_hooks()\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    # ! Get the (non-BOS) attention, split by whether it's in the above_threshold_indices or not\n",
    "\n",
    "    pattern: Float[Tensor, \"batch seqQ seqK_m1\"] = external_storage.pop(\"pattern\")\n",
    "    BOS_pattern = pattern[..., 0]\n",
    "    pattern = pattern[..., 1:]\n",
    "    logit_lens_is_above_threshold: Bool[Tensor, \"batch seqQ seqK_m1\"] = external_storage.pop(\"logit_lens_is_above_threshold\")[..., 1:]\n",
    "    assert pattern.shape == logit_lens_is_above_threshold.shape == (batch_size, seq_len, seq_len - 1)\n",
    "\n",
    "    pattern_above_threshold = pattern[logit_lens_is_above_threshold]\n",
    "    pattern_below_threshold = pattern[~logit_lens_is_above_threshold]\n",
    "\n",
    "    print(f\"Avg attn above threshold: {pattern_above_threshold.mean():.3f}\")\n",
    "    print(f\"Avg attn below threshold: {pattern_below_threshold.mean():.3f}\")\n",
    "\n",
    "    if threshold_cs_classification is None:\n",
    "        attn_is_classified_as_cs: Bool[Tensor, \"batch seqQ seqK_m1\"] = (pattern + 1e-6) > pattern.max(dim=-1, keepdim=True).values\n",
    "    elif isinstance(threshold_cs_classification, int):\n",
    "        attn_is_classified_as_cs: Bool[Tensor, \"batch seqQ seqK_m1\"] = (pattern + 1e-6) > pattern.topk(k=threshold_cs_classification, dim=-1).values[..., [-1]]\n",
    "    elif isinstance(threshold_cs_classification, float):\n",
    "        attn_is_classified_as_cs: Bool[Tensor, \"batch seqQ seqK_m1\"] = pattern > threshold_cs_classification\n",
    "    elif isinstance(threshold_cs_classification, t.Tensor):\n",
    "        threshold_cs_classification = einops.repeat(threshold_cs_classification, \"seqQ -> batch seqQ seqK\", batch=batch_size, seqK=seq_len-1).to(toks.device)\n",
    "        attn_is_classified_as_cs: Bool[Tensor, \"batch seqQ seqK_m1\"] = pattern > threshold_cs_classification\n",
    "\n",
    "    # ! Possibly filter for when BOS isn't attended to (these are more interesting)\n",
    "\n",
    "    if filter_for_BOS_not_largest:\n",
    "        BOS_not_largest = einops.repeat(\n",
    "            BOS_pattern < pattern.max(dim=-1).values,\n",
    "            \"batch seqQ -> batch seqQ seqK_m1\",\n",
    "            seqK_m1=seq_len-1\n",
    "        )\n",
    "        logit_lens_is_above_threshold = logit_lens_is_above_threshold[BOS_not_largest]\n",
    "        attn_is_classified_as_cs = attn_is_classified_as_cs[BOS_not_largest]\n",
    "\n",
    "    # ! Display the actual table, and return the patterns so we can make histograms (later I'll put hist in this function)\n",
    "\n",
    "    table = Table(\n",
    "        \"\", \"CS is expected (logit lens)\", \"CS not expected (logit lens)\", \"% we would have guessed correctly\",\n",
    "        title = \"Confusion plot\" if title is None else title\n",
    "    )\n",
    "\n",
    "    result_yy = (logit_lens_is_above_threshold & attn_is_classified_as_cs).int().sum().item()\n",
    "    result_yn = (~logit_lens_is_above_threshold & attn_is_classified_as_cs).int().sum().item()\n",
    "    result_ny = (logit_lens_is_above_threshold & ~attn_is_classified_as_cs).int().sum().item()\n",
    "    result_nn = (~logit_lens_is_above_threshold & ~attn_is_classified_as_cs).int().sum().item()\n",
    "\n",
    "    def format_fraction(numerator, denominator, bold=False):\n",
    "        return f\"[bold dark_orange]{numerator/denominator:.1%}\" if bold else f\"{numerator/denominator:.1%}\"\n",
    "\n",
    "    table.add_row(\"[b]CS is happening (attn is max)\", str(result_yy), str(result_yn), format_fraction(result_yy, result_yy+result_yn))\n",
    "    table.add_row(\"[b]CS not happening (attn not max)\", str(result_ny), str(result_nn), format_fraction(result_nn, result_nn+result_ny))\n",
    "    table.add_row(\"[b]% we would have guessed correctly\", format_fraction(result_yy, result_yy+result_ny, bold=True), format_fraction(result_nn, result_nn+result_yn, bold=True), \"\")\n",
    "    rprint(table)\n",
    "\n",
    "    # * Deleting violin plot for the time being, because it's way too squashed at zero to present interesting information\n",
    "    # df = pd.DataFrame({\n",
    "    #     \"Copy suppression expected (from logit lens)\": [\"Yes\" if i else \"No\" for i in logit_lens_is_above_threshold.flatten()],\n",
    "    #     \"Attention probabilities\": pattern.flatten().tolist(),\n",
    "    # })\n",
    "    # fig = px.violin(df, x=\"Copy suppression expected (from logit lens)\", y=\"Attention probabilities\", box=True, points=\"all\")\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First experiment - when we think sK will be maximally attended to, how often is it maximally attended to?\n",
    "\n",
    "Answer - about 38% of the time we make correct classifications. So, there's a lot of times when CS is expected but it doesn't happen.\n",
    "\n",
    "I should investigate the \"Browse Examples\" page to figure out whether we should actually have reasonably expected it in these circumstances. But first, I'll run a few more experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg attn above threshold: 0.075\n",
      "Avg attn below threshold: 0.004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Max logit lens &amp; Max attn                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> CS is expected (logit      </span>┃<span style=\"font-weight: bold\"> CS not expected (logit    </span>┃<span style=\"font-weight: bold\"> % we would have guessed    </span>┃\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> lens)                      </span>┃<span style=\"font-weight: bold\"> lens)                     </span>┃<span style=\"font-weight: bold\"> correctly                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"font-weight: bold\">CS is happening (attn is </span> │ 1935                       │ 7667                      │ 20.2%                      │\n",
       "│ <span style=\"font-weight: bold\">max)</span>                      │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">CS not happening (attn </span>   │ 3165                       │ 280033                    │ 98.9%                      │\n",
       "│ <span style=\"font-weight: bold\">not max)</span>                  │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">% we would have guessed </span>  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">37.9%</span>                      │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">97.3%</span>                     │                            │\n",
       "│ <span style=\"font-weight: bold\">correctly</span>                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Max logit lens & Max attn                                             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS is expected (logit     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS not expected (logit   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% we would have guessed   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrectly                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[1mCS is happening (attn is \u001b[0m │ 1935                       │ 7667                      │ 20.2%                      │\n",
       "│ \u001b[1mmax)\u001b[0m                      │                            │                           │                            │\n",
       "│ \u001b[1mCS not happening (attn \u001b[0m   │ 3165                       │ 280033                    │ 98.9%                      │\n",
       "│ \u001b[1mnot max)\u001b[0m                  │                            │                           │                            │\n",
       "│ \u001b[1m% we would have guessed \u001b[0m  │ \u001b[1;38;5;208m37.9%\u001b[0m                      │ \u001b[1;38;5;208m97.3%\u001b[0m                     │                            │\n",
       "│ \u001b[1mcorrectly\u001b[0m                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_data_for_table(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    threshold_logit_lens = None,\n",
    "    threshold_cs_classification = None,\n",
    "    head = (10, 7),\n",
    "    title = \"Max logit lens & Max attn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second experiment - what if we only look at cases when BOS isn't attended to?\n",
    "\n",
    "Result - much better! This suggests we understand well what happens when the head does shit, the only thing we don't understand is when it prefers to attend to BOS. This fits with my own personal model - the head has ways of switching off if it decides that attending to BOS isn't good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg attn above threshold: 0.075\n",
      "Avg attn below threshold: 0.004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Max logit lens &amp; Max attn (filter for BOS not largest)                               </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> CS is expected (logit      </span>┃<span style=\"font-weight: bold\"> CS not expected (logit    </span>┃<span style=\"font-weight: bold\"> % we would have guessed    </span>┃\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> lens)                      </span>┃<span style=\"font-weight: bold\"> lens)                     </span>┃<span style=\"font-weight: bold\"> correctly                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"font-weight: bold\">CS is happening (attn is </span> │ 317                        │ 154                       │ 67.3%                      │\n",
       "│ <span style=\"font-weight: bold\">max)</span>                      │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">CS not happening (attn </span>   │ 218                        │ 27571                     │ 99.2%                      │\n",
       "│ <span style=\"font-weight: bold\">not max)</span>                  │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">% we would have guessed </span>  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">59.3%</span>                      │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">99.4%</span>                     │                            │\n",
       "│ <span style=\"font-weight: bold\">correctly</span>                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Max logit lens & Max attn (filter for BOS not largest)                               \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS is expected (logit     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS not expected (logit   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% we would have guessed   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrectly                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[1mCS is happening (attn is \u001b[0m │ 317                        │ 154                       │ 67.3%                      │\n",
       "│ \u001b[1mmax)\u001b[0m                      │                            │                           │                            │\n",
       "│ \u001b[1mCS not happening (attn \u001b[0m   │ 218                        │ 27571                     │ 99.2%                      │\n",
       "│ \u001b[1mnot max)\u001b[0m                  │                            │                           │                            │\n",
       "│ \u001b[1m% we would have guessed \u001b[0m  │ \u001b[1;38;5;208m59.3%\u001b[0m                      │ \u001b[1;38;5;208m99.4%\u001b[0m                     │                            │\n",
       "│ \u001b[1mcorrectly\u001b[0m                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_data_for_table(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    threshold_logit_lens = None,\n",
    "    threshold_cs_classification = None,\n",
    "    head = (10, 7),\n",
    "    filter_for_BOS_not_largest = True, # removing all cases when BOS is the most attended to\n",
    "    title = \"Max logit lens & Max attn (filter for BOS not largest)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third experiment - top 3, not just top 1\n",
    "\n",
    "Makes both the experiments above look better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg attn above threshold: 0.075\n",
      "Avg attn below threshold: 0.004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Max logit lens &amp; Top-3 attn                                            </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> CS is expected (logit      </span>┃<span style=\"font-weight: bold\"> CS not expected (logit    </span>┃<span style=\"font-weight: bold\"> % we would have guessed    </span>┃\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> lens)                      </span>┃<span style=\"font-weight: bold\"> lens)                     </span>┃<span style=\"font-weight: bold\"> correctly                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"font-weight: bold\">CS is happening (attn is </span> │ 3147                       │ 25174                     │ 11.1%                      │\n",
       "│ <span style=\"font-weight: bold\">max)</span>                      │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">CS not happening (attn </span>   │ 1953                       │ 262526                    │ 99.3%                      │\n",
       "│ <span style=\"font-weight: bold\">not max)</span>                  │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">% we would have guessed </span>  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">61.7%</span>                      │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">91.2%</span>                     │                            │\n",
       "│ <span style=\"font-weight: bold\">correctly</span>                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Max logit lens & Top-3 attn                                            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS is expected (logit     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS not expected (logit   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% we would have guessed   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrectly                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[1mCS is happening (attn is \u001b[0m │ 3147                       │ 25174                     │ 11.1%                      │\n",
       "│ \u001b[1mmax)\u001b[0m                      │                            │                           │                            │\n",
       "│ \u001b[1mCS not happening (attn \u001b[0m   │ 1953                       │ 262526                    │ 99.3%                      │\n",
       "│ \u001b[1mnot max)\u001b[0m                  │                            │                           │                            │\n",
       "│ \u001b[1m% we would have guessed \u001b[0m  │ \u001b[1;38;5;208m61.7%\u001b[0m                      │ \u001b[1;38;5;208m91.2%\u001b[0m                     │                            │\n",
       "│ \u001b[1mcorrectly\u001b[0m                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg attn above threshold: 0.075\n",
      "Avg attn below threshold: 0.004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                             Max logit lens &amp; Top-3 attn (filter for BOS not largest)                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> CS is expected (logit      </span>┃<span style=\"font-weight: bold\"> CS not expected (logit    </span>┃<span style=\"font-weight: bold\"> % we would have guessed    </span>┃\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> lens)                      </span>┃<span style=\"font-weight: bold\"> lens)                     </span>┃<span style=\"font-weight: bold\"> correctly                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"font-weight: bold\">CS is happening (attn is </span> │ 441                        │ 972                       │ 31.2%                      │\n",
       "│ <span style=\"font-weight: bold\">max)</span>                      │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">CS not happening (attn </span>   │ 94                         │ 26753                     │ 99.6%                      │\n",
       "│ <span style=\"font-weight: bold\">not max)</span>                  │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">% we would have guessed </span>  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">82.4%</span>                      │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">96.5%</span>                     │                            │\n",
       "│ <span style=\"font-weight: bold\">correctly</span>                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                             Max logit lens & Top-3 attn (filter for BOS not largest)                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS is expected (logit     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS not expected (logit   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% we would have guessed   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrectly                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[1mCS is happening (attn is \u001b[0m │ 441                        │ 972                       │ 31.2%                      │\n",
       "│ \u001b[1mmax)\u001b[0m                      │                            │                           │                            │\n",
       "│ \u001b[1mCS not happening (attn \u001b[0m   │ 94                         │ 26753                     │ 99.6%                      │\n",
       "│ \u001b[1mnot max)\u001b[0m                  │                            │                           │                            │\n",
       "│ \u001b[1m% we would have guessed \u001b[0m  │ \u001b[1;38;5;208m82.4%\u001b[0m                      │ \u001b[1;38;5;208m96.5%\u001b[0m                     │                            │\n",
       "│ \u001b[1mcorrectly\u001b[0m                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_data_for_table(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    threshold_logit_lens = None,\n",
    "    threshold_cs_classification = 3, # classify as CS if attn is in the top 3, rather than just the max\n",
    "    head = (10, 7),\n",
    "    title = \"Max logit lens & Top-3 attn\",\n",
    ")\n",
    "\n",
    "get_data_for_table(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    threshold_logit_lens = None,\n",
    "    threshold_cs_classification = 3, # classify as CS if attn is in the top 3, rather than just the max\n",
    "    head = (10, 7),\n",
    "    filter_for_BOS_not_largest = True,\n",
    "    title = \"Max logit lens & Top-3 attn (filter for BOS not largest)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth experiment - classify CS by prob absolute value, not prob relative rank\n",
    "\n",
    "I'll classify smth as CS if it has 3x the uniform probability of $1/N$ (where $N$ is sequence length).\n",
    "\n",
    "Result - also pretty excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg attn above threshold: 0.075\n",
      "Avg attn below threshold: 0.004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Max logit lens &amp; Max attn (filter for BOS not largest)                               </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> CS is expected (logit      </span>┃<span style=\"font-weight: bold\"> CS not expected (logit    </span>┃<span style=\"font-weight: bold\"> % we would have guessed    </span>┃\n",
       "┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\"> lens)                      </span>┃<span style=\"font-weight: bold\"> lens)                     </span>┃<span style=\"font-weight: bold\"> correctly                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <span style=\"font-weight: bold\">CS is happening (attn is </span> │ 372                        │ 331                       │ 52.9%                      │\n",
       "│ <span style=\"font-weight: bold\">max)</span>                      │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">CS not happening (attn </span>   │ 163                        │ 27394                     │ 99.4%                      │\n",
       "│ <span style=\"font-weight: bold\">not max)</span>                  │                            │                           │                            │\n",
       "│ <span style=\"font-weight: bold\">% we would have guessed </span>  │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">69.5%</span>                      │ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">98.8%</span>                     │                            │\n",
       "│ <span style=\"font-weight: bold\">correctly</span>                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Max logit lens & Max attn (filter for BOS not largest)                               \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS is expected (logit     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCS not expected (logit   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% we would have guessed   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlens)                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrectly                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ \u001b[1mCS is happening (attn is \u001b[0m │ 372                        │ 331                       │ 52.9%                      │\n",
       "│ \u001b[1mmax)\u001b[0m                      │                            │                           │                            │\n",
       "│ \u001b[1mCS not happening (attn \u001b[0m   │ 163                        │ 27394                     │ 99.4%                      │\n",
       "│ \u001b[1mnot max)\u001b[0m                  │                            │                           │                            │\n",
       "│ \u001b[1m% we would have guessed \u001b[0m  │ \u001b[1;38;5;208m69.5%\u001b[0m                      │ \u001b[1;38;5;208m98.8%\u001b[0m                     │                            │\n",
       "│ \u001b[1mcorrectly\u001b[0m                 │                            │                           │                            │\n",
       "└───────────────────────────┴────────────────────────────┴───────────────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_data_for_table(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    threshold_logit_lens = None,\n",
    "    threshold_cs_classification = 5 / t.arange(1, SEQ_LEN+1),\n",
    "    head = (10, 7),\n",
    "    filter_for_BOS_not_largest = True, # removing all cases when BOS is the most attended to\n",
    "    title = \"Max logit lens & Max attn (filter for BOS not largest)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman\n",
    "\n",
    "Another cool idea - measure spearman corrcoef between our expected rank orderings of the attn score (just based on logit lens), and the actual rank orderings. This is an elegant way to ignore BOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5441, 0.9387, 0.7216, 0.2433],\n",
       "        [0.6815, 0.8348, 0.1832, 0.1904],\n",
       "        [0.4015, 0.7922, 0.9931, 0.1871]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.rand(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 0, 2, 1],\n",
       "        [2, 3, 0, 1],\n",
       "        [3, 0, 1, 2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argsort(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800000011920929\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "def rankdata(tensor):\n",
    "    \"\"\"Compute the ranks of a tensor.\"\"\"\n",
    "    # Argsort the tensor\n",
    "    sorted_indices = t.argsort(tensor)\n",
    "    \n",
    "    # Create an empty tensor for ranks\n",
    "    ranks = t.zeros_like(tensor)\n",
    "    \n",
    "    # Assign ranks\n",
    "    rank = 1\n",
    "    for i in sorted_indices:\n",
    "        ranks[i] = rank\n",
    "        rank += 1\n",
    "        \n",
    "    return ranks\n",
    "\n",
    "def spearman_rank_correlation_coefficient(tensor1, tensor2):\n",
    "    \"\"\"Compute the Spearman Rank Correlation Coefficient between two tensors.\"\"\"\n",
    "    assert tensor1.shape == tensor2.shape, \"Tensors must have the same shape\"\n",
    "    \n",
    "    # Get the ranks of each tensor\n",
    "    rank1 = rankdata(tensor1)\n",
    "    rank2 = rankdata(tensor2)\n",
    "    \n",
    "    # Calculate the difference between the ranks\n",
    "    d = rank1 - rank2\n",
    "    \n",
    "    # Compute the Spearman Rank Correlation Coefficient\n",
    "    n = tensor1.numel()\n",
    "    rs = 1 - (6 * t.sum(d**2)) / (n * (n**2 - 1))\n",
    "    \n",
    "    return rs.item()\n",
    "\n",
    "\n",
    "tensor1 = torch.tensor([3.1, 2.3, 9.5, 4.1], dtype=torch.float32)\n",
    "tensor2 = torch.tensor([1.5, 2.1, 10.2, 3.9], dtype=torch.float32) # quite similar, not perfect (zeroth is smaller)\n",
    "\n",
    "print(spearman_rank_correlation_coefficient(tensor1, tensor2))\n",
    "print(spearman_rank_correlation_coefficient(tensor1, tensor1))\n",
    "print(spearman_rank_correlation_coefficient(tensor1, -tensor1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_experiment(\n",
    "    toks: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    minibatch_size: Optional[int],\n",
    "    head: Tuple[int, int] = (10, 7),\n",
    "):\n",
    "    layer, head_idx = head\n",
    "    batch_size, seq_len = toks.shape\n",
    "    model.reset_hooks()\n",
    "    FUNCTION_TOKS = model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze()\n",
    "    buffer = 5\n",
    "\n",
    "    # Create external storage, which we append to\n",
    "    SPEARMAN_LIST = []\n",
    "    # SPEARMAN_BASELINE_LIST = []\n",
    "\n",
    "    # ! First we define hook functions to do most of the work for us\n",
    "\n",
    "    attn_scores_hook_name = utils.get_act_name(\"attn_scores\", layer)\n",
    "    resid_pre_hook_name = utils.get_act_name(\"resid_pre\", layer)\n",
    "\n",
    "    progress_bar = tqdm(total=batch_size * (seq_len - buffer - 1), desc=\"Computing Spearman\")\n",
    "\n",
    "    def hook_fn_compute_spearman(attn_scores: Float[Tensor, \"batch nheads seqQ seqK\"], hook: HookPoint, _toks: Int[Tensor, \"batch seqK\"]):\n",
    "        '''\n",
    "        We compute spearman here, because the ranks for logit lens have already been calculated & stored in the hook for resid_pre.\n",
    "        '''\n",
    "        _batch_size, _seq_len = _toks.shape\n",
    "        \n",
    "        # Mask to remove function words (true where we want to keep values)\n",
    "        sK_mask: Bool[Tensor, \"batch seqK\"] = (FUNCTION_TOKS[None, None, :] != _toks[:, 1:, None]).all(dim=-1)\n",
    "        # And for causality\n",
    "        sK_mask = einops.repeat(sK_mask, \"batch seqK -> batch seqQ seqK\", seqQ = _seq_len-1)\n",
    "        sK_indices = einops.repeat(t.arange(1, _seq_len, device=_toks.device), \"seqK -> 1 1 seqK\") # first sK isn't BOS, it's BOS+1\n",
    "        sQ_indices = einops.repeat(t.arange(1, _seq_len, device=_toks.device), \"seqQ -> 1 seqQ 1\")\n",
    "        sK_mask = t.where(sQ_indices < sK_indices, sK_mask, False)\n",
    "\n",
    "        head_score = attn_scores[:, head_idx, 1:, 1:]\n",
    "        logit_lens: Tensor = model.hook_dict[resid_pre_hook_name].ctx.pop(\"logit_lens\")\n",
    "        assert logit_lens.shape == head_score.shape == (_batch_size, _seq_len-1, _seq_len-1)\n",
    "\n",
    "        for b in range(_batch_size):\n",
    "            # To be fair, we're starting queries with at least 5 elements (noisy and overweighting weird shit otherwise)\n",
    "            for sQ in range(buffer, _seq_len-1):\n",
    "                _head_score = head_score[b, sQ, sK_mask[b, sQ]]\n",
    "                _logit_lens = logit_lens[b, sQ, sK_mask[b, sQ]]\n",
    "                SPEARMAN_LIST.append(spearman_rank_correlation_coefficient(_head_score, _logit_lens))\n",
    "                # # only calculate baseline random sparingly, to save time\n",
    "                # if b == 0:\n",
    "                #     SPEARMAN_BASELINE_LIST.append(spearman_rank_correlation_coefficient(_head_score, t.rand_like(_logit_lens)))\n",
    "                progress_bar.update()\n",
    "\n",
    "\n",
    "    def hook_fn_compute_logit_lens(resid_pre: Float[Tensor, \"batch seqK d_model\"], hook: HookPoint, _toks: Int[Tensor, \"batch seqK\"]):\n",
    "        '''\n",
    "        Computes logit lens at the residual stream before the head, and figures out which (b, sQ, sK) should\n",
    "        have copy suppression activated and which shouldn't.\n",
    "        '''\n",
    "        logit_lens_for_src = einops.einsum(\n",
    "            resid_pre[:, 1:],\n",
    "            model.W_U.T[_toks[:, 1:]], # ignore BOS\n",
    "            \"batch seqQ d_model, batch seqK d_model -> batch seqQ seqK\"\n",
    "        )\n",
    "        hook.ctx[\"logit_lens\"] = logit_lens_for_src\n",
    "     \n",
    "    \n",
    "    # ! Next we run a fwd pass, to activate these hook fns\n",
    "\n",
    "    toks_for_fwd_pass = (toks,) if (minibatch_size is None) else toks.split(minibatch_size, dim=0)\n",
    "\n",
    "    for _toks in toks_for_fwd_pass:\n",
    "        model.run_with_hooks(\n",
    "            _toks,\n",
    "            return_type = None,\n",
    "            fwd_hooks = [\n",
    "                (attn_scores_hook_name, partial(hook_fn_compute_spearman, _toks=_toks)),\n",
    "                (resid_pre_hook_name, partial(hook_fn_compute_logit_lens, _toks=_toks)),\n",
    "            ]\n",
    "        )\n",
    "        model.reset_hooks()\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    # ! Print results (including random spearman baseline)\n",
    "\n",
    "    SPEARMAN_LIST = t.tensor(SPEARMAN_LIST)[~t.isnan(t.tensor(SPEARMAN_LIST))]\n",
    "    # SPEARMAN_BASELINE_LIST = t.tensor(SPEARMAN_BASELINE_LIST)[~t.isnan(t.tensor(SPEARMAN_BASELINE_LIST))]\n",
    "\n",
    "    print(f\"Avg spearman correlation coefficient = {SPEARMAN_LIST.mean()}\")\n",
    "    # print(f\"Baseline: random spearman = {SPEARMAN_BASELINE_LIST.mean()}\")\n",
    "\n",
    "    return SPEARMAN_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Spearman: 100%|██████████| 4400/4400 [00:03<00:00, 1120.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg spearman correlation coefficient = 0.013215672224760056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SPEARMAN_LIST = spearman_experiment(\n",
    "    toks = DATA_TOKS,\n",
    "    model = model,\n",
    "    minibatch_size = 10,\n",
    "    head = (10, 7),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
