{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum2.utils import get_effective_embedding\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cpu\",\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_attn_result(False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE = get_effective_embedding(model, use_codys_without_attention_changes=False)[\"W_E (only MLPs)\"]\n",
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE_scaled = W_EE / W_EE.std(dim=-1, keepdim=True)\n",
    "W_U_scaled = W_U / W_U.std(dim=0, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for the scale factors\n",
    "\n",
    "### QK\n",
    "\n",
    "For keys, we should use the effective embedding divided by its std (because it'll have been layernormed).\n",
    "\n",
    "For queries, I'm not totally sure. I think we should scale it, because we're pretending that the token is predicted in the residual stream as strongly as it could possibly be.\n",
    "\n",
    "### OV\n",
    "\n",
    "Things are a little more suble here. `W_EE_scaled @ W_V @ W_O` gets scaled before we extract logit lens. So we need to find this matrix, find its std deviation, and then divide `W_EE_scaled @ W_V` by this. `W_O @ W_U` is kept as is, because this is meant to represent the logit lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_dict = {\"tokenizer\": model.tokenizer}\n",
    "\n",
    "for layer, head in [(10, 7)]: # (11, 10)\n",
    "\n",
    "    W_EE_V = W_EE_scaled @ model.W_V[layer, head]\n",
    "    W_EE_V_O = W_EE_V @ model.W_O[layer, head]\n",
    "    W_EE_V_O_scale = W_EE_V_O.std(dim=-1)\n",
    "    W_EE_V = W_EE_V / W_EE_V_O_scale[:, None]\n",
    "\n",
    "    W_U_O = (model.W_O[layer, head] @ W_U)\n",
    "\n",
    "    W_U_Q = W_U_scaled.T @ model.W_Q[layer, head]\n",
    "\n",
    "    W_EE_K = W_EE_scaled @ model.W_K[layer, head]\n",
    "\n",
    "    mega_dict[f\"{layer}.{head}\"] = {\n",
    "        \"W_EE_V\": W_EE_V.clone(),\n",
    "        \"W_U_O\": W_U_O.clone(),\n",
    "        \"W_U_Q\": W_U_Q.clone(),\n",
    "        \"W_EE_K\": W_EE_K.clone(),\n",
    "        # \"b_Q\": model.b_Q[10, 7],\n",
    "        # \"b_K\": model.b_K[10, 7],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokenizer', '10.1', '10.7', '11.10'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mega_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/\"\n",
    "with gzip.open(path + \"OV_QK_circuits_less_local.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mega_dict, f)\n",
    "with gzip.open(path + \"OV_QK_circuits_less_public.pkl\", \"wb\") as f:\n",
    "    pickle.dump({k: v for k, v in mega_dict.items() if k != (10, 1)}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokenizer', 'W_EE_V', 'W_U_O', 'W_U_Q', 'W_EE_K'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(gzip.open(path + \"OV_QK_circuits_less.pkl\", \"rb\")).keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
