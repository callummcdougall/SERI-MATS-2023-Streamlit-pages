{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install plotly\n",
    "# %pip install protobuf==3.20.0\n",
    "# %pip install circuitsvis\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum2.ioi_and_bos.ioi_functions import (\n",
    "    attn_scores_as_linear_func_of_keys,\n",
    "    attn_scores_as_linear_func_of_queries,\n",
    "    get_attn_scores_as_linear_func_of_queries_for_histogram,\n",
    "    get_attn_scores_as_linear_func_of_keys_for_histogram,\n",
    "    decompose_attn_scores,\n",
    "    plot_contribution_to_attn_scores,\n",
    "    project,\n",
    "    decompose_attn_scores_full,\n",
    "    create_fucking_massive_plot_1,\n",
    "    create_fucking_massive_plot_2,\n",
    "    get_nonspace_name_tokenIDs,\n",
    "    get_nonspace_name_tokenIDs,\n",
    "    get_lowercase_name_tokenIDs,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    get_effective_embedding,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_embeddings = get_effective_embedding(model) \n",
    "\n",
    "W_U = model.W_U\n",
    "W_EE = effective_embeddings[\"W_E (including MLPs)\"]\n",
    "W_EE_subE = effective_embeddings[\"W_E (only MLPs)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching positions\n",
    "\n",
    "Below, I'm going to perform a patching experiment where the residual stream values for `key_input` before heads in layer 10 are replaced with their values on the flipped dataset: where the token identities are swapped around. I expect this not to completely reverse the effect, which would be a sign that there's positional shit going on that we just failed to account for (maybe because it was mediated via the MLPs)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
