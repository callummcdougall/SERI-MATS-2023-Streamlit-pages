{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum2.ioi_and_bos.ioi_functions import (\n",
    "    attn_scores_as_linear_func_of_keys,\n",
    "    attn_scores_as_linear_func_of_queries,\n",
    "    get_attn_scores_as_linear_func_of_queries_for_histogram,\n",
    "    get_attn_scores_as_linear_func_of_keys_for_histogram,\n",
    "    decompose_attn_scores,\n",
    "    plot_contribution_to_attn_scores,\n",
    "    project,\n",
    "    decompose_attn_scores_full,\n",
    "    create_fucking_massive_plot_1,\n",
    "    create_fucking_massive_plot_2,\n",
    "    get_nonspace_name_tokenIDs,\n",
    "    get_nonspace_name_tokenIDs,\n",
    "    get_lowercase_name_tokenIDs,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    get_effective_embedding,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_embeddings = get_effective_embedding(model, use_codys_without_attention_changes=False) \n",
    "\n",
    "W_U = model.W_U\n",
    "W_EE = effective_embeddings[\"W_E (including MLPs)\"]\n",
    "W_EE_subE = effective_embeddings[\"W_E (only MLPs)\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching positions\n",
    "\n",
    "Below, I'm going to perform a patching experiment where the residual stream values for `q_input` before heads in layer 10 are replaced with their values on the flipped dataset: where the token identities are swapped around. Essentially, this means that head 10.7's *\"look for token `IO` at position `pos(IO)`\"* desire will be replaced with *\"look for token `IO` at position `pos(S1)`\"*. If positions don't matter, this experiment won't do anything. But if positions do matter, then I expect this to reduce attention diff.\n",
    "\n",
    "Why is this meaningfully different than things we've done before? Because before I think we either didn't implement it well (I don't trust myself back at this point!) or else we just replaced the positional embeddings `W_pos` rather than patching \"the whole way up to head 10.7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_hook_names = [utils.get_act_name(\"attn_scores\", 10), utils.get_act_name(\"scale\", 10, \"ln1\")]\n",
    "\n",
    "ioi_dataset, ioi_cache = generate_data_and_caches(\n",
    "    N = 150,\n",
    "    model = model,\n",
    "    verbose = False,\n",
    "    seed = 42,\n",
    "    prepend_bos = True,\n",
    "    only_ioi = True,\n",
    "    symmetric = True,\n",
    "    return_cache = True,\n",
    "    names_filter = lambda name: name in clean_hook_names,\n",
    ")\n",
    "ioi_cache = cast(ActivationCache, ioi_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/callummcdougall/eindex.git\n",
    "from eindex import eindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 sentences of IOI dataset:\n",
      "Then, Alex and Anthony had a lot of fun at the school. Anthony gave a ring to Alex\n",
      "Then, Anthony and Alex had a lot of fun at the school. Alex gave a ring to Anthony\n",
      "Then, Connor and Roman were working at the house. Roman decided to give a basketball to Connor\n",
      "\n",
      "First 3 sentences of IOI-flipped dataset:\n",
      "Then, Anthony and Alex had a lot of fun at the school. Anthony gave a ring to Alex\n",
      "Then, Alex and Anthony had a lot of fun at the school. Alex gave a ring to Anthony\n",
      "Then, Roman and Connor were working at the house. Roman decided to give a basketball to Connor\n",
      "patching\n",
      "projecting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                IOI patching, effect on 10.7 attn score diff                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Intervention                             </span>┃<span style=\"font-weight: bold\"> Attention Score Diff (IO - S1) </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Clean                                    │ 3.2874                         │\n",
       "│ Flipped positional information (patched) │ 2.9462                         │\n",
       "│ Project queries onto W_U[IO, S1]         │ 1.2586                         │\n",
       "└──────────────────────────────────────────┴────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                IOI patching, effect on 10.7 attn score diff                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIntervention                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAttention Score Diff (IO - S1)\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Clean                                    │ 3.2874                         │\n",
       "│ Flipped positional information (patched) │ 2.9462                         │\n",
       "│ Project queries onto W_U[IO, S1]         │ 1.2586                         │\n",
       "└──────────────────────────────────────────┴────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_attn_diff_from_cache(\n",
    "    cache: ActivationCache,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    NEG_HEAD: Tuple[int, int] = (10, 7),\n",
    ") -> float:\n",
    "    \n",
    "    LAYER, HEAD = NEG_HEAD\n",
    "    end_posns = ioi_dataset.word_idx[\"end\"]\n",
    "    IO_posns = ioi_dataset.word_idx[\"IO\"]\n",
    "    S1_posns = ioi_dataset.word_idx[\"S1\"]\n",
    "\n",
    "    # Get attn scores from cache\n",
    "    attn_score_clean = cache[\"attn_scores\", LAYER][:, HEAD] # [batch seqQ seqK]\n",
    "\n",
    "    # We care abt attn from END to IO1, and END to S1\n",
    "    attn_score_IO_clean = eindex(attn_score_clean, end_posns, IO_posns, \"batch [batch] [batch]\")\n",
    "    attn_score_S1_clean = eindex(attn_score_clean, end_posns, S1_posns, \"batch [batch] [batch]\")\n",
    "    attn_score_diff_avg_clean = (attn_score_IO_clean - attn_score_S1_clean).mean()\n",
    "\n",
    "    return attn_score_diff_avg_clean.item()\n",
    "    \n",
    "\n",
    "\n",
    "def run_patching_experiment(\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    ioi_cache: ActivationCache = ioi_cache,\n",
    "    model: HookedTransformer = model,\n",
    "    NEG_HEAD: Tuple[int, int] = (10, 7),\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    '''\n",
    "    Runs the described patching experiment. Prints the average attn score diff in both cases.\n",
    "    '''\n",
    "    model.reset_hooks()\n",
    "    LAYER, HEAD = NEG_HEAD\n",
    "    batch_size, seq_len = ioi_dataset.toks.shape\n",
    "\n",
    "    end_posns = ioi_dataset.word_idx[\"end\"]\n",
    "\n",
    "    hook_name_resid_pre = utils.get_act_name(\"resid_pre\", LAYER)\n",
    "    hook_name_q_input = utils.get_act_name(\"q_input\", LAYER)\n",
    "    hook_name_attn_scores = utils.get_act_name(\"attn_scores\", LAYER)\n",
    "    hook_name_scale = utils.get_act_name(\"scale\", 10, \"ln1\")\n",
    "\n",
    "    # Get clean attn scores\n",
    "    attn_score_diff_avg_clean = get_attn_diff_from_cache(ioi_cache, NEG_HEAD=NEG_HEAD)\n",
    "\n",
    "    # Generate dataset with IO and S1 reversed (i.e. negating results of this is like just flipping posns of these)\n",
    "    flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB -> BAB, BAB -> ABB\")\n",
    "    # Sanity check\n",
    "    if verbose:\n",
    "        print(\"First 3 sentences of IOI dataset:\")\n",
    "        for i in range(3): print(ioi_dataset.sentences[i])\n",
    "        print(\"\\nFirst 3 sentences of IOI-flipped dataset:\")\n",
    "        for i in range(3): print(flipped_dataset.sentences[i])\n",
    "    \n",
    "    # Get new resid_pre values from this dataset\n",
    "    _, flipped_cache = model.run_with_cache(\n",
    "        flipped_dataset.toks,\n",
    "        return_type = None,\n",
    "        names_filter = lambda name: name == hook_name_resid_pre,\n",
    "    )\n",
    "    flipped_resid_pre = flipped_cache[hook_name_resid_pre] # [batch seq d_model]\n",
    "\n",
    "    # Define hook fns to patch query input onto these new values\n",
    "    def hook_queries(query_input: Float[Tensor, \"batch seq heads d_model\"], hook: HookPoint, mode: Literal[\"project\", \"patch\"]):\n",
    "        assert mode in [\"project\", \"patch\"]\n",
    "        if mode == \"project\":\n",
    "            if verbose: print(\"projecting\")\n",
    "            W_U_IO = W_U.T[ioi_dataset.io_tokenIDs]\n",
    "            W_U_S1 = W_U.T[ioi_dataset.s_tokenIDs]\n",
    "            projection_directions = t.stack([W_U_IO, W_U_S1], dim=-1) # [batch d_model 2]\n",
    "            query_input[range(batch_size), end_posns, HEAD] = project(\n",
    "                query_input[range(batch_size), end_posns, HEAD],\n",
    "                projection_directions,\n",
    "            )\n",
    "        elif mode == \"patch\":\n",
    "            if verbose: print(\"patching\")\n",
    "            query_input[:, :, HEAD] = flipped_resid_pre        \n",
    "        return query_input\n",
    "\n",
    "    def hook_freeze_scale(scale: Float[Tensor, \"batch seq *heads 1\"], hook: HookPoint):\n",
    "        return ioi_cache[hook_name_scale]\n",
    "    \n",
    "    # Run hooked fwd pass for both hook fns\n",
    "    model.reset_hooks()\n",
    "    model.add_hook(hook_name_q_input, partial(hook_queries, mode=\"patch\"))\n",
    "    model.add_hook(hook_name_scale, hook_freeze_scale)\n",
    "    _, patched_cache = model.run_with_cache(ioi_dataset.toks, return_type=None, names_filter = lambda name: name == hook_name_attn_scores)\n",
    "    attn_score_diff_avg_patched = get_attn_diff_from_cache(patched_cache)\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    model.add_hook(hook_name_q_input, partial(hook_queries, mode=\"project\"))\n",
    "    model.add_hook(hook_name_scale, hook_freeze_scale)\n",
    "    _, projected_cache = model.run_with_cache(ioi_dataset.toks, return_type=None, names_filter = lambda name: name == hook_name_attn_scores)\n",
    "    attn_score_diff_avg_projected = get_attn_diff_from_cache(projected_cache)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    # Print all results\n",
    "    table = Table(\"Intervention\", \"Attention Score Diff (IO - S1)\", title=\"IOI patching, effect on 10.7 attn score diff\")\n",
    "    table.add_row(\"Clean\", f\"{attn_score_diff_avg_clean:.4f}\")\n",
    "    table.add_row(\"Flipped positional information (patched)\", f\"{attn_score_diff_avg_patched:.4f}\")\n",
    "    table.add_row(\"Project queries onto W_U[IO, S1]\", f\"{attn_score_diff_avg_projected:.4f}\")\n",
    "    rprint(table)\n",
    "\n",
    "run_patching_experiment(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Positional information is more important than we thought (maybe explains about 5-10% of the attention), but not important enough to make a meaningful difference I also anticipate it'll be hard to work into projections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
