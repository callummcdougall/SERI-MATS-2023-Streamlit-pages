{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Prompts\n",
    "\n",
    "This is the notebook I use to test out the functions in this directory, and generate the plots in the Streamlit page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/root/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum2.ov_qk_circuits.ov_qk_plot_functions import (\n",
    "    plot_logit_lens,\n",
    "    plot_full_matrix_histogram,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    CSS,\n",
    "    generate_4_html_plots,\n",
    "    generate_4_html_plots_batched,\n",
    "    generate_html_for_DLA_plot,\n",
    "    generate_html_for_logit_plot,\n",
    "    generate_html_for_loss_plot,\n",
    "    generate_html_for_unembedding_components_plot,\n",
    "    attn_filter,\n",
    "    _get_color,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_result_mean,\n",
    "    get_model_results,\n",
    "    HeadResults,\n",
    "    LayerResults,\n",
    "    DictOfHeadResults,\n",
    "    ModelResults,\n",
    "    first_occurrence,\n",
    "    project,\n",
    "    model_fwd_pass_from_resid_pre,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    create_title_and_subtitles,\n",
    "    get_effective_embedding,\n",
    "    parse_str,\n",
    "    parse_str_tok_for_printing,\n",
    "    parse_str_toks_for_printing,\n",
    "    topk_of_Nd_tensor,\n",
    "    ST_HTML_PATH,\n",
    "    NEGATIVE_HEADS,\n",
    "    process_webtext,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cpu\" # \"cuda\"\n",
    "    # fold value bias?\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "W_EE_dict = get_effective_embedding(model, use_codys_without_attention_changes=False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = torch.Size([41, 51])\n",
      "\n",
      "First prompt:\n",
      "<|endoftext|>Oh boy was this damn hard to crack.\n",
      "\n",
      "Ok, I believe before it was established before that Aperture Science headquarters are in Cleveland, OH.\n",
      "\n",
      "Source: HL2EP2\n",
      "\n",
      "Though, this has been found.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 41 # 51 for viz, 200 for my local one\n",
    "SEQ_LEN = 51 # (61 for viz, no more because attn)\n",
    "batch_idx = 36\n",
    "\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(seed=6, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, verbose=True, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test html in small case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 51])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TOKS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS         ... 0.02s\n",
      "LOGITS ORIG  ... 0.00s\n",
      "LOGITS 10.7  ... 0.03s\n",
      "LOGITS 11.10 ... 0.02s\n",
      "ATTN         ... 0.01s\n",
      "UNEMBEDDINGS ... 0.01s\n",
      "\n",
      "LOSS\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n",
      "LOGITS_ORIG\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n",
      "LOGITS_ABLATED\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n",
      "DLA\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n",
      "ATTN\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n",
      "UNEMBEDDINGS\n",
      "0 = [0]\n",
      "1 = ['10.7', '11.10']\n",
      "2 = ['both+frozen+mean', 'both+unfrozen+mean', 'direct+frozen+mean', 'direct+unfrozen+mean', 'indirect (excluding 11.10)+frozen+mean', 'indirect (excluding 11.10)+unfrozen+mean', 'indirect+frozen+mean', 'indirect+unfrozen+mean']\n",
      "3 = [False, True]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
       "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@350&display=swap\" rel=\"stylesheet\">\n",
       "\n",
       "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
       "\n",
       "<style>\n",
       "body {\n",
       "    font-family: 'Source Sans 3', sans-serif;\n",
       "}\n",
       "\n",
       "table {\n",
       "    border-collapse: collapse;\n",
       "    width: 100%;\n",
       "}\n",
       "th, td {\n",
       "    border: 1px solid black;\n",
       "    padding: 6px;\n",
       "    text-align: left;\n",
       "    line-height: 0.8em;\n",
       "}\n",
       ".empty-row td {\n",
       "    border: none;\n",
       "}\n",
       "mark {\n",
       "    font-size: 1rem;\n",
       "    line-height: 1.8rem;\n",
       "    padding: 1px;\n",
       "    margin-right: 1px;\n",
       "}\n",
       ".tooltip {\n",
       "    position: relative;\n",
       "    display: inline-block;\n",
       "    cursor: pointer;\n",
       "}\n",
       ".tooltip .tooltiptext {\n",
       "    font-size: 0.9rem;\n",
       "    min-width: 275px;\n",
       "    visibility: hidden;\n",
       "    background-color: #eee;\n",
       "    color: #000;\n",
       "    text-align: center;\n",
       "    padding: 5px;\n",
       "    position: absolute;\n",
       "    z-index: 1;\n",
       "    top: 125%;\n",
       "    left: 50%;\n",
       "    margin-left: -10px;\n",
       "    opacity: 0;\n",
       "    transition: opacity 0.0s;\n",
       "}\n",
       ".tooltip.hovered .tooltiptext,\n",
       ".tooltip.clicked .tooltiptext {\n",
       "    visibility: visible;\n",
       "    opacity: 1;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "$(document).ready(function(){\n",
       "  $('.tooltip').hover(function(){\n",
       "    if (!$(this).hasClass('clicked')) {\n",
       "      $(this).addClass('hovered');\n",
       "    }\n",
       "    var tooltipWidth = $(this).children('.tooltiptext').outerWidth();\n",
       "    var viewportWidth = $(window).width();\n",
       "    var tooltipRight = $(this).offset().left + tooltipWidth;\n",
       "    if (tooltipRight > viewportWidth) {\n",
       "      $(this).children('.tooltiptext').css('left', 'auto').css('right', '0');\n",
       "    }\n",
       "  }, function() {\n",
       "    if (!$(this).hasClass('clicked')) {\n",
       "      $(this).removeClass('hovered');\n",
       "    }\n",
       "    $(this).children('.tooltiptext').css('left', '50%').css('right', 'auto');\n",
       "  });\n",
       "\n",
       "  $('.tooltip').click(function(e){\n",
       "    e.stopPropagation();\n",
       "    if ($(this).hasClass('clicked')) {\n",
       "      $(this).removeClass('clicked');\n",
       "    } else {\n",
       "      $('.tooltip').removeClass('clicked');\n",
       "      $(this).addClass('clicked');\n",
       "    }\n",
       "  });\n",
       "\n",
       "  $(document).click(function() {\n",
       "    $('.tooltip').removeClass('clicked');\n",
       "  });\n",
       "});\n",
       "</script>\n",
       "<td>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\"><|endoftext|></font></mark><span class=\"tooltiptext\">(0) <b>'<|endoftext|>'</b><br>0.0000</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 246)\"><font color=\"black\">All</font></mark><span class=\"tooltiptext\">(1) <b>'All'</b><br>-0.0005</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(246, 246, 247)\"><font color=\"black\">'s</font></mark><span class=\"tooltiptext\">(2) <b>''s'</b><br>0.0009</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(245, 246, 247)\"><font color=\"black\">&nbsp;fair</font></mark><span class=\"tooltiptext\">(3) <b>' fair'</b><br>0.0018</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(241, 244, 246)\"><font color=\"black\">&nbsp;in</font></mark><span class=\"tooltiptext\">(4) <b>' in'</b><br>0.0041</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(246, 247, 247)\"><font color=\"black\">&nbsp;love</font></mark><span class=\"tooltiptext\">(5) <b>' love'</b><br>0.0007</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(5, 48, 97)\"><font color=\"white\">&nbsp;and</font></mark><span class=\"tooltiptext\">(6) <b>' and'</b><br>0.1401</span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">&nbsp;war</font></mark><span class=\"tooltiptext\">(next token unknown)</span></span>&nbsp;</td><br><br><td>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\"><|endoftext|></font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>'<|endoftext|>'</span> ➔ <span background-color:'#ddd'>'All'</span><br><br>Δ logprob on correct token = -0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#66</td><td style=\"background-color:#c2d9ff; color:black\">'All'</td><td style=\"background-color:#c2d9ff; color:black\">-6.26</td><td style=\"background-color:#c2d9ff; color:black\">0.19%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>'\\n'</td><td>-2.78</td><td>6.23%</td></tr><tr><td>#1</td><td>'The'</td><td>-3.28</td><td>3.77%</td></tr><tr><td>#2</td><td>'\"'</td><td>-3.73</td><td>2.41%</td></tr><tr><td>#3</td><td>'A'</td><td>-3.94</td><td>1.94%</td></tr><tr><td>#4</td><td>'I'</td><td>-4.00</td><td>1.83%</td></tr><tr><td>#5</td><td>'In'</td><td>-4.46</td><td>1.16%</td></tr><tr><td>#6</td><td>'.'</td><td>-4.48</td><td>1.13%</td></tr><tr><td>#7</td><td>'It'</td><td>-4.70</td><td>0.91%</td></tr><tr><td>#8</td><td>'S'</td><td>-4.75</td><td>0.87%</td></tr><tr><td>#9</td><td>'This'</td><td>-4.86</td><td>0.78%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">All</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>'All'</span> ➔ <span background-color:'#ddd'>\"'s\"</span><br><br>Δ logprob on correct token = 0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#205</td><td style=\"background-color:#c2d9ff; color:black\">\"'s\"</td><td style=\"background-color:#c2d9ff; color:black\">-7.54</td><td style=\"background-color:#c2d9ff; color:black\">0.05%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>' of'</td><td>-2.61</td><td>7.39%</td></tr><tr><td>#1</td><td>' the'</td><td>-2.90</td><td>5.48%</td></tr><tr><td>#2</td><td>'-'</td><td>-4.17</td><td>1.55%</td></tr><tr><td>#3</td><td>' you'</td><td>-4.20</td><td>1.49%</td></tr><tr><td>#4</td><td>' this'</td><td>-4.27</td><td>1.40%</td></tr><tr><td>#5</td><td>' images'</td><td>-4.33</td><td>1.32%</td></tr><tr><td>#6</td><td>' rights'</td><td>-4.36</td><td>1.28%</td></tr><tr><td>#7</td><td>' in'</td><td>-4.44</td><td>1.18%</td></tr><tr><td>#8</td><td>' that'</td><td>-4.62</td><td>0.98%</td></tr><tr><td>#9</td><td>' these'</td><td>-4.71</td><td>0.90%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">'s</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>\"'s\"</span> ➔ <span background-color:'#ddd'>' fair'</span><br><br>Δ logprob on correct token = -0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#0</td><td style=\"background-color:#c2d9ff; color:black\">' fair'</td><td style=\"background-color:#c2d9ff; color:black\">-1.02</td><td style=\"background-color:#c2d9ff; color:black\">35.89%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>' fair'</td><td>-1.02</td><td>35.89%</td></tr><tr><td>#1</td><td>' well'</td><td>-1.82</td><td>16.21%</td></tr><tr><td>#2</td><td>' Well'</td><td>-3.20</td><td>4.06%</td></tr><tr><td>#3</td><td>' right'</td><td>-3.39</td><td>3.39%</td></tr><tr><td>#4</td><td>' good'</td><td>-3.91</td><td>2.01%</td></tr><tr><td>#5</td><td>' clear'</td><td>-4.13</td><td>1.61%</td></tr><tr><td>#6</td><td>' not'</td><td>-4.23</td><td>1.45%</td></tr><tr><td>#7</td><td>' in'</td><td>-4.38</td><td>1.26%</td></tr><tr><td>#8</td><td>' a'</td><td>-4.48</td><td>1.13%</td></tr><tr><td>#9</td><td>' Fair'</td><td>-4.53</td><td>1.08%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">&nbsp;fair</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>' fair'</span> ➔ <span background-color:'#ddd'>' in'</span><br><br>Δ logprob on correct token = -0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#0</td><td style=\"background-color:#c2d9ff; color:black\">' in'</td><td style=\"background-color:#c2d9ff; color:black\">-0.38</td><td style=\"background-color:#c2d9ff; color:black\">68.28%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>' in'</td><td>-0.38</td><td>68.28%</td></tr><tr><td>#1</td><td>','</td><td>-2.50</td><td>8.18%</td></tr><tr><td>#2</td><td>' here'</td><td>-4.00</td><td>1.83%</td></tr><tr><td>#3</td><td>' when'</td><td>-4.01</td><td>1.82%</td></tr><tr><td>#4</td><td>' and'</td><td>-4.14</td><td>1.59%</td></tr><tr><td>#5</td><td>' to'</td><td>-4.18</td><td>1.54%</td></tr><tr><td>#6</td><td>'.'</td><td>-4.45</td><td>1.17%</td></tr><tr><td>#7</td><td>' there'</td><td>-4.49</td><td>1.12%</td></tr><tr><td>#8</td><td>' for'</td><td>-4.67</td><td>0.94%</td></tr><tr><td>#9</td><td>' with'</td><td>-4.75</td><td>0.87%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">&nbsp;in</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>' in'</span> ➔ <span background-color:'#ddd'>' love'</span><br><br>Δ logprob on correct token = -0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#3</td><td style=\"background-color:#c2d9ff; color:black\">' love'</td><td style=\"background-color:#c2d9ff; color:black\">-3.39</td><td style=\"background-color:#c2d9ff; color:black\">3.36%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>' the'</td><td>-2.08</td><td>12.53%</td></tr><tr><td>#1</td><td>' a'</td><td>-3.02</td><td>4.90%</td></tr><tr><td>#2</td><td>' this'</td><td>-3.24</td><td>3.90%</td></tr><tr><td>#3</td><td>' love'</td><td>-3.39</td><td>3.36%</td></tr><tr><td>#4</td><td>' heaven'</td><td>-3.90</td><td>2.02%</td></tr><tr><td>#5</td><td>' America'</td><td>-4.22</td><td>1.47%</td></tr><tr><td>#6</td><td>' life'</td><td>-4.38</td><td>1.26%</td></tr><tr><td>#7</td><td>' that'</td><td>-4.63</td><td>0.97%</td></tr><tr><td>#8</td><td>' New'</td><td>-4.87</td><td>0.77%</td></tr><tr><td>#9</td><td>' Scotland'</td><td>-4.95</td><td>0.71%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">&nbsp;love</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>' love'</span> ➔ <span background-color:'#ddd'>' and'</span><br><br>Δ logprob on correct token = -0.00<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#2</td><td style=\"background-color:#c2d9ff; color:black\">' and'</td><td style=\"background-color:#c2d9ff; color:black\">-2.23</td><td style=\"background-color:#c2d9ff; color:black\">10.79%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>'.'</td><td>-1.97</td><td>14.00%</td></tr><tr><td>#1</td><td>','</td><td>-2.02</td><td>13.22%</td></tr><tr><td>#2</td><td>' and'</td><td>-2.23</td><td>10.79%</td></tr><tr><td>#3</td><td>'!'</td><td>-2.51</td><td>8.12%</td></tr><tr><td>#4</td><td>'\\n'</td><td>-2.82</td><td>5.96%</td></tr><tr><td>#5</td><td>' with'</td><td>-3.29</td><td>3.72%</td></tr><tr><td>#6</td><td>' when'</td><td>-3.60</td><td>2.72%</td></tr><tr><td>#7</td><td>' in'</td><td>-3.78</td><td>2.29%</td></tr><tr><td>#8</td><td>' is'</td><td>-3.79</td><td>2.25%</td></tr><tr><td>#9</td><td>':'</td><td>-3.81</td><td>2.21%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(249, 239, 234)\"><font color=\"black\">&nbsp;and</font></mark><span class=\"tooltiptext\"><span background-color:'#ddd'>' and'</span> ➔ <span background-color:'#ddd'>' war'</span><br><br>Δ logprob on correct token = -0.14<br><br><table><thead><tr><th>Rank</th><th>Word</th><th>Logprob</th><th>Prob</th></tr></thead><tbody><tr><td style=\"background-color:#c2d9ff; color:black\">#1</td><td style=\"background-color:#c2d9ff; color:black\">' war'</td><td style=\"background-color:#c2d9ff; color:black\">-2.20</td><td style=\"background-color:#c2d9ff; color:black\">11.13%</td></tr><tr class=\"empty-row\"><td></td><td></td><td></td><td></td></tr><tr><td>#0</td><td>' love'</td><td>-1.96</td><td>14.07%</td></tr><tr><td>#1</td><td>' war'</td><td>-2.20</td><td>11.13%</td></tr><tr><td>#2</td><td>' hate'</td><td>-2.30</td><td>10.07%</td></tr><tr><td>#3</td><td>' peace'</td><td>-3.65</td><td>2.60%</td></tr><tr><td>#4</td><td>' all'</td><td>-3.94</td><td>1.94%</td></tr><tr><td>#5</td><td>' friendship'</td><td>-3.97</td><td>1.89%</td></tr><tr><td>#6</td><td>' happiness'</td><td>-4.02</td><td>1.80%</td></tr><tr><td>#7</td><td>' marriage'</td><td>-4.23</td><td>1.45%</td></tr><tr><td>#8</td><td>' romance'</td><td>-4.26</td><td>1.41%</td></tr><tr><td>#9</td><td>' good'</td><td>-4.32</td><td>1.33%</td></tr></tbody></table></span></span>&nbsp;<span class=\"tooltip\"><mark style=\"background-color:rgb(247, 247, 247)\"><font color=\"black\">&nbsp;war</font></mark><span class=\"tooltiptext\">(next token unknown)</span></span>&nbsp;</td><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "prompt = \"All's fair in love and war\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "result_mean = get_result_mean(\n",
    "    head_list = NEGATIVE_HEADS,\n",
    "    toks = DATA_TOKS[:, :toks.shape[1]],\n",
    "    model = model,\n",
    "    minibatch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "MODEL_RESULTS = get_model_results(\n",
    "    model = model,\n",
    "    toks = toks,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    result_mean = result_mean,\n",
    ")\n",
    "\n",
    "HTML_PLOTS: Dict[str, Dict[Tuple, str]] = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = toks,\n",
    "    data_str_toks_parsed = str_toks_parsed,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = False,\n",
    "    result_mean = result_mean,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "for k, v in HTML_PLOTS.items():\n",
    "    print(k)\n",
    "    k2 = list(zip(*HTML_PLOTS[\"LOSS\"].keys()))\n",
    "    for j, _k2 in enumerate(k2):\n",
    "        print(f\"{(j)} = {sorted(set(_k2))}\")\n",
    "\n",
    "display(HTML(\"\".join([\n",
    "    CSS,\n",
    "    HTML_PLOTS[\"LOSS\"][(0, \"10.7\", \"direct+frozen+mean\", True)],\n",
    "    \"<br>\" * 2,\n",
    "    # HTML_PLOTS[\"LOGITS_ORIG\"][(0,)],\n",
    "    # \"<br>\" * 2,\n",
    "    HTML_PLOTS[\"LOGITS_ABLATED\"][(0, \"10.7\", \"direct+frozen+mean\")],\n",
    "    \"<br>\" * 21,\n",
    "])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual HTML plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HTML plots...\n",
      "\n",
      "Running forward pass     ... 0.40s\n",
      "Computing model results  ... 4.93s\n",
      "LOSS         ... 0.92s\n",
      "LOGITS ORIG  ... 0.40s\n",
      "LOGITS 10.7  ... 4.24s\n",
      "LOGITS 11.10 ... 3.39s\n",
      "ATTN         ... 1.58s\n",
      "UNEMBEDDINGS ... 2.15s\n",
      "\n",
      "Gathering HTML plots...\n",
      "Saving HTML plots as a single dict, at 'GZIP_HTML_PLOTS_b41_s51.pkl'...\n",
      "Deleting HTML plots we no longer need...\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "result_mean = get_result_mean(NEGATIVE_HEADS, DATA_TOKS, model, minibatch_size=8)\n",
    "\n",
    "generate_4_html_plots_batched(\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS,\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    # max_batch_size = 10,\n",
    "    start_idx = 0,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    result_mean = result_mean,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "with gzip.open(ST_HTML_PATH / \"GZIP_HTML_PLOTS_b41_s51.pkl\", \"rb\") as f:\n",
    "    HTML_PLOTS = pickle.load(f)\n",
    "\n",
    "# Once you've done this, go to the `cspa/cspa_implementation` page to finish the Stremalit stuff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: logit lens (TODO: fix up everything below this; it's old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "neg = False\n",
    "all_ranks = []\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(DATA_TOKS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_to_plot = [\n",
    "#     (35, 39, \" About\"),\n",
    "#     (67, 21, \" delays\"),\n",
    "#     (8, 35, \" rentals\"),\n",
    "#     (8, 54, \" require\"),\n",
    "#     (53, 18, [\" San\", \" Francisco\"]),\n",
    "#     (33, 9, \" Hollywood\"),\n",
    "#     (49, 7, \" Home\"),\n",
    "#     (71, 34, \" sound\"),\n",
    "#     (14, 56, \" Kara\"),\n",
    "# ]\n",
    "# points_to_plot = [\n",
    "#     (45, 42, [\" editorial\"]),\n",
    "#     (45, 58, [\" stadium\", \" Stadium\", \" stadiums\"]),\n",
    "#     (43, 56, [\" Biden\"]),\n",
    "#     (43, 44, [\" interview\", \" campaign\"]),\n",
    "#     (38, 54, [\" Mary\", \" Catholics\"]),\n",
    "#     (33, 29, \" Hollywood\"),\n",
    "#     (33, 42, \" BlackBerry\"),\n",
    "#     (31, 33, [\" Church\", \" churches\"]),\n",
    "#     (28, 53, [\" mobile\", \" phone\", \" device\"]),\n",
    "#     (25, 32, [\" abstraction\", \" abstract\", \" Abstract\"]),\n",
    "#     (18, 25, [\"TPP\", \" Lee\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 35, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 25, [\" Italian\", \" Italy\"]),\n",
    "#     (6, 52, [\" landfill\", \" waste\"]),\n",
    "#     (4, 52, \" jury\"),\n",
    "# ]\n",
    "points_to_plot = [\n",
    "    # (14, 56, \" Kara\"),\n",
    "    # (67, 47, \" case\"),\n",
    "    # (24, 73, \" negotiation\"),\n",
    "    (2, 35, [\" Berkeley\", \"keley\"]),\n",
    "]\n",
    "\n",
    "resid_pre_head = (cache[\"resid_pre\", 10]) / cache[\"scale\", 10, \"ln1\"]  #  - cache[\"resid_pre\", 1]\n",
    "\n",
    "plot_logit_lens(points_to_plot, resid_pre_head, model, DATA_STR_TOKS_PARSED_2, k=15, title=\"Predictions at token ' of', before head 10.7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: QK and OV circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" Berkeley\", src=\"keley\", focus_on=\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" negotiation\", src=\" negotiations\", focus_on=\"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_orig = MODEL_RESULTS.logits_orig[32, 19].log_softmax(-1)\n",
    "logprobs_abl = MODEL_RESULTS.logits[(\"direct\", \"frozen\", \"mean\")][10, 7][32, 19].log_softmax(-1)\n",
    "\n",
    "logprobs_orig_topk = logprobs_orig.topk(10, dim=-1, largest=True)\n",
    "y_orig = logprobs_orig_topk.values.tolist()\n",
    "x = logprobs_orig_topk.indices\n",
    "y_abl = logprobs_abl[x].tolist()\n",
    "x = list(map(repr, model.to_str_tokens(x)))\n",
    "\n",
    "orig_colors = [\"darkblue\"] * len(x)\n",
    "abl_colors = [\"blue\"] * len(x)\n",
    "\n",
    "correct_next_str_tok = \" heated\"\n",
    "correct_next_token = model.to_single_token(\" heated\")\n",
    "# if repr(correct_next_str_tok) in x:\n",
    "#     idx = x.index(repr(correct_next_str_tok))\n",
    "#     orig_colors[idx] = \"darkgreen\"\n",
    "#     abl_colors[idx] = \"green\"\n",
    "\n",
    "x.append(repr(correct_next_str_tok))\n",
    "y_orig.append(logprobs_orig[correct_next_token].item())\n",
    "y_abl.append(logprobs_abl[correct_next_token].item())\n",
    "orig_colors.append(\"darkgreen\")\n",
    "abl_colors.append(\"green\")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Bar(x=x, y=y_orig, name='Original', marker_color=[\"#FF7700\"] * (len(x)-1) + [\"#024B7A\"]), # 7A30AB\n",
    "        go.Bar(x=x, y=y_abl, name='Ablated', marker_color=[\"#FFAE49\"] * (len(x)-1) + [\"#44A5C2\"]), # D44BFA\n",
    "    ],\n",
    "    # data = [\n",
    "    #     go.Bar(x=x[:-1], y=y_orig[:-1], name='Original', marker_color=\"#FF7700\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=x[:-1], y=y_abl[:-1], name='Ablated', marker_color=\"#FFAE49\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_orig[-1]], name='Original (correct token)', marker_color=\"#024B7A\", legendgroup=\"group2\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_abl[-1]], name='Ablated (correct token)', marker_color=\"#44A5C2\", legendgroup=\"group2\"),\n",
    "    # ],\n",
    "    layout = dict(\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=30,\n",
    "        title=\"Logprobs: original vs ablated\",\n",
    "        xaxis_title_text=\"Predicted next token\",\n",
    "        yaxis_title_text=\"Logprob\",\n",
    "        width=800,\n",
    "        bargap=0.35,\n",
    "    )\n",
    ")\n",
    "fig.data = fig.data #+ ({\"name\": \"New\"},)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_matrix_histogram(W_EE_dict, \" device\", k=10, include=[\" devices\"], circuit=\"OV\", neg=True, head=(10, 7))\n",
    "plot_full_matrix_histogram(W_EE_dict, \" devices\", k=10, include=[\" device\"], circuit=\"QK\", neg=False, head=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE = W_EE_dict[\"W_E (including MLPs)\"]\n",
    "W_EE = W_EE_dict[\"W_E (only MLPs)\"]\n",
    "W_U = W_EE_dict[\"W_U\"].T\n",
    "\n",
    "tok_strs = [\"pier\"]\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i].capitalize())\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i] + \"s\")\n",
    "for i in range(len(tok_strs)): tok_strs.append(\" \" + tok_strs[i])\n",
    "tok_strs = [s for s in tok_strs if model.to_tokens(s, prepend_bos=False).squeeze().ndim == 0]\n",
    "\n",
    "toks = model.to_tokens(tok_strs, prepend_bos=False).squeeze()\n",
    "\n",
    "W_EE_toks = W_EE[toks]\n",
    "W_EE_normed = W_EE_toks / W_EE_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_embeddings = W_EE_normed @ W_EE_normed.T\n",
    "\n",
    "W_U_toks = W_U.T[toks]\n",
    "W_U_normed = W_U_toks / W_U_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_unembeddings = W_U_normed @ W_U_normed.T\n",
    "\n",
    "W_EE_OV_toks_107 = W_EE_toks @ model.W_V[10, 7] @ model.W_O[10, 7]\n",
    "W_EE_OV_toks_99 = W_EE_toks @ model.W_V[9, 9] @ model.W_O[9, 9]\n",
    "W_EE_OV_toks_107_normed = W_EE_OV_toks_107 / W_EE_OV_toks_107.norm(dim=-1, keepdim=True)\n",
    "W_EE_OV_toks_99_normed = W_EE_OV_toks_99 / W_EE_OV_toks_99.norm(dim=-1, keepdim=True)\n",
    "cos_sim_107 = W_EE_OV_toks_107_normed @ W_EE_OV_toks_107_normed.T\n",
    "cos_sim_99 = W_EE_OV_toks_99_normed @ W_EE_OV_toks_99_normed.T\n",
    "\n",
    "imshow(\n",
    "    t.stack([\n",
    "        cos_sim_embeddings,\n",
    "        cos_sim_unembeddings,\n",
    "        cos_sim_107,\n",
    "        cos_sim_99,\n",
    "    ]),\n",
    "    x = list(map(repr, tok_strs)),\n",
    "    y = list(map(repr, tok_strs)),\n",
    "    title = \"Cosine similarity of variants of ' pier'\",\n",
    "    facet_col = 0,\n",
    "    facet_labels = [\"Effective embeddings\", \"Unembeddings\", \"W_OV output (10.7)\", \"W_OV output (9.9)\"],\n",
    "    border = True,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "# W_EE_OV_normed = W_EE_OV / W_EE_OV.std(dim=-1, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create OV and QK circuits Streamlit page\n",
    "\n",
    "I need to save the following things:\n",
    "\n",
    "* The QK and OV matrices for head 10.7 and 11.10\n",
    "* The extended embedding and unembedding matrices\n",
    "* The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_store = {\n",
    "    \"tokenizer\": model.tokenizer,\n",
    "    \"W_V_107\": model.W_V[10, 7],\n",
    "    \"W_O_107\": model.W_O[10, 7],\n",
    "    \"W_V_1110\": model.W_V[11, 10],\n",
    "    \"W_O_1110\": model.W_O[11, 10],\n",
    "    \"W_Q_107\": model.W_Q[10, 7],\n",
    "    \"W_K_107\": model.W_K[10, 7],\n",
    "    \"W_Q_1110\": model.W_Q[11, 10],\n",
    "    \"W_K_1110\": model.W_K[11, 10],\n",
    "    \"b_Q_107\": model.b_Q[10, 7],\n",
    "    \"b_K_107\": model.b_K[10, 7],\n",
    "    \"b_Q_1110\": model.b_Q[11, 10],\n",
    "    \"b_K_1110\": model.b_K[11, 10],\n",
    "    \"W_EE\": W_EE_dict[\"W_E (including MLPs)\"],\n",
    "    \"W_U\": model.W_U,\n",
    "}\n",
    "dict_to_store = {k: v.half() if isinstance(v, t.Tensor) else v for k, v in dict_to_store.items()}\n",
    "\n",
    "with gzip.open(_ST_HTML_PATH / f\"OV_QK_circuits.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_to_store, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate `explore_prompts` HTML plots for Streamlit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS,\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = True,\n",
    "    progress_bar = True,\n",
    "    restrict_computation = [\"LOSS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")].norm().item(), \n",
    "    model.W_U.T[model.to_single_token(\" Pier\")].norm().item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")],\n",
    "    model.W_U.T[model.to_single_token(\" Pier\")],\n",
    "    dim=-1\n",
    ").item()\n",
    "\n",
    "\n",
    "pier = model.W_U.T[model.to_single_token(\" pier\")]\n",
    "Pier = model.W_U.T[model.to_single_token(\" Pier\")]\n",
    "pier /= pier.norm()\n",
    "Pier /= Pier.norm()\n",
    "print(pier @ Pier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_U(s):\n",
    "    return model.W_U.T[model.to_single_token(s)]\n",
    "def W_EE0(s):\n",
    "    return W_EE_dict[\"W_E (only MLPs)\"][model.to_single_token(s)]\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return v1 @ v2 / (v1.norm() * v2.norm())\n",
    "\n",
    "print(f\"Unembeddings cosine similarity (Berkeley) = {cos_sim(W_U('keley'), W_U(' Berkeley')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (Berkeley)   = {cos_sim(W_EE0('keley'), W_EE0(' Berkeley')):.3f}\") \n",
    "print(\"\")\n",
    "print(f\"Unembeddings cosine similarity (pier) = {cos_sim(W_U(' pier'), W_U(' Pier')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (pier)   = {cos_sim(W_EE0(' pier'), W_EE0(' Pier')):.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE0(\" screen\") - W_EE0(\" screens\"),\n",
    "    W_EE0(\" device\") - W_EE0(\" devices\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE(\" computer\") - W_EE(\" computers\"),\n",
    "    W_EE(\" sign\") - W_EE(\" signs\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
