{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This file stores experiments for both old (filtering) and experimental new work on CSPA.\n",
    "\n",
    "The setup and semantic similarity sections must be run. But the rest of the sections can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pattern --no-dependencies\n",
    "# %pip install nltk\n",
    "# %pip install protobuf==3.20.0\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Note - I couldn't figure out how to fix local imports for a while. Solution ended up being to make sure that this version of `transformer_lens` is before my libraries in `sys.path` (hence I'm inserting it at position zero in the code below).\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens.rs.callum2.cspa.cspa_functions import (\n",
    "    FUNCTION_STR_TOKS,\n",
    "    get_cspa_results,\n",
    "    get_cspa_results_batched,\n",
    "    get_performance_recovered,\n",
    "    OVProjectionConfig, \n",
    "    QKProjectionConfig,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    parse_str,\n",
    "    parse_str_toks_for_printing,\n",
    "    parse_str_tok_for_printing,\n",
    "    ST_HTML_PATH,\n",
    "    process_webtext,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_plots import (\n",
    "    generate_scatter,\n",
    "    generate_loss_based_scatter,\n",
    "    show_graphs_and_summary_stats,\n",
    "    add_cspa_to_streamlit_page,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_result_mean,\n",
    "    get_model_results,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    generate_4_html_plots,\n",
    "    CSS,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_semantic_similarity import (\n",
    "    get_equivalency_toks,\n",
    "    get_related_words,\n",
    "    concat_lists,\n",
    "    make_list_correct_length,\n",
    "    create_full_semantic_similarity_dict,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = torch.Size([500, 1000])\n",
      "\n",
      "First prompt:\n",
      "<|endoftext|>Going into Election Day, few industries seemed in worse shape than America's private prisons. Prison populations, which had been rising for decades, were falling. In 2014, Corrections Corporation of America, the biggest private-prison company in the U.S., lost its contract to run Idaho's largest prison, after lawsuits relating to understaffing and violence that had earned the place the nickname Gladiator School. There were press exposés of shocking conditions in the industry and signs of a policy shift toward it. In April, Hillary Clinton said, \"We should end private prisons.\" In August, the Justice Department said that private federal prisons were less safe and less secure than government-run ones. The same month, the department announced that it would phase out the use of private prisons at the federal level. Although most of the private-prison industry operates on the state level (immigrant-detention centers are its other big business), the news sent C.C.A.'s stock down by thirty-five per cent.\n",
      "\n",
      "Donald Trump's victory changed all that: within days, C.C.A.'s stock had jumped forty-seven per cent. His faith in privatization is no secret, and prison companies aren't the only ones rubbing their hands. The stock price of for-profit schools has also rocketed. Still, the outlook for private prisons is particularly rosy, because many Trump policies work to their benefit. The Justice Department's plan to phase out private prisons will likely be scrapped, and a growing bipartisan movement for prison and sentencing reform is about to run up against a President who campaigned as a defender of \"law and order.\" Above all, Trump's hard-line position on immigration seems certain to fill detention centers, one of the biggest money spinners for private-prison operators.\n",
      "\n",
      "The boom in private prisons in the past two decades was part of a broader privatization trend, fuelled by a belief in the superior efficiency of the private sector. But privatizing prisons makes little economic or political sense. Some studies find private prisons to be less cost-effective than government ones, some more, and further studies suggest that any savings are likely the result of cutting corners. In a study of prisons in nine states, Chris Petrella, a lecturer at Bates College, found that private ones avoid taking sick and elderly inmates, since health care is a huge expense for prisons. They employ a younger, less well trained, and less well paid workforce and have higher inmate-to-guard ratios, all of which saves money but also makes prisons more dangerous. When you consider that the government still spends money monitoring private prisons, and that it's stuck running the parts of the system that private companies thought were money losers, the case that private prisons save money looks shaky.\n",
      "\n",
      "Even if they did, the ethical cost would be too high. Imprisoning people is one of the weightiest things that government does, yet outsourcing imprisonment means that treatment of inmates is shaped by bottom-line considerations. This has led to understaffing, inadequate mental-health care, and, in some cases, inadequate meals. Worse, private prisons have an obvious incentive to keep people inside as long as possible. Last year, Anita Mukherjee, an assistant professor of actuarial science at the University of Wisconsin, studied Mississippi's prison system, and found that people in private prisons received many more \"prison conduct violations\" than those in government-run ones. This made it harder for them to get parole, and, on average, they served two to three more months of prison time.\n",
      "\n",
      "The perversities of profit-driven prison policy don't end there. The need for inmates leads companies, in effect, to lobby state and federal governments to maintain the current system of mass incarceration. Government-run prisons aren't blameless here—prison-guard unions lobby for longer sentences and tougher laws—but the private companies know how to throw their weight around, and they benefit from strong local support, as they are often in rural towns without many other sources of jobs or tax revenue. Since the mid-aughts, the industry has spent tens of millions of dollars lobbying on the state and federal levels. Its successes include an Arizona law that required cops to stop suspected undocumented immigrants, major increases in spending on immigration enforcement, and the blocking of congressional efforts to ban private prisons.\n",
      "\n",
      "It's become common to speak of \"the prison-industrial complex,\" and the analogy to the military-industrial complex is a good one: in both cases, government spending helps fund very profitable businesses, which, in turn, lobby legislators and regulators to keep the funds flowing. Just as we spend billions on weapons systems that we may not need, so, too, we jail more people than we need for longer than necessary, because it keeps someone's balance sheet healthy. In recent years, an unlikely coalition of conservatives and liberals had made some progress in weakening this system, going after policies like mandatory sentences\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500 # 80 for viz\n",
    "SEQ_LEN = 1000 # 61 for viz\n",
    "\n",
    "current_batch_size = 17 # These are smaller values we use for vizualization since only these appear on streamlit\n",
    "current_seq_len = 61\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED, indices = process_webtext(seed=6, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, model=model, verbose=True, return_indices=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded semantic similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use semantic similarity in the maintext for CSPA, so understanding this can be skipped if you're only interested in that.\n",
    "\n",
    "This will take tokens, and return the tokens of semantically similar words. \n",
    "\n",
    "There are 3 categories of semantically similar tokens `s*` for any given token `s`:\n",
    "\n",
    "1. Equivalence relations - this captures things like plurals, tokenization, capitalization.\n",
    "2. Superstrings - for instance, of you have `\"keley\"` this gives you `\" Berkeley\"`.\n",
    "3. Substrings - for instance, of you have `\" Berkeley\"` this gives you `\"keley\"`.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "1. For each token, generate all (1), and then see which ones actually split into multiple tokens (these become (3)).\n",
    "2. Iterate through this entire dict to generate (2)s for every token (this is basically like flipping the arrows in the other direction).\n",
    "\n",
    "### Problems with this method\n",
    "\n",
    "There are 4 problems with this method. I think (1) and (3) are the most problematic.\n",
    "\n",
    "1. The pluralization isn't sufficiently flexible, and it'll miss out on categories of things, for example:\n",
    "    * `\" write\"` and `\" writing\"` and `\" writer\"`\n",
    "    * `\" rental\"` and `\" rented\"` and `\" renting\"`\n",
    "    * (OV and QK circuits show that these do suppress each other)\n",
    "    * Possible solution - more hardcoded rules?\n",
    "2. Misses some important things which aren't semantically similar as we've defined it, e.g. `1984` and `1985` aren't semantically similar (OV and QK circuits show that they do suppress each other)\n",
    "    * Possible solution - ???\n",
    "\n",
    "However, this maybe isn't worth further optimization, because it doesn't marginally improve the results by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SEMANTICITY = True\n",
    "\n",
    "if USE_SEMANTICITY:\n",
    "    from pattern.text.en import conjugate, PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE, PLURAL, SINGULAR\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    MY_TENSES = [PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE]\n",
    "    MY_NUMBERS = [PLURAL, SINGULAR]\n",
    "    from nltk.corpus import wordnet\n",
    "    nltk.download('wordnet')\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SEMANTICITY:\n",
    "    cspa_semantic_dict = pickle.load(open(ST_HTML_PATH.parent.parent / \"cspa/cspa_semantic_dict_full.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    warnings.warn(\"Not using semanticity unlike old notebook versions!\")\n",
    "    cspa_semantic_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Related words</h2>This doesn't include tokenization fragments; it's just linguistic."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Berkeley']\n",
      "(Worked on second try!)\n",
      "['pier']\n",
      "['pie', 'pies', 'pier']\n",
      "['ring', 'rings', 'ringing']\n",
      "['device', 'devices']\n",
      "['robot', 'robots', 'robotics', 'robotic']\n",
      "['w']\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<h2>Related words</h2>This doesn't include tokenization fragments; it's just linguistic.\"))\n",
    "\n",
    "for word in [\"Berkeley\", \"pier\", \"pie\", \"ring\", \"device\", \"robot\", \"w\"]:\n",
    "    try: print(get_related_words(word, model))\n",
    "    except: print(get_related_words(word, model)); print(\"(Worked on second try!)\") # maybe because it downloads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Equivalency words</h2>These are the words which will be included in the semantic similarity cluster, during CSPA."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Berkeley' -> ([' Berkeley'], ['Ber', 'keley'])\n",
      "   ' Pier' -> ([' pier', ' Pier'], [])\n",
      "   ' pier' -> ([' pier', ' Pier'], [])\n",
      "     'pie' -> (['pie', 'Pie', ' pie', ' pies', ' Pie'], [])\n",
      "   ' pies' -> (['pie', 'Pie', ' pies', ' pie', ' Pie'], [])\n",
      "   ' ring' -> (['ring', 'rings', 'Ring', ' ring', ' rings', ' Ring', ' Rings'], [])\n",
      " ' device' -> (['device', 'devices', 'Device', ' device', ' devices', ' Device', ' Devices'], [])\n",
      "  ' robot' -> ([' robot', ' robots', ' Robot', ' Robots'], [])\n",
      "       'w' -> (['w', 'ws', 'W', 'Ws', 'WS', ' w', ' W', ' WS'], [])\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<h2>Equivalency words</h2>These are the words which will be included in the semantic similarity cluster, during CSPA.\"))\n",
    "\n",
    "for tok in [\" Berkeley\", \" Pier\", \" pier\", \"pie\", \" pies\", \" ring\", \" device\", \" robot\", \"w\"]:\n",
    "    print(f\"{tok!r:>10} -> {get_equivalency_toks(tok, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                           Semantic similarity: bidirectional, superstrings, substrings                            </span>\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Source token  </span>┃<span style=\"font-weight: bold\"> All semantically related                                                                        </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ' Berkeley'   │ 1 bidirectional: [' Berkeley']                                                                  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 2 sub-tokens:    ['keley', 'Ber']                                                               │\n",
       "│               │                                                                                                 │\n",
       "│ 'keley'       │ 1 bidirectional: ['keley']                                                                      │\n",
       "│               │ 1 super-tokens:  [' Berkeley']                                                                  │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' University' │ 5 bidirectional: [' University', 'University', ' university', ' universities', ' Universities'] │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Mary'       │ 3 bidirectional: [' Mary', 'Mary', 'mary']                                                      │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Pier'       │ 2 bidirectional: [' Pier', ' pier']                                                             │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' pier'       │ 2 bidirectional: [' pier', ' Pier']                                                             │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ 'NY'          │ 4 bidirectional: ['NY', ' NY', 'ny', ' Ny']                                                     │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' ring'       │ 8 bidirectional: [' ring', 'ring', ' rings', 'rings', 'Ring', ' ringing', ' Ring', ' Rings']    │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' W'          │ 4 bidirectional: [' W', 'W', 'w', ' w']                                                         │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' device'     │ 7 bidirectional: [' device', 'device', ' devices', 'devices', 'Device', ' Device', ' Devices']  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' robot'      │ 7 bidirectional: [' robot', ' robotic', ' robots', ' robotics', ' Robot', ' Robotics', '        │\n",
       "│               │ Robots']                                                                                        │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' jump'       │ 8 bidirectional: [' jump', 'jump', ' jumps', 'Jump', ' jumper', ' jumping', ' jumped', ' Jump'] │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' driver'     │ 7 bidirectional: [' driver', 'driver', ' drivers', 'drivers', 'Driver', ' Driver', ' Drivers']  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Cairo'      │ 1 bidirectional: [' Cairo']                                                                     │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "└───────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                           Semantic similarity: bidirectional, superstrings, substrings                            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mSource token \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAll semantically related                                                                       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ' Berkeley'   │ 1 bidirectional: [' Berkeley']                                                                  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 2 sub-tokens:    ['keley', 'Ber']                                                               │\n",
       "│               │                                                                                                 │\n",
       "│ 'keley'       │ 1 bidirectional: ['keley']                                                                      │\n",
       "│               │ 1 super-tokens:  [' Berkeley']                                                                  │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' University' │ 5 bidirectional: [' University', 'University', ' university', ' universities', ' Universities'] │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Mary'       │ 3 bidirectional: [' Mary', 'Mary', 'mary']                                                      │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Pier'       │ 2 bidirectional: [' Pier', ' pier']                                                             │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' pier'       │ 2 bidirectional: [' pier', ' Pier']                                                             │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ 'NY'          │ 4 bidirectional: ['NY', ' NY', 'ny', ' Ny']                                                     │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' ring'       │ 8 bidirectional: [' ring', 'ring', ' rings', 'rings', 'Ring', ' ringing', ' Ring', ' Rings']    │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' W'          │ 4 bidirectional: [' W', 'W', 'w', ' w']                                                         │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' device'     │ 7 bidirectional: [' device', 'device', ' devices', 'devices', 'Device', ' Device', ' Devices']  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' robot'      │ 7 bidirectional: [' robot', ' robotic', ' robots', ' robotics', ' Robot', ' Robotics', '        │\n",
       "│               │ Robots']                                                                                        │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' jump'       │ 8 bidirectional: [' jump', 'jump', ' jumps', 'Jump', ' jumper', ' jumping', ' jumped', ' Jump'] │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' driver'     │ 7 bidirectional: [' driver', 'driver', ' drivers', 'drivers', 'Driver', ' Driver', ' Drivers']  │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "│ ' Cairo'      │ 1 bidirectional: [' Cairo']                                                                     │\n",
       "│               │ 0 super-tokens:  []                                                                             │\n",
       "│               │ 0 sub-tokens:    []                                                                             │\n",
       "│               │                                                                                                 │\n",
       "└───────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_SEMANTICITY:\n",
    "    table = Table(\"Source token\", \"All semantically related\", title=\"Semantic similarity: bidirectional, superstrings, substrings\") #  \"Top 3 related\" in the middle\n",
    "\n",
    "    str_toks = [\" Berkeley\", \"keley\", \" University\", \" Mary\", \" Pier\", \" pier\", \"NY\", \" ring\", \" W\", \" device\", \" robot\", \" jump\", \" driver\", \" Cairo\"]\n",
    "    print_cutoff = 105 # 70\n",
    "    def cutoff(s):\n",
    "        if len(s_str := str(s)) >= print_cutoff: return s_str[:print_cutoff-4] + ' ...'\n",
    "        else: return s_str\n",
    "\n",
    "    for str_tok in str_toks:\n",
    "        top3_sim = \"\\n\".join(list(map(repr, concat_lists(cspa_semantic_dict[str_tok])[:3])))\n",
    "        bidir, superstr, substr = cspa_semantic_dict[str_tok]\n",
    "        all_sim = \"\\n\".join([\n",
    "            cutoff(f\"{len(bidir)} bidirectional: {bidir}\"),\n",
    "            cutoff(f\"{len(superstr)} super-tokens:  {superstr}\"),\n",
    "            cutoff(f\"{len(substr)} sub-tokens:    {substr}\"),\n",
    "        ]) + \"\\n\"\n",
    "        table.add_row(repr(str_tok), all_sim) # top3_sim in the middle\n",
    "\n",
    "    rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's save a mean for later use...\n",
    "\n",
    "result_mean = get_result_mean([(10, 7), (11, 10)], DATA_TOKS[:100, :], model, verbose=True)\n",
    "# t.save(result_mean, f\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/result_mean.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Filtering CSPA code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings: `K_u = 0.05`, no semantic similarity, batch size = 100\n",
    "\n",
    "We should recover ~76.9% KL Divergence, as reported in the paper draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, as long as SEQ_LEN large, small BATCH_SIZE gives quite good estimates\n",
    "QK_OV_BATCH_SIZE = 20\n",
    "QK_OV_SEQ_LEN = 600\n",
    "\n",
    "cspa_results_qk_ov = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:QK_OV_BATCH_SIZE, :QK_OV_SEQ_LEN],\n",
    "    max_batch_size = 1, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"ov\", \"qk\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    "    computation_device = \"cpu\", # device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The performance recovered is...\",\n",
    "    get_performance_recovered(cspa_results_qk_ov), # 79.6\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show several nice visuals for every CSPA run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs_and_summary_stats(cspa_results_qk_ov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>KL and Loss seems to be measuring fairly different things</h3>\n",
    "\n",
    "(Though this section isn't very exciting).\n",
    "\n",
    "Answering the question: \"Is there a correlation between the absolute value of 10.7 effect on loss and 10.7's effect on increasing KL Divergence from the model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIZ_BATCH_SIZE = QK_OV_BATCH_SIZE # change to current_batch_size for a smaller subset that can be streamlit vizualized\n",
    "VIZ_SEQ_LEN = SEQ_LEN -1 # change to current_seq_len for a smaller subset that can be streamlit vizualized\n",
    "BATCH_INDICES = torch.arange(VIZ_BATCH_SIZE) # (change to indices to filter for correct vals on streamlit)\n",
    "\n",
    "batch_indices = (BATCH_INDICES[:VIZ_BATCH_SIZE].unsqueeze(-1) + torch.zeros(VIZ_SEQ_LEN).unsqueeze(0))\n",
    "seq_indices = (torch.zeros(VIZ_BATCH_SIZE).unsqueeze(-1) + torch.arange(VIZ_SEQ_LEN).unsqueeze(0))\n",
    "\n",
    "my_dict = {\n",
    "    \"Mean Ablate 10.7 Loss - Normal Loss\": cspa_results_qk_ov[\"loss_ablated\"].clone().cpu() - cspa_results_qk_ov[\"loss\"].clone().cpu(),\n",
    "    \"Mean Ablation KL Divergence to Model\": cspa_results_qk_ov[\"kl_div_ablated_to_orig\"][:, :-1].clone().cpu(),\n",
    "    \"CSPA Loss - Normal Loss\": cspa_results_qk_ov[\"loss_cspa\"].clone().cpu() - cspa_results_qk_ov[\"loss\"].clone().cpu(),\n",
    "    \"CSPA KL\": cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].clone().cpu()[:, :-1],\n",
    "}\n",
    "\n",
    "for k in list(my_dict.keys()):\n",
    "    print(k)\n",
    "    print(my_dict[k].shape)\n",
    "\n",
    "    if len(my_dict[k].shape) == 2:\n",
    "        my_dict[k] = my_dict[k][:VIZ_BATCH_SIZE, :VIZ_SEQ_LEN].flatten()\n",
    "    else:\n",
    "        my_dict[k] = my_dict[k]\n",
    "\n",
    "    print(my_dict[k].shape)\n",
    "\n",
    "my_dict[\"Batch Indices\"] = batch_indices.flatten().float()\n",
    "my_dict[\"Seq Indices\"] = seq_indices.flatten().float()\n",
    "\n",
    "print(my_dict[\"Batch Indices\"].shape)\n",
    "print(my_dict[\"Seq Indices\"].shape)\n",
    "\n",
    "df = pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings; warnings.warn(\"Check out color when working\")\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(\n",
    "\n",
    "color = \"CSPA KL\"\n",
    "px.scatter(\n",
    "    df,\n",
    "    x = \"Mean Ablate 10.7 Loss - Normal Loss\",\n",
    "    y = \"Mean Ablation KL Divergence to Model\",\n",
    "    hover_data = [\"Batch Indices\", \"Seq Indices\"],\n",
    "    color = color,\n",
    "    color_continuous_scale = \"Blues\" if \"KL\" in color else \"RdBu_r\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    range_color=((0.0, 0.09) if \"KL\" in color else None),\n",
    ").update_traces(\n",
    "    marker=dict(\n",
    "        line=dict(\n",
    "            color='black', # Border color\n",
    "            width=0.5  # Border width\n",
    "        )\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: the correlation is pretty weak. The good news is that there are very few cases where change in loss is ~0 and KL is large.\n",
    "\n",
    "The line of best fit (when we take absolute values) to 3DP is\n",
    "\n",
    "`Change in KL = 0.067 * (Absolute change in loss)`\n",
    "\n",
    "with no constant factor is nice, though R^2 = 0.34 is low (this isn't surprising, the quantities are measuring different things).\n",
    "\n",
    "With the following heatmap, we can see that indeed the spread of losses is a fair bit larger than the spread of KLs. None of this is wildly exciting though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 50\n",
    "\n",
    "x_std = df['CSPA Loss - Normal Loss'].std()\n",
    "y_std = df['CSPA KL'].std()\n",
    "\n",
    "x_min = - x_std # df['CSPA Loss - Normal Loss'].min()\n",
    "x_max = x_std # df['CSPA Loss - Normal Loss'].max()\n",
    "y_min = 0.0\n",
    "y_max = y_std # df['CSPA KL'].max()\n",
    "\n",
    "heatmap_vals = torch.zeros(Q, Q)\n",
    "\n",
    "for x_quantile in range(Q):\n",
    "    for y_quantile in range(Q):\n",
    "        x_subset = df['CSPA Loss - Normal Loss'] >= x_min + (x_max - x_min) * x_quantile / Q \n",
    "        x_subset = x_subset & (df['CSPA Loss - Normal Loss'] <= x_min + (x_max - x_min) * (x_quantile+1) / Q)\n",
    "        \n",
    "        y_subset = df['CSPA KL'] >= y_min + (y_max - y_min) * y_quantile / Q \n",
    "        y_subset = y_subset & (df['CSPA KL'] <= y_min + (y_max - y_min) * (y_quantile+1) / Q)\n",
    "\n",
    "        heatmap_size = (x_subset & y_subset).to_numpy().astype(\"int\").sum() / len(df['CSPA KL'])\n",
    "        heatmap_vals[x_quantile, y_quantile] = np.log(heatmap_size) # Can use log here...\n",
    "\n",
    "fig = imshow(\n",
    "    heatmap_vals[:, torch.arange(heatmap_vals.shape[0]-1, -1, -1)].T, # Does two things: makes axes the right way around, and in my opinion heatmaps x and y are the wrong way round\n",
    "    title = f\"Log Density of Points in CSPA Ranges\",\n",
    "    width = 500, \n",
    "    height = 500,\n",
    "    labels = {\"x\": \"CSPA Loss - Model Loss\", \"y\": \"CSPA KL\"},\n",
    "    x = [str(round(x_min + (x_max - x_min) * x_quantile / Q, 4)) for x_quantile in range(Q)],\n",
    "    y = [str(round(y_min + (y_max - y_min) * y_quantile / Q, 5)) for y_quantile in range(Q)][::-1],\n",
    "    # text_auto = \".2f\",\n",
    "    range_color=(heatmap_vals.min().item(), heatmap_vals.max().item()),\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    return_fig=True,\n",
    "    color_continuous_midpoint=None,\n",
    ")\n",
    "\n",
    "# Set background color to white\n",
    "fig.update_layout(\n",
    "    paper_bgcolor='rgba(255,255,255,255)',\n",
    "    plot_bgcolor='rgba(255,255,255,255)'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"{df['CSPA Loss - Normal Loss'].std()=} {df['CSPA KL'].std()=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `K_u = 0.05`, no semantic similarity, batch size = 500\n",
    "\n",
    "(Skipped by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SLOW_CSPA = False\n",
    "\n",
    "if RUN_SLOW_CSPA:\n",
    "    cspa_results_qk_ov = get_cspa_results_batched(\n",
    "        model = model,\n",
    "        toks = DATA_TOKS[:, :], # [:50],\n",
    "        max_batch_size = 5, # 50,\n",
    "        negative_head = (10, 7),\n",
    "        interventions = [\"qk\", \"ov\"],\n",
    "        K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "        K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "        only_keep_negative_components = True,\n",
    "        semantic_dict = cspa_semantic_dict,\n",
    "        result_mean = result_mean,\n",
    "        use_cuda = False,\n",
    "        verbose = True,\n",
    "        compute_s_sstar_dict = False,\n",
    "    )\n",
    "\n",
    "    # fig_dict = generate_scatter(cspa_results_qk_ov, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "    fig_loss_line = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"loss\")\n",
    "    fig_loss_line_kl = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"kl-div\")\n",
    "\n",
    "    kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean()\n",
    "    kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean()\n",
    "\n",
    "    print(f\"Mean KL divergence from ablated to original: {kl_div_ablated_to_orig:.4f}\")\n",
    "    print(f\"Mean KL divergence from CSPA to original: {kl_div_cspa_to_orig:.4f}\")\n",
    "    print(f\"Ratio = {kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "    print(f\"Performance explained = {1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "    ma_max = fig_loss_line_kl.data[0].x[-1]\n",
    "    cspa_max = fig_loss_line_kl.data[0].y[-1]\n",
    "    print(f\"Most extreme quantile: fraction explained = 1 - ({cspa_max:.3f}/{ma_max:.3f}) = {1 - cspa_max/ma_max:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Projection CSPA\n",
    "\n",
    "The hardest part of the projection CSPA setup is *likely* the query direction (for example, in IOI this caused us a massive headache).\n",
    "\n",
    "So I tried to work on just doing query projections first (by setting `k_direction=None` and `ov_projection_config = None` in the below cells).\n",
    "\n",
    "I can only get to 64% of the KL explained :( and it's with a pretty cursed setup. I tried lots of different setups and this seems much better than others\n",
    "\n",
    "1. For every key token, project the query token onto the subspace of $\\mathbb{R}^{d_\\text{model}}$ spanned by the unembedding vector for the key token (`q_direction=\"unembedding\"`), and the other 7 semantically similar tokens to it (see `K_unembedding=8`)\n",
    "2. After doing this projection, double this query input vector (`q_input_multiplier=2.0`)\n",
    "3. After calculating attention scores, manually set all BOS attention scores to the exact value needed so the same attention to BOS is achieved (`mantain_bos_attention=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, as long as SEQ_LEN large, small BATCH_SIZE gives quite good estimates (experiments about this deleted, too in the weeds)\n",
    "Q_PROJECTION_BATCH_SIZE = 20\n",
    "Q_PROJECTION_SEQ_LEN = 300\n",
    "\n",
    "qk_projection_config = QKProjectionConfig(\n",
    "    q_direction=\"layer9_heads\",\n",
    "    actually_project=False,\n",
    "    k_direction=None,\n",
    "    q_input_multiplier=1.0,\n",
    "    use_same_scaling=False,\n",
    "    mantain_bos_attention=False,\n",
    "    model = model,\n",
    "    save_scores = True,\n",
    "    save_scaled_resid_pre = True,\n",
    ")\n",
    "\n",
    "# ov_projection_config = OVProjectionConfig()\n",
    "ov_projection_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_q_projection = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:Q_PROJECTION_BATCH_SIZE, :Q_PROJECTION_SEQ_LEN],\n",
    "    max_batch_size = 1,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [],\n",
    "    qk_projection_config=qk_projection_config,\n",
    "    ov_projection_config=ov_projection_config,\n",
    "    K_unembeddings = 1.0,\n",
    "    K_semantic = 8, # Be very careful making this big... very slow...\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    "    computation_device = \"cpu\",\n",
    ")\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance recovered is... 0.6687323800128029\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The performance recovered is...\",\n",
    "    get_performance_recovered(cspa_results_q_projection), # ~64\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does this metric do for other heads?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the paper details results for the filtering version of CSPA. Currently this is a bit of a mess since I used it to try and develop a projection version of CSPA.\n",
    "\n",
    "It turns out that we get some of the best *relative* results when we turn on the OV Projection, too.\n",
    "\n",
    "We'll also just do `K_semantic = 1` here, as larger values slow stuff down a lot. As a datapoint, L10H7 with the 2x-query-unembedding-projection and `K_semantic=1` (i.e this experimental setup without OV projection too) gets 57% KL recovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean = get_result_mean([\n",
    "    (layer, head)\n",
    "    for layer in [8, 9, 10, 11] for head in range(12)\n",
    "], DATA_TOKS[:100, :], model, verbose=True)\n",
    "\n",
    "qk_projection_config = QKProjectionConfig( # Not good, we really don't get strong results here...\n",
    "    # q_direction=\"use_copying_as_query\",\n",
    "    q_direction=\"unembedding\",\n",
    "    q_input_multiplier=2.0,\n",
    "    use_same_scaling=False,\n",
    "    mantain_bos_attention=True,\n",
    "    # projection_directions = \"earlier_heads\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "ov_projection_config = OVProjectionConfig()\n",
    "# ov_projection_config = None\n",
    "\n",
    "# qk_projection_config = QKProjectionConfig(\n",
    "#     q_direction=\"unembedding\",\n",
    "#     k_direction=None,\n",
    "#     q_input_multiplier=1.0,\n",
    "#     use_same_scaling=False,\n",
    "#     mantain_bos_attention=True,\n",
    "#     # projection_directions = \"earlier_heads\",\n",
    "#     model = model, \n",
    "# )\n",
    "# result_mean = get_result_mean([(10, 7), (11, 10)], DATA_TOKS[:100, :], model, verbose=True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcd2b7d33804597ab335967ff7de8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Layer 8 0 KL performance explained: 0.036\n",
      "Head Layer 8 1 KL performance explained: 0.062\n",
      "Head Layer 8 2 KL performance explained: 0.050\n",
      "Head Layer 8 3 KL performance explained: 0.064\n",
      "Head Layer 8 4 KL performance explained: 0.008\n",
      "Head Layer 8 5 KL performance explained: 0.003\n",
      "Head Layer 8 6 KL performance explained: 0.042\n",
      "Head Layer 8 7 KL performance explained: 0.014\n",
      "Head Layer 8 8 KL performance explained: 0.115\n",
      "Head Layer 8 9 KL performance explained: 0.017\n",
      "Head Layer 8 10 KL performance explained: 0.088\n",
      "Head Layer 8 11 KL performance explained: 0.177\n",
      "Head Layer 9 0 KL performance explained: 0.153\n",
      "Head Layer 9 1 KL performance explained: 0.131\n"
     ]
    }
   ],
   "source": [
    "kl_results = t.zeros(2, 4, 12).to(device)\n",
    "loss_results = kl_results.clone()\n",
    "normed_loss_results = kl_results.clone()\n",
    "non_normed_loss_results = kl_results.clone()\n",
    "squared_loss_results = kl_results.clone()\n",
    "\n",
    "for i, only_keep_negative_components in enumerate([True, False]):\n",
    "\n",
    "    for layer, head in tqdm(list(itertools.product([8, 9, 10, 11], range(12)))):\n",
    "\n",
    "        # Usage with your function\n",
    "        cspa_results_qk_ov=get_cspa_results_batched(model=model, toks=DATA_TOKS[-50:,:200], max_batch_size=10, negative_head=(layer,head), qk_projection_config=qk_projection_config, ov_projection_config=ov_projection_config, interventions=[], K_unembeddings=1.0, K_semantic=1, semantic_dict=cspa_semantic_dict, result_mean=result_mean, use_cuda=True, verbose=False, compute_s_sstar_dict=False, computation_device=None)\n",
    "\n",
    "        kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean().item()\n",
    "        kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean().item()\n",
    "\n",
    "        diff_of_loss_ablated_to_orig = (cspa_results_qk_ov[\"loss_ablated\"] - cspa_results_qk_ov[\"loss\"])\n",
    "        squared_loss_diff = (diff_of_loss_ablated_to_orig**2).mean().item()\n",
    "        normed_loss_diff = diff_of_loss_ablated_to_orig.abs().mean().item()\n",
    "        non_normed_loss_diff = diff_of_loss_ablated_to_orig.mean().item()\n",
    "\n",
    "        diff_of_loss_cspa_to_orig = (cspa_results_qk_ov[\"loss_cspa\"] - cspa_results_qk_ov[\"loss\"])\n",
    "        normed_cspa_loss_diff = diff_of_loss_cspa_to_orig.abs().mean().item() \n",
    "        squared_cspa_loss_diff = (diff_of_loss_cspa_to_orig**2).mean().item()\n",
    "        non_normed_cspa_loss_diff = diff_of_loss_cspa_to_orig.mean().item()\n",
    "\n",
    "        kl_performance_explained = 1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig\n",
    "        kl_results[i, layer - 8, head] = kl_performance_explained\n",
    "        print(f\"Head Layer {layer} {head} KL performance explained: {kl_performance_explained:.3f}\")\n",
    "\n",
    "        normed_loss_performance_explained = 1 - normed_cspa_loss_diff / normed_loss_diff\n",
    "        normed_loss_results[i, layer - 8, head] = normed_loss_performance_explained\n",
    "\n",
    "        squared_loss_performance_explained = 1 - squared_cspa_loss_diff / squared_loss_diff\n",
    "        squared_loss_results[i, layer - 8, head] = squared_loss_performance_explained\n",
    "\n",
    "        non_normed_loss_performance_explained = 1 - non_normed_cspa_loss_diff / non_normed_loss_diff\n",
    "        non_normed_loss_results[i, layer - 8, head] = non_normed_loss_performance_explained\n",
    "\n",
    "    break # We're currently not thinking much about the non-negative components\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plots:\")\n",
    "for results_name, results in zip([\"KL\", \"Net effect on loss\", \"Absolute difference in loss\", \"Squared effect on loss\"], [kl_results, non_normed_loss_results, normed_loss_results, squared_loss_results], strict=True):\n",
    "    imshow(\n",
    "        results,\n",
    "        facet_col = 0,\n",
    "        facet_labels = [\"Only keep negative components\", \"Keep negative and positive components\"],\n",
    "        title = f\"{results_name} performance of head explained by CSPA\",\n",
    "        width = 1800, \n",
    "        height = 450,\n",
    "        labels = {\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "        y = [str(i) for i in [8, 9, 10, 11]],\n",
    "        text_auto = \".2f\",\n",
    "        range_color=[0,1],\n",
    "        color_continuous_scale=\"Blues\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual code which appears on the dedicated streamlit page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_qk_ov, s_sstar_pairs_qk_ov = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS, # [:50],\n",
    "    max_batch_size = 60, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"qk\", \"ov\"],\n",
    "    K_unembeddings = 5,\n",
    "    K_semantic = 1,\n",
    "    only_keep_negative_components = True,\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = True,\n",
    ")\n",
    "# TODO - figure out where the bottleneck is via line profiler. I thought it was projections, but now it seems like this is not the case\n",
    "# Seems like it's this func: get_top_predicted_semantically_similar_tokens\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f func func(arg, kwarg=kwarg)\n",
    "\n",
    "fig_dict = generate_scatter(cspa_results_qk_ov, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "fig_loss_line = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"loss\")\n",
    "fig_loss_line_kl = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"kl-div\")\n",
    "\n",
    "kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean()\n",
    "kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean()\n",
    "print(f\"Mean KL divergence from ablated to original: {kl_div_ablated_to_orig:.4f}\")\n",
    "print(f\"Mean KL divergence from CSPA to original: {kl_div_cspa_to_orig:.4f}\")\n",
    "print(f\"Ratio = {kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "print(f\"Performance explained = {1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CSPA to the Streamlit page (\"Browse Examples\")\n",
    "\n",
    "This code adds the CSPA plots to the HTML plots for the Streamlit page. It creates a 5th tab called `CSPA`, and adds to the logit and DLA plots in the second tab (the latter is mainly for our use, while we're iterating on and improving the CSPA code).\n",
    "\n",
    "I've added to this code in a pretty janky way, so that it can show more than one CSPA plot stacked on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results, s_sstar_pairs = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:48, :61], # [:50],\n",
    "    max_batch_size = 2, # 50,\n",
    "    negative_head = (10, 1),\n",
    "    interventions = [\"qk\", \"ov\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    only_keep_negative_components = False,\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = True,\n",
    "    return_dla = True,\n",
    "    return_logits = True,\n",
    "    keep_self_attn = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cspa_to_streamlit_page(\n",
    "    cspa_results = cspa_results,\n",
    "    s_sstar_pairs = s_sstar_pairs,\n",
    "    html_plots_filename = f\"GZIP_HTML_PLOTS_b48_s61.pkl\",\n",
    "    data_str_toks_parsed = [s[:61] for s in DATA_STR_TOKS_PARSED[:48]],\n",
    "    toks_for_doing_DLA = DATA_TOKS[:48, :61],\n",
    "    model = model,\n",
    "    verbose = True,\n",
    "    # test_idx = 36,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_of_toks(\n",
    "    toks1: List[str],\n",
    "    toks2: List[str],\n",
    "):\n",
    "    U1 = model.W_U.T[model.to_tokens(toks1, prepend_bos=False).squeeze()]\n",
    "    U2 = model.W_U.T[model.to_tokens(toks2, prepend_bos=False).squeeze()]\n",
    "\n",
    "    if U1.ndim == 1: U1 = U1.unsqueeze(0)\n",
    "    if U2.ndim == 1: U2 = U2.unsqueeze(0)\n",
    "\n",
    "    U1_normed = U1 / t.norm(U1, dim=-1, keepdim=True)\n",
    "    U2_normed = U2 / t.norm(U2, dim=-1, keepdim=True)\n",
    "\n",
    "    imshow(\n",
    "        U1_normed @ U2_normed.T,\n",
    "        title = \"Cosine similarity of unembeddings\",\n",
    "        x = toks2,\n",
    "        y = toks1,\n",
    "    )\n",
    "\n",
    "cos_sim_of_toks(\n",
    "    [\" stuff\"],\n",
    "    [\" devices\", \" phones\", \" screens\", \" device\", \" phone\", \" Android\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the code for \"love and war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean_as_tensor = t.load(ST_HTML_PATH / \"result_mean.pt\")\n",
    "result_mean = {(10, 7): result_mean_as_tensor[0], (11, 10): result_mean_as_tensor[1]}\n",
    "\n",
    "prompt = \"I picked up the first box. I picked up the second box. I picked up the third and final box.\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "# Parse the string tokens for printing\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "model_results = get_model_results(\n",
    "    model,\n",
    "    toks=toks,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    result_mean=result_mean,\n",
    "    verbose=False\n",
    ")\n",
    "HTML_PLOTS_NEW = generate_4_html_plots(\n",
    "    model=model,\n",
    "    data_toks=toks,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    model_results=model_results,\n",
    "    save_files=False,\n",
    ")\n",
    "cspa_results, s_sstar_pairs = get_cspa_results(\n",
    "    model=model,\n",
    "    toks=toks,\n",
    "    negative_head=(10, 7), #  this currently doesn't do anything; it's always 10.7\n",
    "    components_to_project=[\"o\"],\n",
    "    K_unembeddings=5,\n",
    "    K_semantic=3,\n",
    "    semantic_dict=cspa_semantic_dict,\n",
    "    effective_embedding=\"W_E (including MLPs)\",\n",
    "    result_mean=result_mean,\n",
    "    use_cuda=False,\n",
    "    return_dla=True,\n",
    ")\n",
    "HTML_PLOTS_NEW = add_cspa_to_streamlit_page(\n",
    "    cspa_results=cspa_results,\n",
    "    s_sstar_pairs=s_sstar_pairs,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    model=model,\n",
    "    HTML_PLOTS=HTML_PLOTS_NEW,\n",
    "    toks_for_doing_DLA=toks,\n",
    "    verbose=False,\n",
    "    test_idx=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular CSPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OV_BATCH_SIZE = 50\n",
    "\n",
    "cspa_results_qk = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:OV_BATCH_SIZE],\n",
    "    max_batch_size = 1, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"qk\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = False,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    ")\n",
    "clear_output() # Weird cell, it hogs space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs_and_summary_stats(cspa_results_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    get_performance_recovered(cspa_results_q_projection, verbose=True),\n",
    "    Q_PROJECTION_BATCH_SIZE,\n",
    "    Q_PROJECTION_SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating individual prompts\n",
    "\n",
    "It seems important to dive down into why the projection CSPA only gets ~64%. These cells do this, though TODO Arthur move this out of here, it's better placed in an experimental file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_BATCH_SIZE = 18\n",
    "PLOT_SEQ_LEN = 50\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"cspa_kl\": to_numpy(cspa_results_q_projection[\"kl_div_cspa_to_orig\"][:PLOT_BATCH_SIZE, :PLOT_SEQ_LEN].flatten()),\n",
    "    \"ablated_kl\": to_numpy(cspa_results_q_projection[\"kl_div_ablated_to_orig\"][:PLOT_BATCH_SIZE, :PLOT_SEQ_LEN].flatten()),\n",
    "    # \"indices\": sum(list(enumerate([[seq_idx for seq_idx in range(Q_PROJECTION_SEQ_LEN)] for _ in range(Q_PROJECTION_BATCH_SIZE)]))),\n",
    "    \"hover_data\" : [str((indices[batch_idx], seq_idx, \"which is\", batch_idx)) for batch_idx in range(PLOT_BATCH_SIZE) for seq_idx in range(PLOT_SEQ_LEN)],\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x = \"cspa_kl\",\n",
    "    y = \"ablated_kl\",\n",
    "    hover_data = \"hover_data\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# Save figure\n",
    "# fig.write_image(\"fig_of_points.pdf\")\n",
    "\n",
    "# Studying some failures of projection, across several different runs. Maybe mean ablation means different things are differently destructive? Scary if so!\n",
    "# (8, 49), (1, 40), (18, 10), (8, 35)\n",
    "# (8, 49): Covered below; we pick \" homeowners\" and \" rules\" but the model attends to \" Aurora\" (and \" Council\"). Note this is on a comma. Maybe we surpress capital letters after that?\n",
    "# (1, 40): We should attend to private. But instead we attend to prisons. Really weird though, as private is higher predicted, and OV circuit analysis didn't seem to help\n",
    "# (18, 10): with\" -> \"TPP\" should be suppressed. But \" Lee\" is predicted more.\n",
    "# (8, 35): requiring\" -> \" rentals\" is actually attended to. But \" homeowners\" is predicted more, and hence this ablation picks it more.\n",
    "# (33, 42): model attends to \" remove\" -> \" Blackberry\", we attend to \" remove\" -> \" ban\". And Blackberry is the top prediction! Ban is in fact third\n",
    "# (12, 26): we have half the amount of attention to ' Art' as we should. Context is \"\\n\\n\" -> \" Art\" and the only higher predicted thing is \" This\" (which is likely a function word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some examples of failures?\n",
    "\n",
    "def print_attentions(batch_idx, seq_idx):\n",
    "    print(sorted(list(enumerate(cspa_results_q_projection[\"pattern\"][batch_idx, seq_idx, :seq_idx].tolist())), key=lambda x: x[1], reverse=True))    \n",
    "\n",
    "# cspa_results_q_projection[\"pattern\"][13, 14, :15] # indices[13] = 35. of -> Neil. We put 90% prob on Neil, but for some reason the model also suppresses \"About\"\n",
    "print_attentions(3, 49) # indices[3] = 8. We pick \" homeowners\" and \" rules\" but the model attends to \" Aurora\" and \" Council\". Note this is on a comma. Maybe we surpress capital letters after that?\n",
    "# print_attentions(12, 26) # indices[12] = 34. We put too much attention on \"This\" rather than full 50% on \"Art\". Failure due to not using semantically similar tokens\n",
    "# print_attentions(11, 42) # indices[11] = 33. We should attend to Blackberry more. No idea how \" meetings\" is almost the same amount of attention...\n",
    "print_attentions(0, 40) # ... [1] we should put weight on \" prisons\" not private...\n",
    "\n",
    "print_attentions(5, 10)\n",
    "print_attentions(3, 35)\n",
    "print_attentions(11, 42) \n",
    "print_attentions(15, 37)\n",
    "print_attentions(12, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a sanity check on some cases where we are doing well, to check that really this \"surprising attention\" bug is real.\n",
    "print_attentions(2, 28) \n",
    "print(\"Yeah we absolutely nailed the ~80% attention here\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what's going wrong? Are our attention scores too high on incorrect, or too low on correct?\n",
    "\n",
    "First let's survey the cases where things go well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_q_projection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the cases where ablated KL >0.1 and CSPA KL <0.05\n",
    "\n",
    "index_relevant = (cspa_results_q_projection[\"kl_div_ablated_to_orig\"] > 0.1) &  (cspa_results_q_projection[\"kl_div_cspa_to_orig\"] < 0.05)\n",
    "indices_raw = np.nonzero(to_numpy(index_relevant.flatten()))[0]\n",
    "indices = list(zip(\n",
    "    indices_raw//index_relevant.shape[-1],\n",
    "    indices_raw%index_relevant.shape[-1],\n",
    "    strict=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
