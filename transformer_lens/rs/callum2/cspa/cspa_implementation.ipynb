{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This file stores experiments for both old (filtering) and experimental new work on CSPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pattern --no-dependencies\n",
    "# %pip install nltk\n",
    "# %pip install protobuf==3.20.0\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "# Note - I couldn't figure out how to fix local imports for a while. Solution ended up being to make sure that this version of `transformer_lens` is before my libraries in `sys.path` (hence I'm inserting it at position zero in the code below).\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from transformer_lens.rs.callum2.cspa.cspa_functions import (\n",
    "    FUNCTION_STR_TOKS,\n",
    "    get_cspa_results,\n",
    "    get_cspa_results_batched,\n",
    "    get_performance_recovered,\n",
    "    OVProjectionConfig, \n",
    "    QKProjectionConfig,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    parse_str,\n",
    "    parse_str_toks_for_printing,\n",
    "    parse_str_tok_for_printing,\n",
    "    ST_HTML_PATH,\n",
    "    process_webtext,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_plots import (\n",
    "    generate_scatter,\n",
    "    generate_loss_based_scatter,\n",
    "    show_graphs_and_summary_stats,\n",
    "    add_cspa_to_streamlit_page,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.model_results import (\n",
    "    get_result_mean,\n",
    "    get_model_results,\n",
    ")\n",
    "from transformer_lens.rs.callum2.generate_st_html.generate_html_funcs import (\n",
    "    generate_4_html_plots,\n",
    "    CSS,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_semantic_similarity import (\n",
    "    get_equivalency_toks,\n",
    "    get_related_words,\n",
    "    concat_lists,\n",
    "    make_list_correct_length,\n",
    "    create_full_semantic_similarity_dict,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500 # 80 for viz\n",
    "SEQ_LEN = 1000 # 61 for viz\n",
    "\n",
    "current_batch_size = 17 # These are smaller values we use for vizualization since only these appear on streamlit\n",
    "current_seq_len = 61\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED, indices = process_webtext(seed=6, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, model=model, verbose=True, return_indices=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded semantic similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use semantic similarity in the maintext for CSPA, so understanding this can be skipped if you're only interested in that.\n",
    "\n",
    "This will take tokens, and return the tokens of semantically similar words. \n",
    "\n",
    "There are 3 categories of semantically similar tokens `s*` for any given token `s`:\n",
    "\n",
    "1. Equivalence relations - this captures things like plurals, tokenization, capitalization.\n",
    "2. Superstrings - for instance, of you have `\"keley\"` this gives you `\" Berkeley\"`.\n",
    "3. Substrings - for instance, of you have `\" Berkeley\"` this gives you `\"keley\"`.\n",
    "\n",
    "How does this work?\n",
    "\n",
    "1. For each token, generate all (1), and then see which ones actually split into multiple tokens (these become (3)).\n",
    "2. Iterate through this entire dict to generate (2)s for every token (this is basically like flipping the arrows in the other direction).\n",
    "\n",
    "### Problems with this method\n",
    "\n",
    "There are 4 problems with this method. I think (1) and (3) are the most problematic.\n",
    "\n",
    "1. The pluralization isn't sufficiently flexible, and it'll miss out on categories of things, for example:\n",
    "    * `\" write\"` and `\" writing\"` and `\" writer\"`\n",
    "    * `\" rental\"` and `\" rented\"` and `\" renting\"`\n",
    "    * (OV and QK circuits show that these do suppress each other)\n",
    "    * Possible solution - more hardcoded rules?\n",
    "2. Misses some important things which aren't semantically similar as we've defined it, e.g. `1984` and `1985` aren't semantically similar (OV and QK circuits show that they do suppress each other)\n",
    "    * Possible solution - ???\n",
    "\n",
    "However, this maybe isn't worth further optimization, because it doesn't marginally improve the results by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SEMANTICITY = True\n",
    "\n",
    "if USE_SEMANTICITY:\n",
    "    from pattern.text.en import conjugate, PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE, PLURAL, SINGULAR\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    MY_TENSES = [PRESENT, PAST, FUTURE, SUBJUNCTIVE, INFINITIVE, PROGRESSIVE]\n",
    "    MY_NUMBERS = [PLURAL, SINGULAR]\n",
    "    from nltk.corpus import wordnet\n",
    "    nltk.download('wordnet')\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SEMANTICITY:\n",
    "    cspa_semantic_dict = pickle.load(open(ST_HTML_PATH.parent.parent / \"cspa/cspa_semantic_dict_full.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    warnings.warn(\"Not using semanticity unlike old notebook versions!\")\n",
    "    cspa_semantic_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h2>Related words</h2>This doesn't include tokenization fragments; it's just linguistic.\"))\n",
    "\n",
    "for word in [\"Berkeley\", \"pier\", \"pie\", \"ring\", \"device\", \"robot\", \"w\"]:\n",
    "    try: print(get_related_words(word, model))\n",
    "    except: print(get_related_words(word, model)); print(\"(Worked on second try!)\") # maybe because it downloads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<h2>Equivalency words</h2>These are the words which will be included in the semantic similarity cluster, during CSPA.\"))\n",
    "\n",
    "for tok in [\" Berkeley\", \" Pier\", \" pier\", \"pie\", \" pies\", \" ring\", \" device\", \" robot\", \"w\"]:\n",
    "    print(f\"{tok!r:>10} -> {get_equivalency_toks(tok, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SEMANTICITY:\n",
    "    table = Table(\"Source token\", \"All semantically related\", title=\"Semantic similarity: bidirectional, superstrings, substrings\") #  \"Top 3 related\" in the middle\n",
    "\n",
    "    str_toks = [\" Berkeley\", \"keley\", \" University\", \" Mary\", \" Pier\", \" pier\", \"NY\", \" ring\", \" W\", \" device\", \" robot\", \" jump\", \" driver\", \" Cairo\"]\n",
    "    print_cutoff = 105 # 70\n",
    "    def cutoff(s):\n",
    "        if len(s_str := str(s)) >= print_cutoff: return s_str[:print_cutoff-4] + ' ...'\n",
    "        else: return s_str\n",
    "\n",
    "    for str_tok in str_toks:\n",
    "        top3_sim = \"\\n\".join(list(map(repr, concat_lists(cspa_semantic_dict[str_tok])[:3])))\n",
    "        bidir, superstr, substr = cspa_semantic_dict[str_tok]\n",
    "        all_sim = \"\\n\".join([\n",
    "            cutoff(f\"{len(bidir)} bidirectional: {bidir}\"),\n",
    "            cutoff(f\"{len(superstr)} super-tokens:  {superstr}\"),\n",
    "            cutoff(f\"{len(substr)} sub-tokens:    {substr}\"),\n",
    "        ]) + \"\\n\"\n",
    "        table.add_row(repr(str_tok), all_sim) # top3_sim in the middle\n",
    "\n",
    "    rprint(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Filtering CSPA code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings: `K_u = 0.05`, no semantic similarity, batch size = 100\n",
    "\n",
    "We should recover ~76.9% KL Divergence, as reported in the paper draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean = get_result_mean([(10, 7), (11, 10)], DATA_TOKS[:100, :], model, verbose=True)\n",
    "# t.save(result_mean, f\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/result_mean.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, as long as SEQ_LEN large, small BATCH_SIZE gives quite good estimates\n",
    "QK_OV_BATCH_SIZE = 20\n",
    "QK_OV_SEQ_LEN = 600\n",
    "\n",
    "cspa_results_qk_ov = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:QK_OV_BATCH_SIZE, :QK_OV_SEQ_LEN],\n",
    "    max_batch_size = 1, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"ov\", \"qk\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    "    computation_device = \"cpu\", # device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The performance recovered is...\",\n",
    "    get_performance_recovered(cspa_results_qk_ov), # 79.6\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show several nice visuals for every CSPA run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs_and_summary_stats(cspa_results_qk_ov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL and Loss seems to be measuring fairly different things\n",
    "\n",
    "(Though this section isn't very exciting).\n",
    "\n",
    "Answering the question: \"Is there a correlation between the absolute value of 10.7 effect on loss and 10.7's effect on increasing KL Divergence from the model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIZ_BATCH_SIZE = QK_OV_BATCH_SIZE # change to current_batch_size for a smaller subset that can be streamlit vizualized\n",
    "VIZ_SEQ_LEN = SEQ_LEN -1 # change to current_seq_len for a smaller subset that can be streamlit vizualized\n",
    "BATCH_INDICES = torch.arange(VIZ_BATCH_SIZE) # (change to indices to filter for correct vals on streamlit)\n",
    "\n",
    "batch_indices = (BATCH_INDICES[:VIZ_BATCH_SIZE].unsqueeze(-1) + torch.zeros(VIZ_SEQ_LEN).unsqueeze(0))\n",
    "seq_indices = (torch.zeros(VIZ_BATCH_SIZE).unsqueeze(-1) + torch.arange(VIZ_SEQ_LEN).unsqueeze(0))\n",
    "\n",
    "my_dict = {\n",
    "    \"Mean Ablate 10.7 Loss - Normal Loss\": cspa_results_qk_ov[\"loss_ablated\"].clone().cpu() - cspa_results_qk_ov[\"loss\"].clone().cpu(),\n",
    "    \"Mean Ablation KL Divergence to Model\": cspa_results_qk_ov[\"kl_div_ablated_to_orig\"][:, :-1].clone().cpu(),\n",
    "    \"CSPA Loss - Normal Loss\": cspa_results_qk_ov[\"loss_cspa\"].clone().cpu() - cspa_results_qk_ov[\"loss\"].clone().cpu(),\n",
    "    \"CSPA KL\": cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].clone().cpu()[:, :-1],\n",
    "}\n",
    "\n",
    "for k in list(my_dict.keys()):\n",
    "    print(k)\n",
    "    print(my_dict[k].shape)\n",
    "\n",
    "    if len(my_dict[k].shape) == 2:\n",
    "        my_dict[k] = my_dict[k][:VIZ_BATCH_SIZE, :VIZ_SEQ_LEN].flatten()\n",
    "    else:\n",
    "        my_dict[k] = my_dict[k]\n",
    "\n",
    "    print(my_dict[k].shape)\n",
    "\n",
    "my_dict[\"Batch Indices\"] = batch_indices.flatten().float()\n",
    "my_dict[\"Seq Indices\"] = seq_indices.flatten().float()\n",
    "\n",
    "print(my_dict[\"Batch Indices\"].shape)\n",
    "print(my_dict[\"Seq Indices\"].shape)\n",
    "\n",
    "df = pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings; warnings.warn(\"Check out color when working\")\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(\n",
    "\n",
    "color = \"CSPA KL\"\n",
    "px.scatter(\n",
    "    df,\n",
    "    x = \"Mean Ablate 10.7 Loss - Normal Loss\",\n",
    "    y = \"Mean Ablation KL Divergence to Model\",\n",
    "    hover_data = [\"Batch Indices\", \"Seq Indices\"],\n",
    "    color = color,\n",
    "    color_continuous_scale = \"Blues\" if \"KL\" in color else \"RdBu_r\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    range_color=((0.0, 0.09) if \"KL\" in color else None),\n",
    ").update_traces(\n",
    "    marker=dict(\n",
    "        line=dict(\n",
    "            color='black', # Border color\n",
    "            width=0.5  # Border width\n",
    "        )\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: the correlation is pretty weak. The good news is that there are very few cases where change in loss is ~0 and KL is large.\n",
    "\n",
    "The line of best fit (when we take absolute values) to 3DP is\n",
    "\n",
    "`Change in KL = 0.067 * (Absolute change in loss)`\n",
    "\n",
    "with no constant factor is nice, though R^2 = 0.34 is low (this isn't surprising, the quantities are measuring different things).\n",
    "\n",
    "With the following heatmap, we can see that indeed the spread of losses is a fair bit larger than the spread of KLs. None of this is wildly exciting though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 50\n",
    "\n",
    "x_std = df['CSPA Loss - Normal Loss'].std()\n",
    "y_std = df['CSPA KL'].std()\n",
    "\n",
    "x_min = - x_std # df['CSPA Loss - Normal Loss'].min()\n",
    "x_max = x_std # df['CSPA Loss - Normal Loss'].max()\n",
    "y_min = 0.0\n",
    "y_max = y_std # df['CSPA KL'].max()\n",
    "\n",
    "heatmap_vals = torch.zeros(Q, Q)\n",
    "\n",
    "for x_quantile in range(Q):\n",
    "    for y_quantile in range(Q):\n",
    "        x_subset = df['CSPA Loss - Normal Loss'] >= x_min + (x_max - x_min) * x_quantile / Q \n",
    "        x_subset = x_subset & (df['CSPA Loss - Normal Loss'] <= x_min + (x_max - x_min) * (x_quantile+1) / Q)\n",
    "        \n",
    "        y_subset = df['CSPA KL'] >= y_min + (y_max - y_min) * y_quantile / Q \n",
    "        y_subset = y_subset & (df['CSPA KL'] <= y_min + (y_max - y_min) * (y_quantile+1) / Q)\n",
    "\n",
    "        heatmap_size = (x_subset & y_subset).to_numpy().astype(\"int\").sum() / len(df['CSPA KL'])\n",
    "        heatmap_vals[x_quantile, y_quantile] = np.log(heatmap_size) # Can use log here...\n",
    "\n",
    "fig = imshow(\n",
    "    heatmap_vals[:, torch.arange(heatmap_vals.shape[0]-1, -1, -1)].T, # Does two things: makes axes the right way around, and in my opinion heatmaps x and y are the wrong way round\n",
    "    title = f\"Log Density of Points in CSPA Ranges\",\n",
    "    width = 500, \n",
    "    height = 500,\n",
    "    labels = {\"x\": \"CSPA Loss - Model Loss\", \"y\": \"CSPA KL\"},\n",
    "    x = [str(round(x_min + (x_max - x_min) * x_quantile / Q, 4)) for x_quantile in range(Q)],\n",
    "    y = [str(round(y_min + (y_max - y_min) * y_quantile / Q, 5)) for y_quantile in range(Q)][::-1],\n",
    "    # text_auto = \".2f\",\n",
    "    range_color=(heatmap_vals.min().item(), heatmap_vals.max().item()),\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    return_fig=True,\n",
    "    color_continuous_midpoint=None,\n",
    ")\n",
    "\n",
    "# Set background color to white\n",
    "fig.update_layout(\n",
    "    paper_bgcolor='rgba(255,255,255,255)',\n",
    "    plot_bgcolor='rgba(255,255,255,255)'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"{df['CSPA Loss - Normal Loss'].std()=} {df['CSPA KL'].std()=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `K_u = 0.05`, no semantic similarity, batch size = 500\n",
    "\n",
    "(Skipped by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SLOW_CSPA = False\n",
    "\n",
    "if RUN_SLOW_CSPA:\n",
    "    cspa_results_qk_ov = get_cspa_results_batched(\n",
    "        model = model,\n",
    "        toks = DATA_TOKS[:, :], # [:50],\n",
    "        max_batch_size = 5, # 50,\n",
    "        negative_head = (10, 7),\n",
    "        interventions = [\"qk\", \"ov\"],\n",
    "        K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "        K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "        only_keep_negative_components = True,\n",
    "        semantic_dict = cspa_semantic_dict,\n",
    "        result_mean = result_mean,\n",
    "        use_cuda = False,\n",
    "        verbose = True,\n",
    "        compute_s_sstar_dict = False,\n",
    "    )\n",
    "\n",
    "    # fig_dict = generate_scatter(cspa_results_qk_ov, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "    fig_loss_line = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"loss\")\n",
    "    fig_loss_line_kl = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"kl-div\")\n",
    "\n",
    "    kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean()\n",
    "    kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean()\n",
    "\n",
    "    print(f\"Mean KL divergence from ablated to original: {kl_div_ablated_to_orig:.4f}\")\n",
    "    print(f\"Mean KL divergence from CSPA to original: {kl_div_cspa_to_orig:.4f}\")\n",
    "    print(f\"Ratio = {kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "    print(f\"Performance explained = {1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "    ma_max = fig_loss_line_kl.data[0].x[-1]\n",
    "    cspa_max = fig_loss_line_kl.data[0].y[-1]\n",
    "    print(f\"Most extreme quantile: fraction explained = 1 - ({cspa_max:.3f}/{ma_max:.3f}) = {1 - cspa_max/ma_max:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Projection CSPA\n",
    "\n",
    "The hardest part of the projection CSPA setup is *likely* the query direction (for example, in IOI this caused us a massive headache).\n",
    "\n",
    "So I tried to work on just doing query projections first (by setting `k_direction=None` and `ov_projection_config = None` in the below cells).\n",
    "\n",
    "I can only get to 64% of the KL explained :( and it's with a pretty cursed setup. I tried lots of different setups and this seems much better than others\n",
    "\n",
    "1. For every key token, project the query token onto the subspace of $\\mathbb{R}^{d_\\text{model}}$ spanned by the unembedding vector for the key token (`q_direction=\"unembedding\"`), and the other 7 semantically similar tokens to it (see `K_unembedding=8`)\n",
    "2. After doing this projection, double this query input vector (`q_input_multiplier=2.0`)\n",
    "3. After calculating attention scores, manually set all BOS attention scores to the exact value needed so the same attention to BOS is achieved (`mantain_bos_attention=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, as long as SEQ_LEN large, small BATCH_SIZE gives quite good estimates (experiments about this deleted, too in the weeds)\n",
    "Q_PROJECTION_BATCH_SIZE = 20\n",
    "Q_PROJECTION_SEQ_LEN = 300\n",
    "\n",
    "qk_projection_config = QKProjectionConfig(\n",
    "    q_direction=\"unembedding\",\n",
    "    k_direction=None,\n",
    "    q_input_multiplier=2.0,\n",
    "    use_same_scaling=False,\n",
    "    mantain_bos_attention=True,\n",
    "    model = model,\n",
    "    save_scores = True,\n",
    ")\n",
    "\n",
    "# ov_projection_config = OVProjectionConfig()\n",
    "ov_projection_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_q_projection = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:Q_PROJECTION_BATCH_SIZE, :Q_PROJECTION_SEQ_LEN],\n",
    "    max_batch_size = 1,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [],\n",
    "    qk_projection_config=qk_projection_config,\n",
    "    ov_projection_config=ov_projection_config,\n",
    "    K_unembeddings = 1.0,\n",
    "    K_semantic = 8, # Be very careful making this big... very slow...\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    "    computation_device = \"cpu\",\n",
    ")\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance recovered is... 0.6410360405728113\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The performance recovered is...\",\n",
    "    get_performance_recovered(cspa_results_q_projection), # ~64\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does this metric do for other heads?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the paper details results for the filtering version of CSPA. Currently this is a bit of a mess since I used it to try and develop a projection version of CSPA.\n",
    "\n",
    "It turns out that we get some of the best *relative* results when we turn on the OV Projection, too. But this makes things even worse than 64% for L10H7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean = get_result_mean([\n",
    "    (layer, head)\n",
    "    for layer in [8, 9, 10, 11] for head in range(12)\n",
    "], DATA_TOKS[:100, :], model, verbose=True)\n",
    "\n",
    "qk_projection_config = QKProjectionConfig( # Not good, we really don't get strong results here...\n",
    "    # q_direction=\"use_copying_as_query\",\n",
    "    q_direction=\"unembedding\",\n",
    "    q_input_multiplier=2.0,\n",
    "    use_same_scaling=False,\n",
    "    mantain_bos_attention=True,\n",
    "    # projection_directions = \"earlier_heads\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "ov_projection_config = OVProjectionConfig()\n",
    "# ov_projection_config = None\n",
    "\n",
    "# qk_projection_config = QKProjectionConfig(\n",
    "#     q_direction=\"unembedding\",\n",
    "#     k_direction=None,\n",
    "#     q_input_multiplier=1.0,\n",
    "#     use_same_scaling=False,\n",
    "#     mantain_bos_attention=True,\n",
    "#     # projection_directions = \"earlier_heads\",\n",
    "#     model = model, \n",
    "# )\n",
    "# result_mean = get_result_mean([(10, 7), (11, 10)], DATA_TOKS[:100, :], model, verbose=True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df35a8c90ff41a094467fbc035de8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kl_results = t.zeros(2, 4, 12).to(device)\n",
    "loss_results = kl_results.clone()\n",
    "normed_loss_results = kl_results.clone()\n",
    "non_normed_loss_results = kl_results.clone()\n",
    "squared_loss_results = kl_results.clone()\n",
    "\n",
    "for i, only_keep_negative_components in enumerate([True, False]):\n",
    "\n",
    "    for layer, head in tqdm(list(itertools.product([8, 9, 10, 11], range(12)))):\n",
    "\n",
    "        # Usage with your function\n",
    "        cspa_results_qk_ov=get_cspa_results_batched(model=model, toks=DATA_TOKS[-50:,:200], max_batch_size=10, negative_head=(layer,head), qk_projection_config=qk_projection_config, ov_projection_config=ov_projection_config, interventions=[], K_unembeddings=1.0, K_semantic=1, semantic_dict=cspa_semantic_dict, result_mean=result_mean, use_cuda=True, verbose=False, compute_s_sstar_dict=False, computation_device=None)\n",
    "\n",
    "        kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean().item()\n",
    "        kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean().item()\n",
    "\n",
    "        diff_of_loss_ablated_to_orig = (cspa_results_qk_ov[\"loss_ablated\"] - cspa_results_qk_ov[\"loss\"])\n",
    "        squared_loss_diff = (diff_of_loss_ablated_to_orig**2).mean().item()\n",
    "        normed_loss_diff = diff_of_loss_ablated_to_orig.abs().mean().item()\n",
    "        non_normed_loss_diff = diff_of_loss_ablated_to_orig.mean().item()\n",
    "\n",
    "        diff_of_loss_cspa_to_orig = (cspa_results_qk_ov[\"loss_cspa\"] - cspa_results_qk_ov[\"loss\"])\n",
    "        normed_cspa_loss_diff = diff_of_loss_cspa_to_orig.abs().mean().item() \n",
    "        squared_cspa_loss_diff = (diff_of_loss_cspa_to_orig**2).mean().item()\n",
    "        non_normed_cspa_loss_diff = diff_of_loss_cspa_to_orig.mean().item()\n",
    "\n",
    "        kl_performance_explained = 1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig\n",
    "        kl_results[i, layer - 8, head] = kl_performance_explained\n",
    "        print(f\"Head Layer {layer} {head} KL performance explained: {kl_performance_explained:.3f}\")\n",
    "\n",
    "        normed_loss_performance_explained = 1 - normed_cspa_loss_diff / normed_loss_diff\n",
    "        normed_loss_results[i, layer - 8, head] = normed_loss_performance_explained\n",
    "\n",
    "        squared_loss_performance_explained = 1 - squared_cspa_loss_diff / squared_loss_diff\n",
    "        squared_loss_results[i, layer - 8, head] = squared_loss_performance_explained\n",
    "\n",
    "        non_normed_loss_performance_explained = 1 - non_normed_cspa_loss_diff / non_normed_loss_diff\n",
    "        non_normed_loss_results[i, layer - 8, head] = non_normed_loss_performance_explained\n",
    "\n",
    "    break # We're currently not thinking much about the non-negative components\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plots:\")\n",
    "for results_name, results in zip([\"KL\", \"Net effect on loss\", \"Absolute difference in loss\", \"Squared effect on loss\"], [kl_results, non_normed_loss_results, normed_loss_results, squared_loss_results], strict=True):\n",
    "    imshow(\n",
    "        results,\n",
    "        facet_col = 0,\n",
    "        facet_labels = [\"Only keep negative components\", \"Keep negative and positive components\"],\n",
    "        title = f\"{results_name} performance of head explained by CSPA\",\n",
    "        width = 1800, \n",
    "        height = 450,\n",
    "        labels = {\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "        y = [str(i) for i in [8, 9, 10, 11]],\n",
    "        text_auto = \".2f\",\n",
    "        range_color=[0,1],\n",
    "        color_continuous_scale=\"Blues\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual code which appears on the dedicated streamlit page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_qk_ov, s_sstar_pairs_qk_ov = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS, # [:50],\n",
    "    max_batch_size = 60, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"qk\", \"ov\"],\n",
    "    K_unembeddings = 5,\n",
    "    K_semantic = 1,\n",
    "    only_keep_negative_components = True,\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = True,\n",
    ")\n",
    "# TODO - figure out where the bottleneck is via line profiler. I thought it was projections, but now it seems like this is not the case\n",
    "# Seems like it's this func: get_top_predicted_semantically_similar_tokens\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f func func(arg, kwarg=kwarg)\n",
    "\n",
    "fig_dict = generate_scatter(cspa_results_qk_ov, DATA_STR_TOKS_PARSED, batch_index_colors_to_highlight=[51, 300])\n",
    "fig_loss_line = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"loss\")\n",
    "fig_loss_line_kl = generate_loss_based_scatter(cspa_results_qk_ov, nbins=200, values=\"kl-div\")\n",
    "\n",
    "kl_div_ablated_to_orig = cspa_results_qk_ov[\"kl_div_ablated_to_orig\"].mean()\n",
    "kl_div_cspa_to_orig = cspa_results_qk_ov[\"kl_div_cspa_to_orig\"].mean()\n",
    "print(f\"Mean KL divergence from ablated to original: {kl_div_ablated_to_orig:.4f}\")\n",
    "print(f\"Mean KL divergence from CSPA to original: {kl_div_cspa_to_orig:.4f}\")\n",
    "print(f\"Ratio = {kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")\n",
    "print(f\"Performance explained = {1 - kl_div_cspa_to_orig / kl_div_ablated_to_orig:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CSPA to the Streamlit page (\"Browse Examples\")\n",
    "\n",
    "This code adds the CSPA plots to the HTML plots for the Streamlit page. It creates a 5th tab called `CSPA`, and adds to the logit and DLA plots in the second tab (the latter is mainly for our use, while we're iterating on and improving the CSPA code).\n",
    "\n",
    "I've added to this code in a pretty janky way, so that it can show more than one CSPA plot stacked on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results, s_sstar_pairs = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:48, :61], # [:50],\n",
    "    max_batch_size = 2, # 50,\n",
    "    negative_head = (10, 1),\n",
    "    interventions = [\"qk\", \"ov\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    only_keep_negative_components = False,\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = True,\n",
    "    return_dla = True,\n",
    "    return_logits = True,\n",
    "    keep_self_attn = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cspa_to_streamlit_page(\n",
    "    cspa_results = cspa_results,\n",
    "    s_sstar_pairs = s_sstar_pairs,\n",
    "    html_plots_filename = f\"GZIP_HTML_PLOTS_b48_s61.pkl\",\n",
    "    data_str_toks_parsed = [s[:61] for s in DATA_STR_TOKS_PARSED[:48]],\n",
    "    toks_for_doing_DLA = DATA_TOKS[:48, :61],\n",
    "    model = model,\n",
    "    verbose = True,\n",
    "    # test_idx = 36,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_of_toks(\n",
    "    toks1: List[str],\n",
    "    toks2: List[str],\n",
    "):\n",
    "    U1 = model.W_U.T[model.to_tokens(toks1, prepend_bos=False).squeeze()]\n",
    "    U2 = model.W_U.T[model.to_tokens(toks2, prepend_bos=False).squeeze()]\n",
    "\n",
    "    if U1.ndim == 1: U1 = U1.unsqueeze(0)\n",
    "    if U2.ndim == 1: U2 = U2.unsqueeze(0)\n",
    "\n",
    "    U1_normed = U1 / t.norm(U1, dim=-1, keepdim=True)\n",
    "    U2_normed = U2 / t.norm(U2, dim=-1, keepdim=True)\n",
    "\n",
    "    imshow(\n",
    "        U1_normed @ U2_normed.T,\n",
    "        title = \"Cosine similarity of unembeddings\",\n",
    "        x = toks2,\n",
    "        y = toks1,\n",
    "    )\n",
    "\n",
    "cos_sim_of_toks(\n",
    "    [\" stuff\"],\n",
    "    [\" devices\", \" phones\", \" screens\", \" device\", \" phone\", \" Android\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the code for \"love and war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mean_as_tensor = t.load(ST_HTML_PATH / \"result_mean.pt\")\n",
    "result_mean = {(10, 7): result_mean_as_tensor[0], (11, 10): result_mean_as_tensor[1]}\n",
    "\n",
    "prompt = \"I picked up the first box. I picked up the second box. I picked up the third and final box.\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "# Parse the string tokens for printing\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "model_results = get_model_results(\n",
    "    model,\n",
    "    toks=toks,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    result_mean=result_mean,\n",
    "    verbose=False\n",
    ")\n",
    "HTML_PLOTS_NEW = generate_4_html_plots(\n",
    "    model=model,\n",
    "    data_toks=toks,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    negative_heads=[(10, 7), (11, 10)],\n",
    "    model_results=model_results,\n",
    "    save_files=False,\n",
    ")\n",
    "cspa_results, s_sstar_pairs = get_cspa_results(\n",
    "    model=model,\n",
    "    toks=toks,\n",
    "    negative_head=(10, 7), #  this currently doesn't do anything; it's always 10.7\n",
    "    components_to_project=[\"o\"],\n",
    "    K_unembeddings=5,\n",
    "    K_semantic=3,\n",
    "    semantic_dict=cspa_semantic_dict,\n",
    "    effective_embedding=\"W_E (including MLPs)\",\n",
    "    result_mean=result_mean,\n",
    "    use_cuda=False,\n",
    "    return_dla=True,\n",
    ")\n",
    "HTML_PLOTS_NEW = add_cspa_to_streamlit_page(\n",
    "    cspa_results=cspa_results,\n",
    "    s_sstar_pairs=s_sstar_pairs,\n",
    "    data_str_toks_parsed=str_toks_parsed,\n",
    "    model=model,\n",
    "    HTML_PLOTS=HTML_PLOTS_NEW,\n",
    "    toks_for_doing_DLA=toks,\n",
    "    verbose=False,\n",
    "    test_idx=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular CSPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OV_BATCH_SIZE = 50\n",
    "\n",
    "cspa_results_qk = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:OV_BATCH_SIZE],\n",
    "    max_batch_size = 1, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"qk\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = False,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    ")\n",
    "clear_output() # Weird cell, it hogs space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs_and_summary_stats(cspa_results_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    get_performance_recovered(cspa_results_q_projection, verbose=True),\n",
    "    Q_PROJECTION_BATCH_SIZE,\n",
    "    Q_PROJECTION_SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating individual prompts\n",
    "\n",
    "It seems important to dive down into why the projection CSPA only gets ~64%. These cells do this, though TODO Arthur move this out of here, it's better placed in an experimental file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_BATCH_SIZE = 18\n",
    "PLOT_SEQ_LEN = 50\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"cspa_kl\": to_numpy(cspa_results_q_projection[\"kl_div_cspa_to_orig\"][:PLOT_BATCH_SIZE, :PLOT_SEQ_LEN].flatten()),\n",
    "    \"ablated_kl\": to_numpy(cspa_results_q_projection[\"kl_div_ablated_to_orig\"][:PLOT_BATCH_SIZE, :PLOT_SEQ_LEN].flatten()),\n",
    "    # \"indices\": sum(list(enumerate([[seq_idx for seq_idx in range(Q_PROJECTION_SEQ_LEN)] for _ in range(Q_PROJECTION_BATCH_SIZE)]))),\n",
    "    \"hover_data\" : [str((indices[batch_idx], seq_idx, \"which is\", batch_idx)) for batch_idx in range(PLOT_BATCH_SIZE) for seq_idx in range(PLOT_SEQ_LEN)],\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x = \"cspa_kl\",\n",
    "    y = \"ablated_kl\",\n",
    "    hover_data = \"hover_data\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# Save figure\n",
    "# fig.write_image(\"fig_of_points.pdf\")\n",
    "\n",
    "# Studying some failures of projection, across several different runs. Maybe mean ablation means different things are differently destructive? Scary if so!\n",
    "# (8, 49), (1, 40), (18, 10), (8, 35)\n",
    "# (8, 49): Covered below; we pick \" homeowners\" and \" rules\" but the model attends to \" Aurora\" (and \" Council\"). Note this is on a comma. Maybe we surpress capital letters after that?\n",
    "# (1, 40): We should attend to private. But instead we attend to prisons. Really weird though, as private is higher predicted, and OV circuit analysis didn't seem to help\n",
    "# (18, 10): with\" -> \"TPP\" should be suppressed. But \" Lee\" is predicted more.\n",
    "# (8, 35): requiring\" -> \" rentals\" is actually attended to. But \" homeowners\" is predicted more, and hence this ablation picks it more.\n",
    "# (33, 42): model attends to \" remove\" -> \" Blackberry\", we attend to \" remove\" -> \" ban\". And Blackberry is the top prediction! Ban is in fact third\n",
    "# (12, 26): we have half the amount of attention to ' Art' as we should. Context is \"\\n\\n\" -> \" Art\" and the only higher predicted thing is \" This\" (which is likely a function word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some examples of failures?\n",
    "\n",
    "def print_attentions(batch_idx, seq_idx):\n",
    "    print(sorted(list(enumerate(cspa_results_q_projection[\"pattern\"][batch_idx, seq_idx, :seq_idx].tolist())), key=lambda x: x[1], reverse=True))    \n",
    "\n",
    "# cspa_results_q_projection[\"pattern\"][13, 14, :15] # indices[13] = 35. of -> Neil. We put 90% prob on Neil, but for some reason the model also suppresses \"About\"\n",
    "print_attentions(3, 49) # indices[3] = 8. We pick \" homeowners\" and \" rules\" but the model attends to \" Aurora\" and \" Council\". Note this is on a comma. Maybe we surpress capital letters after that?\n",
    "# print_attentions(12, 26) # indices[12] = 34. We put too much attention on \"This\" rather than full 50% on \"Art\". Failure due to not using semantically similar tokens\n",
    "# print_attentions(11, 42) # indices[11] = 33. We should attend to Blackberry more. No idea how \" meetings\" is almost the same amount of attention...\n",
    "print_attentions(0, 40) # ... [1] we should put weight on \" prisons\" not private...\n",
    "\n",
    "print_attentions(5, 10)\n",
    "print_attentions(3, 35)\n",
    "print_attentions(11, 42) \n",
    "print_attentions(15, 37)\n",
    "print_attentions(12, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a sanity check on some cases where we are doing well, to check that really this \"surprising attention\" bug is real.\n",
    "print_attentions(2, 28) \n",
    "print(\"Yeah we absolutely nailed the ~80% attention here\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what's going wrong? Are our attention scores too high on incorrect, or too low on correct?\n",
    "\n",
    "First let's survey the cases where things go well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspa_results_q_projection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the cases where ablated KL >0.1 and CSPA KL <0.05\n",
    "\n",
    "index_relevant = (cspa_results_q_projection[\"kl_div_ablated_to_orig\"] > 0.1) &  (cspa_results_q_projection[\"kl_div_cspa_to_orig\"] < 0.05)\n",
    "indices_raw = np.nonzero(to_numpy(index_relevant.flatten()))[0]\n",
    "indices = list(zip(\n",
    "    indices_raw//index_relevant.shape[-1],\n",
    "    indices_raw%index_relevant.shape[-1],\n",
    "    strict=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
