{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Prompts\n",
    "\n",
    "This is the notebook I use to test out the functions in this directory, and generate the plots in the Streamlit page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "p = Path(r\"/root/SERI-MATS-2023-Streamlit-pages\")\n",
    "if os.path.exists(str_p := str(p.resolve())):\n",
    "    os.chdir(str_p)\n",
    "    if str_p not in sys.path:\n",
    "        sys.path.append(str_p)\n",
    "\n",
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum2.ov_qk_circuits.ov_qk_plot_functions import (\n",
    "    plot_logit_lens,\n",
    "    plot_full_matrix_histogram,\n",
    ")\n",
    "from transformer_lens.rs.callum2.blog_viz.generate_html_funcs import (\n",
    "    CSS,\n",
    "    generate_4_html_plots,\n",
    "    generate_4_html_plots_batched,\n",
    "    generate_html_for_DLA_plot,\n",
    "    generate_html_for_logit_plot,\n",
    "    generate_html_for_loss_plot,\n",
    "    generate_html_for_unembedding_components_plot,\n",
    "    attn_filter,\n",
    "    _get_color,\n",
    ")\n",
    "from transformer_lens.rs.callum2.blog_viz.model_results import (\n",
    "    get_result_mean,\n",
    "    get_model_results,\n",
    "    HeadResults,\n",
    "    LayerResults,\n",
    "    DictOfHeadResults,\n",
    "    ModelResults,\n",
    "    first_occurrence,\n",
    "    project,\n",
    "    model_fwd_pass_from_resid_pre,\n",
    ")\n",
    "from transformer_lens.rs.callum2.utils import (\n",
    "    create_title_and_subtitles,\n",
    "    get_effective_embedding,\n",
    "    parse_str,\n",
    "    parse_str_tok_for_printing,\n",
    "    parse_str_toks_for_printing,\n",
    "    topk_of_Nd_tensor,\n",
    "    ST_HTML_PATH,\n",
    "    NEGATIVE_HEADS,\n",
    "    process_webtext,\n",
    "    rearrange_list,\n",
    "    clamp,\n",
    "    first_occurrence_2d,\n",
    ")\n",
    "from transformer_lens.rs.callum2.cspa.cspa_functions import (\n",
    "    FUNCTION_STR_TOKS,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eindex import eindex\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 5\n",
    "D_VOCAB = 100\n",
    "\n",
    "logprobs = torch.randn(BATCH_SIZE, SEQ_LEN, D_VOCAB).log_softmax(-1)\n",
    "labels = torch.randint(0, D_VOCAB, (BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "# (1) Using eindex\n",
    "output_1 = eindex(logprobs, labels, \"batch seq [batch seq]\")\n",
    "\n",
    "# (2) Normal PyTorch, using `gather`\n",
    "output_2 = logprobs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# (3) Normal PyTorch, not using `gather` (this is like what `eindex` does under the hood)\n",
    "output_3 = logprobs[torch.arange(BATCH_SIZE).unsqueeze(-1), torch.arange(SEQ_LEN), labels]\n",
    "\n",
    "# Check they're all the same\n",
    "assert torch.allclose(output_1, output_2)\n",
    "assert torch.allclose(output_1, output_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cpu\" # \"cuda\"\n",
    "    # fold value bias?\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "W_EE_dict = get_effective_embedding(model)\n",
    "\n",
    "FUNCTION_TOKS = model.to_tokens(FUNCTION_STR_TOKS, prepend_bos=False).squeeze()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 41 # 51 for viz, 200 for my local one\n",
    "SEQ_LEN = 101 # (61 for viz, no more because attn)\n",
    "batch_idx = 36\n",
    "\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED = process_webtext(seed=6, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, verbose=True, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test html in small case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TOKS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "prompt = \"All's fair in love and war\"\n",
    "toks = model.to_tokens(prompt)\n",
    "str_toks = model.to_str_tokens(toks)\n",
    "if isinstance(str_toks[0], str): str_toks = [str_toks]\n",
    "str_toks_parsed = [list(map(parse_str_tok_for_printing, s)) for s in str_toks]\n",
    "\n",
    "result_mean = get_result_mean(\n",
    "    head_list = NEGATIVE_HEADS,\n",
    "    toks = DATA_TOKS[:, :toks.shape[1]],\n",
    "    model = model,\n",
    "    minibatch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "MODEL_RESULTS = get_model_results(\n",
    "    model = model,\n",
    "    toks = toks,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    result_mean = result_mean,\n",
    ")\n",
    "\n",
    "HTML_PLOTS: Dict[str, Dict[Tuple, str]] = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = toks,\n",
    "    data_str_toks_parsed = str_toks_parsed,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = False,\n",
    "    result_mean = result_mean,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "for k, v in HTML_PLOTS.items():\n",
    "    print(k)\n",
    "    k2 = list(zip(*HTML_PLOTS[\"LOSS\"].keys()))\n",
    "    for j, _k2 in enumerate(k2):\n",
    "        print(f\"{(j)} = {sorted(set(_k2))}\")\n",
    "\n",
    "display(HTML(\"\".join([\n",
    "    CSS,\n",
    "    HTML_PLOTS[\"LOSS\"][(0, \"10.7\", \"direct+frozen+mean\", True)],\n",
    "    \"<br>\" * 2,\n",
    "    # HTML_PLOTS[\"LOGITS_ORIG\"][(0,)],\n",
    "    # \"<br>\" * 2,\n",
    "    HTML_PLOTS[\"LOGITS_ABLATED\"][(0, \"10.7\", \"direct+frozen+mean\")],\n",
    "    \"<br>\" * 21,\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual HTML plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "result_mean = get_result_mean(NEGATIVE_HEADS, DATA_TOKS, model, minibatch_size=8)\n",
    "\n",
    "model_results = get_model_results(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS,\n",
    "    negative_heads = [(10, 7)],\n",
    "    result_mean = result_mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "BLOG_PATH = Path(\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/blog_viz\")\n",
    "DATA_PATH = BLOG_PATH / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_logprobs_in_context(\n",
    "    logprobs: Float[Tensor, \"batch seqQ d_vocab\"],\n",
    "    toks: Int[Tensor, \"batch seqQ\"],\n",
    "    function_toks: Int[Tensor, \"toks\"]\n",
    "):\n",
    "    '''\n",
    "    Returns the top predicted logprobs, over all the source tokens in context.\n",
    "\n",
    "    The indices of the result are the seqK positions, and the values are the logprobs.\n",
    "\n",
    "    Tokens which are function words are filtered out.\n",
    "    '''\n",
    "    b, seq, v = logprobs.shape\n",
    "\n",
    "    # Get all logprobs for the source tokens in context\n",
    "    b_indices = einops.repeat(t.arange(b), \"b -> b sQ sK\", sQ=seq, sK=seq)\n",
    "    sQ_indices = einops.repeat(t.arange(seq), \"sQ -> b sQ sK\", b=b, sK=seq)\n",
    "    toks_rep = einops.repeat(toks, \"b sK -> b sQ sK\", sQ=seq)\n",
    "    logprobs_ctx: Float[Tensor, \"batch seqQ seqK\"] = logprobs[b_indices, sQ_indices, toks_rep]\n",
    "    # The (b, q, k)-th elem is the logprobs of word k at sequence position (b, q)\n",
    "\n",
    "    # Mask: causal\n",
    "    sQ_indices = einops.repeat(t.arange(seq), \"sQ -> b sQ 1\", b=b)\n",
    "    sK_indices = einops.repeat(t.arange(seq), \"sK -> b 1 sK\", b=b)\n",
    "    causal_mask = sQ_indices >= sK_indices\n",
    "    # Mask: first occurrence of each word (because we want the top 5 DISTINCT words)\n",
    "    first_occurrence_mask = einops.repeat(first_occurrence_2d(toks), \"b sK -> b 1 sK\")\n",
    "    # Mask: non-function words\n",
    "    non_fn_word_mask = (toks[:, :, None] != function_toks).all(dim=-1)\n",
    "    non_fn_word_mask = einops.repeat(non_fn_word_mask, \"b sK -> b 1 sK\")\n",
    "    # Apply all 3 masks\n",
    "    logprobs_masked = t.where(\n",
    "        causal_mask & first_occurrence_mask & non_fn_word_mask,\n",
    "        logprobs_ctx,\n",
    "        -float(\"inf\")\n",
    "    )\n",
    "\n",
    "    # Now, we can pick the top 5 (over the seqK-dimension) for each query index\n",
    "    k = min(5, logprobs_masked.size(-1))\n",
    "    logprobs_masked_top5 = logprobs_masked.topk(dim=-1, k=k)\n",
    "\n",
    "    return logprobs_masked_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/callummcdougall/eindex.git\n",
    "from eindex import eindex\n",
    "\n",
    "# %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_norms = model_results.out_norm[10, 7][[36]]\n",
    "# info_norms_rescaled = info_norms / einops.reduce(info_norms, \"batch seqK -> batch 1\", \"max\")\n",
    "# info_norms_rescaled = einops.repeat(info_norms_rescaled, \"batch max_over_seqK -> batch 1 max_over_seqK\")\n",
    "\n",
    "# attn = model_results.pattern[10, 7][[36]]\n",
    "# attn_info_weighted = attn * info_norms_rescaled\n",
    "\n",
    "# viz_standard = cv.attention.attention_patterns(\n",
    "#     attention = attn_info_weighted[:50],\n",
    "#     tokens = DATA_STR_TOKS_PARSED[36][:50],\n",
    "#     attention_head_names = \"10.7\",\n",
    "# )\n",
    "# # viz_attn_weighted = \n",
    "# display(viz_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 100%|██████████| 41/41 [00:00<00:00, 50.99it/s]\n",
      "DLA: 100%|██████████| 41/41 [00:04<00:00, 10.06it/s]\n",
      "logits: 100%|██████████| 41/41 [00:03<00:00, 10.81it/s]\n",
      "logit lens: 100%|██████████| 41/41 [00:02<00:00, 14.67it/s]\n",
      "Saving as JSON: 100%|██████████| 41/41 [00:00<00:00, 147.15it/s]\n",
      "Saving as HTML: 41it [00:00, 10999.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_loss(\n",
    "    model_results: ModelResults,\n",
    "    head = (10, 7),\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    data_toks = DATA_TOKS,\n",
    "    only_include: Optional[Union[str, List[str]]] = None,\n",
    "    only_exclude: Optional[Union[str, List[str]]] = None,\n",
    "):\n",
    "    # ! Get all data, and get dict to store results\n",
    "\n",
    "    if isinstance(only_include, str): only_include = [only_include]\n",
    "    if isinstance(only_exclude, str): only_exclude = [only_exclude]\n",
    "    assert (only_include is None) or (only_exclude is None), \"Can't specify both `only_include` and `only_exclude`\"\n",
    "    include = [\"LOSS\", \"DLA\", \"ATTN\", \"LOGIT LENS\", \"LOGPROBS\"]\n",
    "    if only_exclude is not None:\n",
    "        include = list(set(include) - set(only_exclude))\n",
    "    elif only_include is not None:\n",
    "        include = only_include\n",
    "\n",
    "    batch_size, seq_len = data_toks.shape\n",
    "    LAYER, HEAD = head\n",
    "\n",
    "    loss_orig = model_results.loss_orig\n",
    "    loss_diffs = model_results.loss_diffs[(\"direct\", \"frozen\", \"mean\")][LAYER, HEAD]\n",
    "\n",
    "    dla = model_results.dla[(\"frozen\", \"mean\")][LAYER, HEAD] # [batch_size, seq_len, d_vocab]\n",
    "\n",
    "    logits_orig = model_results.logits_orig # [batch_size, seq_len, d_vocab]\n",
    "    logits_abl = model_results.logits[(\"direct\", \"frozen\", \"mean\")][LAYER, HEAD] # [batch_size, seq_len, d_vocab]\n",
    "\n",
    "    logit_lens = model_results.logit_lens[LAYER] # [batch_size, seq_len, d_vocab]\n",
    "\n",
    "    json_values: Dict[int, Dict[str, list]] = {\n",
    "        i: {\n",
    "            # LOSS\n",
    "            \"loss_values\": [], \"words\": [], \"loss_colors\": [], \"loss_text_colors\": [],\n",
    "            # DLA\n",
    "            \"dla_pos_words\": [], \"dla_pos_values\": [], \"dla_pos_colors\": [], \"dla_pos_text_colors\": [],\n",
    "            \"dla_neg_words\": [], \"dla_neg_values\": [], \"dla_neg_colors\": [], \"dla_neg_text_colors\": [],\n",
    "            # LOGPROBS\n",
    "            \"logprobs_orig_words\": [], \"logprobs_orig_values\": [], \"logprobs_orig_colors\": [], \"logprobs_orig_text_colors\": [],\n",
    "            \"logprobs_abl_words\": [], \"logprobs_abl_values\": [], \"logprobs_abl_colors\": [], \"logprobs_abl_text_colors\": [],\n",
    "            # LOGIT LENS\n",
    "            \"logit_lens_words\": [], \"logit_lens_values\": [], \"logit_lens_colors\": [], \"logit_lens_text_colors\": [],\n",
    "            \"logit_lens_ctx_words\": [], \"logit_lens_ctx_values\": [], \"logit_lens_ctx_ranks\": [],\n",
    "            # ATTN\n",
    "        }\n",
    "        for i in range(batch_size)\n",
    "    }\n",
    "\n",
    "    # ! LOSS\n",
    "\n",
    "    if \"LOSS\" in include:\n",
    "\n",
    "        LOSS_MAX_COLOR = 2\n",
    "\n",
    "        for i, (loss_diff, data_str_toks) in tqdm(list(enumerate(zip(loss_diffs, data_str_toks_parsed))), desc=\"Loss\"):\n",
    "\n",
    "            loss_colors, loss_text_colors = _get_color(0.5 + t.clamp(loss_diff / (2 * LOSS_MAX_COLOR), -0.5, 0.5))\n",
    "            json_values[i][\"words\"] = data_str_toks\n",
    "            json_values[i][\"loss_values\"] = [round(ld.item(), 4) for ld in loss_diff]\n",
    "            json_values[i][\"loss_colors\"] = loss_colors\n",
    "            json_values[i][\"loss_text_colors\"] = loss_text_colors\n",
    "\n",
    "    # ! DLA\n",
    "\n",
    "    if \"DLA\" in include:\n",
    "\n",
    "        DLA_MAX_COLOR = 2.5\n",
    "        TOPK = 10\n",
    "\n",
    "        for i, _dla in tqdm(list(enumerate(dla)), desc=\"DLA\"):\n",
    "\n",
    "            dla_pos = _dla.topk(TOPK, dim=-1) # [seq_len, TOPK]\n",
    "            dla_neg = _dla.topk(TOPK, dim=-1, largest=False) # [seq_len, TOPK]\n",
    "            \n",
    "            dla_pos_words_all = rearrange_list(model.to_str_tokens(dla_pos.indices.flatten(), prepend_bos=False), TOPK)\n",
    "            dla_neg_words_all = rearrange_list(model.to_str_tokens(dla_neg.indices.flatten(), prepend_bos=False), TOPK)\n",
    "\n",
    "            for seq in range(seq_len):\n",
    "                dla_pos_color, dla_pos_text_color = _get_color(0.5 + clamp(dla_pos.values[seq, 0].item() / (2 * DLA_MAX_COLOR), 0.0, 0.5))\n",
    "                json_values[i][\"dla_pos_words\"].append(dla_pos_words_all[seq])\n",
    "                json_values[i][\"dla_pos_values\"].append([round(v, 2) for v in dla_pos.values[seq].tolist()])\n",
    "                json_values[i][\"dla_pos_colors\"].append(dla_pos_color)\n",
    "                json_values[i][\"dla_pos_text_colors\"].append(dla_pos_text_color)\n",
    "\n",
    "                dla_neg_color, dla_neg_text_color = _get_color(0.5 + clamp(dla_neg.values[seq, 0].item() / (2 * DLA_MAX_COLOR), -0.5, 0.0))\n",
    "                json_values[i][\"dla_neg_words\"].append(dla_neg_words_all[seq])\n",
    "                json_values[i][\"dla_neg_values\"].append([round(v, 2) for v in dla_neg.values[seq].tolist()])\n",
    "                json_values[i][\"dla_neg_colors\"].append(dla_neg_color)\n",
    "                json_values[i][\"dla_neg_text_colors\"].append(dla_neg_text_color)\n",
    "\n",
    "    # ! LOGPROBS\n",
    "\n",
    "    if \"LOGPROBS\" in include:\n",
    "\n",
    "        LOGPROBS_MAX_COLOR = 2.5\n",
    "        TOPK = 10\n",
    "\n",
    "        for i, (_logits_abl, _logits_orig) in tqdm(list(enumerate(zip(logits_abl, logits_orig))), desc=\"logits\"):\n",
    "\n",
    "            logprobs_abl = _logits_abl.log_softmax(-1)\n",
    "            logprobs_orig = _logits_orig.log_softmax(-1)\n",
    "            \n",
    "            logprobs_abl_topk = logprobs_abl.topk(TOPK, dim=-1) # [seq_len, TOPK]\n",
    "            logprobs_orig_topk = logprobs_orig.topk(TOPK, dim=-1) # [seq_len, TOPK]\n",
    "            \n",
    "            logprobs_abl_words_all = rearrange_list(model.to_str_tokens(logprobs_abl_topk.indices.flatten(), prepend_bos=False), TOPK)\n",
    "            logprobs_orig_words_all = rearrange_list(model.to_str_tokens(logprobs_orig_topk.indices.flatten(), prepend_bos=False), TOPK)\n",
    "\n",
    "            for seq in range(seq_len):\n",
    "                values = logprobs_abl_topk.values[seq]\n",
    "                logprobs_abl_color, logprobs_abl_text_color = _get_color(1 + max(values[0].item() / (2 * LOGPROBS_MAX_COLOR), -0.5))\n",
    "                json_values[i][\"logprobs_abl_words\"].append(logprobs_abl_words_all[seq])\n",
    "                json_values[i][\"logprobs_abl_values\"].append([round(v, 2) for v in values.tolist()])\n",
    "                json_values[i][\"logprobs_abl_colors\"].append(logprobs_abl_color)\n",
    "                json_values[i][\"logprobs_abl_text_colors\"].append(logprobs_abl_text_color)\n",
    "\n",
    "                values = logprobs_orig_topk.values[seq]\n",
    "                logprobs_orig_color, logprobs_orig_text_color = _get_color(1 + max(values[0].item() / (2 * LOGPROBS_MAX_COLOR), -0.5))\n",
    "                json_values[i][\"logprobs_orig_words\"].append(logprobs_orig_words_all[seq])\n",
    "                json_values[i][\"logprobs_orig_values\"].append([round(v, 2) for v in values.tolist()])\n",
    "                json_values[i][\"logprobs_orig_colors\"].append(logprobs_orig_color)\n",
    "                json_values[i][\"logprobs_orig_text_colors\"].append(logprobs_orig_text_color)\n",
    "    \n",
    "    # ! LOGIT LENS\n",
    "\n",
    "    if \"LOGIT LENS\" in include:\n",
    "\n",
    "        LOGPROBS_MAX_COLOR = 2.5\n",
    "        TOPK = 10\n",
    "        TOPK_CTX = 5\n",
    "\n",
    "        # Do everything at once, cause I already wrote code this way!\n",
    "        logprobs = logit_lens.log_softmax(-1) # [batch_size, seq_len, d_vocab]\n",
    "        logprobs_topk = logprobs.topk(TOPK, dim=-1) # [batch_size, seq_len, TOPK]\n",
    "        logprobs_words_all = rearrange_list(model.to_str_tokens(logprobs_topk.indices.flatten(), prepend_bos=False), TOPK) # [batch_size*seq_len, TOPK]\n",
    "        logprobs_words_all = rearrange_list(logprobs_words_all, seq_len) # [batch_size, seq_len, TOPK]\n",
    "        \n",
    "        # Get logprobs for just context (this)\n",
    "        logprobs_ctx_topk = get_top_logprobs_in_context(logprobs, data_toks, FUNCTION_TOKS)\n",
    "        # ctx_top_toks[batch, seq, k] = data_toks[batch, logprobs_ctx_topk.indices[batch, seq, k]]\n",
    "        logprobs_ctx_topk_indices = eindex(data_toks, logprobs_ctx_topk.indices, \"batch [batch seq k]\") # [batch seq TOPK_CTX]\n",
    "        logprobs_ctx_words_all = rearrange_list(model.to_str_tokens(logprobs_ctx_topk_indices.flatten(), prepend_bos=False), TOPK_CTX)\n",
    "        logprobs_ctx_words_all = rearrange_list(logprobs_ctx_words_all, seq_len) # [batch_size, seq_len, TOPK_CTX]\n",
    "\n",
    "        for i in tqdm(range(batch_size), desc=\"logit lens\"):\n",
    "\n",
    "            for seq in range(seq_len):\n",
    "                # Get the first table of values: top 10, not limited to context\n",
    "                json_values[i][\"logit_lens_words\"].append(logprobs_words_all[i][seq])\n",
    "                json_values[i][\"logit_lens_values\"].append([round(v, 2) for v in logprobs_topk.values[i, seq].tolist()])\n",
    "\n",
    "                # Get the second table of values: top 5, in context (this also gives us our colors)\n",
    "                values = logprobs_ctx_topk.values[i, seq] # [TOPK_CTX]\n",
    "                logit_lens_color, logit_lens_text_color = _get_color(1 + max(values[0].item() / (2 * LOGPROBS_MAX_COLOR), -0.5))\n",
    "                ranks = (values.unsqueeze(-1) < logprobs[i, seq]).sum(dim=-1).tolist()\n",
    "                num_src = (values > -float(\"inf\")).sum().item()\n",
    "                json_values[i][\"logit_lens_colors\"].append(logit_lens_color)\n",
    "                json_values[i][\"logit_lens_text_colors\"].append(logit_lens_text_color)\n",
    "                json_values[i][\"logit_lens_ctx_words\"].append(logprobs_ctx_words_all[i][seq][:num_src])\n",
    "                json_values[i][\"logit_lens_ctx_values\"].append([round(v, 2) for v in values.tolist()[:num_src]])\n",
    "                json_values[i][\"logit_lens_ctx_ranks\"].append(ranks[:num_src])\n",
    "\n",
    "\n",
    "    # ! ATTN\n",
    "\n",
    "    attn_list = []\n",
    "    DP = 3\n",
    "\n",
    "    if \"ATTN\" in include:\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            probs = model_results.pattern[LAYER, HEAD][i]\n",
    "            \n",
    "            info_norms = model_results.out_norm[LAYER, HEAD][i]\n",
    "            info_norms_rescaled = info_norms / info_norms.max()\n",
    "            info_norms_rescaled = einops.repeat(info_norms_rescaled, \"max_over_seqK -> seqQ max_over_seqK\", seqQ=1)\n",
    "            probs_info_weighted = probs * info_norms_rescaled\n",
    "\n",
    "            def round_2d_tensor(tensor: Tensor):\n",
    "                assert tensor.ndim == 2\n",
    "                return rearrange_list(list(map(lambda x: round(x, ndigits=DP), tensor.flatten().tolist())), tensor.shape[1])\n",
    "\n",
    "            html = cv.attention.attention_patterns(\n",
    "                attention = [round_2d_tensor(probs), round_2d_tensor(probs_info_weighted)],\n",
    "                tokens = data_str_toks_parsed[i],\n",
    "                attention_head_names = [\"Standard\", \"Info-weighted\"],\n",
    "            )\n",
    "            attn_list.append(html)\n",
    "\n",
    "    # Save as json\n",
    "    for i, d in tqdm(list(json_values.items()), desc=\"Saving as JSON\"):\n",
    "        with open(DATA_PATH / f\"data_{i}.json\", \"w\") as f:\n",
    "            json.dump(d, f, indent=4)\n",
    "    # Save as html\n",
    "    for i, d in tqdm(enumerate(attn_list), desc=\"Saving as HTML\"):\n",
    "        with open(DATA_PATH / f\"attn_{i}.html\", \"w\") as f:\n",
    "            f.write(str(d))\n",
    "\n",
    "save_loss(model_results) # only_include=\"ATTN\", only_exclude=\"ATTN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/OV_QK_circuits_less.pkl\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_ctx.indices[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.loss_diffs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: logit lens (TODO: fix up everything below this; it's old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "neg = False\n",
    "all_ranks = []\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "logits, cache = model.run_with_cache(DATA_TOKS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_to_plot = [\n",
    "#     (35, 39, \" About\"),\n",
    "#     (67, 21, \" delays\"),\n",
    "#     (8, 35, \" rentals\"),\n",
    "#     (8, 54, \" require\"),\n",
    "#     (53, 18, [\" San\", \" Francisco\"]),\n",
    "#     (33, 9, \" Hollywood\"),\n",
    "#     (49, 7, \" Home\"),\n",
    "#     (71, 34, \" sound\"),\n",
    "#     (14, 56, \" Kara\"),\n",
    "# ]\n",
    "# points_to_plot = [\n",
    "#     (45, 42, [\" editorial\"]),\n",
    "#     (45, 58, [\" stadium\", \" Stadium\", \" stadiums\"]),\n",
    "#     (43, 56, [\" Biden\"]),\n",
    "#     (43, 44, [\" interview\", \" campaign\"]),\n",
    "#     (38, 54, [\" Mary\", \" Catholics\"]),\n",
    "#     (33, 29, \" Hollywood\"),\n",
    "#     (33, 42, \" BlackBerry\"),\n",
    "#     (31, 33, [\" Church\", \" churches\"]),\n",
    "#     (28, 53, [\" mobile\", \" phone\", \" device\"]),\n",
    "#     (25, 32, [\" abstraction\", \" abstract\", \" Abstract\"]),\n",
    "#     (18, 25, [\"TPP\", \" Lee\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 52, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 35, [\" Italy\", \" mafia\"]),\n",
    "#     (10, 25, [\" Italian\", \" Italy\"]),\n",
    "#     (6, 52, [\" landfill\", \" waste\"]),\n",
    "#     (4, 52, \" jury\"),\n",
    "# ]\n",
    "points_to_plot = [\n",
    "    # (14, 56, \" Kara\"),\n",
    "    # (67, 47, \" case\"),\n",
    "    # (24, 73, \" negotiation\"),\n",
    "    (2, 35, [\" Berkeley\", \"keley\"]),\n",
    "]\n",
    "\n",
    "resid_pre_head = (cache[\"resid_pre\", 10]) / cache[\"scale\", 10, \"ln1\"]  #  - cache[\"resid_pre\", 1]\n",
    "\n",
    "plot_logit_lens(points_to_plot, resid_pre_head, model, DATA_STR_TOKS_PARSED_2, k=15, title=\"Predictions at token ' of', before head 10.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms: QK and OV circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" Berkeley\", src=\"keley\", focus_on=\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both(dest, src, focus_on: Literal[\"src\", \"dest\"]):\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"OV\", neg=True, head=(10, 7), flip=(focus_on==\"dest\"))\n",
    "    plot_full_matrix_histogram(W_EE_dict, src, dest, model, k=15, circuit=\"QK\", neg=False, head=(10, 7), flip=(focus_on==\"src\"))\n",
    "\n",
    "plot_both(dest=\" negotiation\", src=\" negotiations\", focus_on=\"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_orig = MODEL_RESULTS.logits_orig[32, 19].log_softmax(-1)\n",
    "logprobs_abl = MODEL_RESULTS.logits[(\"direct\", \"frozen\", \"mean\")][10, 7][32, 19].log_softmax(-1)\n",
    "\n",
    "logprobs_orig_topk = logprobs_orig.topk(10, dim=-1, largest=True)\n",
    "y_orig = logprobs_orig_topk.values.tolist()\n",
    "x = logprobs_orig_topk.indices\n",
    "y_abl = logprobs_abl[x].tolist()\n",
    "x = list(map(repr, model.to_str_tokens(x)))\n",
    "\n",
    "orig_colors = [\"darkblue\"] * len(x)\n",
    "abl_colors = [\"blue\"] * len(x)\n",
    "\n",
    "correct_next_str_tok = \" heated\"\n",
    "correct_next_token = model.to_single_token(\" heated\")\n",
    "# if repr(correct_next_str_tok) in x:\n",
    "#     idx = x.index(repr(correct_next_str_tok))\n",
    "#     orig_colors[idx] = \"darkgreen\"\n",
    "#     abl_colors[idx] = \"green\"\n",
    "\n",
    "x.append(repr(correct_next_str_tok))\n",
    "y_orig.append(logprobs_orig[correct_next_token].item())\n",
    "y_abl.append(logprobs_abl[correct_next_token].item())\n",
    "orig_colors.append(\"darkgreen\")\n",
    "abl_colors.append(\"green\")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data = [\n",
    "        go.Bar(x=x, y=y_orig, name='Original', marker_color=[\"#FF7700\"] * (len(x)-1) + [\"#024B7A\"]), # 7A30AB\n",
    "        go.Bar(x=x, y=y_abl, name='Ablated', marker_color=[\"#FFAE49\"] * (len(x)-1) + [\"#44A5C2\"]), # D44BFA\n",
    "    ],\n",
    "    # data = [\n",
    "    #     go.Bar(x=x[:-1], y=y_orig[:-1], name='Original', marker_color=\"#FF7700\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=x[:-1], y=y_abl[:-1], name='Ablated', marker_color=\"#FFAE49\", legendgroup=\"group1\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_orig[-1]], name='Original (correct token)', marker_color=\"#024B7A\", legendgroup=\"group2\"),\n",
    "    #     go.Bar(x=[x[-1]], y=[y_abl[-1]], name='Ablated (correct token)', marker_color=\"#44A5C2\", legendgroup=\"group2\"),\n",
    "    # ],\n",
    "    layout = dict(\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=30,\n",
    "        title=\"Logprobs: original vs ablated\",\n",
    "        xaxis_title_text=\"Predicted next token\",\n",
    "        yaxis_title_text=\"Logprob\",\n",
    "        width=800,\n",
    "        bargap=0.35,\n",
    "    )\n",
    ")\n",
    "fig.data = fig.data #+ ({\"name\": \"New\"},)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_matrix_histogram(W_EE_dict, \" device\", k=10, include=[\" devices\"], circuit=\"OV\", neg=True, head=(10, 7))\n",
    "plot_full_matrix_histogram(W_EE_dict, \" devices\", k=10, include=[\" device\"], circuit=\"QK\", neg=False, head=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE = W_EE_dict[\"W_E (including MLPs)\"]\n",
    "W_EE = W_EE_dict[\"W_E (only MLPs)\"]\n",
    "W_U = W_EE_dict[\"W_U\"].T\n",
    "\n",
    "tok_strs = [\"pier\"]\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i].capitalize())\n",
    "for i in range(len(tok_strs)): tok_strs.append(tok_strs[i] + \"s\")\n",
    "for i in range(len(tok_strs)): tok_strs.append(\" \" + tok_strs[i])\n",
    "tok_strs = [s for s in tok_strs if model.to_tokens(s, prepend_bos=False).squeeze().ndim == 0]\n",
    "\n",
    "toks = model.to_tokens(tok_strs, prepend_bos=False).squeeze()\n",
    "\n",
    "W_EE_toks = W_EE[toks]\n",
    "W_EE_normed = W_EE_toks / W_EE_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_embeddings = W_EE_normed @ W_EE_normed.T\n",
    "\n",
    "W_U_toks = W_U.T[toks]\n",
    "W_U_normed = W_U_toks / W_U_toks.norm(dim=-1, keepdim=True)\n",
    "cos_sim_unembeddings = W_U_normed @ W_U_normed.T\n",
    "\n",
    "W_EE_OV_toks_107 = W_EE_toks @ model.W_V[10, 7] @ model.W_O[10, 7]\n",
    "W_EE_OV_toks_99 = W_EE_toks @ model.W_V[9, 9] @ model.W_O[9, 9]\n",
    "W_EE_OV_toks_107_normed = W_EE_OV_toks_107 / W_EE_OV_toks_107.norm(dim=-1, keepdim=True)\n",
    "W_EE_OV_toks_99_normed = W_EE_OV_toks_99 / W_EE_OV_toks_99.norm(dim=-1, keepdim=True)\n",
    "cos_sim_107 = W_EE_OV_toks_107_normed @ W_EE_OV_toks_107_normed.T\n",
    "cos_sim_99 = W_EE_OV_toks_99_normed @ W_EE_OV_toks_99_normed.T\n",
    "\n",
    "imshow(\n",
    "    t.stack([\n",
    "        cos_sim_embeddings,\n",
    "        cos_sim_unembeddings,\n",
    "        cos_sim_107,\n",
    "        cos_sim_99,\n",
    "    ]),\n",
    "    x = list(map(repr, tok_strs)),\n",
    "    y = list(map(repr, tok_strs)),\n",
    "    title = \"Cosine similarity of variants of ' pier'\",\n",
    "    facet_col = 0,\n",
    "    facet_labels = [\"Effective embeddings\", \"Unembeddings\", \"W_OV output (10.7)\", \"W_OV output (9.9)\"],\n",
    "    border = True,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "# W_EE_OV_normed = W_EE_OV / W_EE_OV.std(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create OV and QK circuits Streamlit page\n",
    "\n",
    "I need to save the following things:\n",
    "\n",
    "* The QK and OV matrices for head 10.7 and 11.10\n",
    "* The extended embedding and unembedding matrices\n",
    "* The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_store = {\n",
    "    \"tokenizer\": model.tokenizer,\n",
    "    \"W_V_107\": model.W_V[10, 7],\n",
    "    \"W_O_107\": model.W_O[10, 7],\n",
    "    \"W_V_1110\": model.W_V[11, 10],\n",
    "    \"W_O_1110\": model.W_O[11, 10],\n",
    "    \"W_Q_107\": model.W_Q[10, 7],\n",
    "    \"W_K_107\": model.W_K[10, 7],\n",
    "    \"W_Q_1110\": model.W_Q[11, 10],\n",
    "    \"W_K_1110\": model.W_K[11, 10],\n",
    "    \"b_Q_107\": model.b_Q[10, 7],\n",
    "    \"b_K_107\": model.b_K[10, 7],\n",
    "    \"b_Q_1110\": model.b_Q[11, 10],\n",
    "    \"b_K_1110\": model.b_K[11, 10],\n",
    "    \"W_EE\": W_EE_dict[\"W_E (including MLPs)\"],\n",
    "    \"W_U\": model.W_U,\n",
    "}\n",
    "dict_to_store = {k: v.half() if isinstance(v, t.Tensor) else v for k, v in dict_to_store.items()}\n",
    "\n",
    "with gzip.open(_ST_HTML_PATH / f\"OV_QK_circuits.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict_to_store, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate `explore_prompts` HTML plots for Streamlit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PLOTS = generate_4_html_plots(\n",
    "    model_results = MODEL_RESULTS,\n",
    "    model = model,\n",
    "    data_toks = DATA_TOKS,\n",
    "    data_str_toks_parsed = DATA_STR_TOKS_PARSED,\n",
    "    negative_heads = NEGATIVE_HEADS,\n",
    "    save_files = True,\n",
    "    progress_bar = True,\n",
    "    restrict_computation = [\"LOSS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")].norm().item(), \n",
    "    model.W_U.T[model.to_single_token(\" Pier\")].norm().item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    model.W_U.T[model.to_single_token(\" pier\")],\n",
    "    model.W_U.T[model.to_single_token(\" Pier\")],\n",
    "    dim=-1\n",
    ").item()\n",
    "\n",
    "\n",
    "pier = model.W_U.T[model.to_single_token(\" pier\")]\n",
    "Pier = model.W_U.T[model.to_single_token(\" Pier\")]\n",
    "pier /= pier.norm()\n",
    "Pier /= Pier.norm()\n",
    "print(pier @ Pier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_U(s):\n",
    "    return model.W_U.T[model.to_single_token(s)]\n",
    "def W_EE0(s):\n",
    "    return W_EE_dict[\"W_E (only MLPs)\"][model.to_single_token(s)]\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return v1 @ v2 / (v1.norm() * v2.norm())\n",
    "\n",
    "print(f\"Unembeddings cosine similarity (Berkeley) = {cos_sim(W_U('keley'), W_U(' Berkeley')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (Berkeley)   = {cos_sim(W_EE0('keley'), W_EE0(' Berkeley')):.3f}\") \n",
    "print(\"\")\n",
    "print(f\"Unembeddings cosine similarity (pier) = {cos_sim(W_U(' pier'), W_U(' Pier')):.3f}\") \n",
    "print(f\"Embeddings cosine similarity (pier)   = {cos_sim(W_EE0(' pier'), W_EE0(' Pier')):.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE0(\" screen\") - W_EE0(\" screens\"),\n",
    "    W_EE0(\" device\") - W_EE0(\" devices\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cosine_similarity(\n",
    "    W_EE(\" computer\") - W_EE(\" computers\"),\n",
    "    W_EE(\" sign\") - W_EE(\" signs\"),\n",
    "    dim=-1\n",
    ").item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
