<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="transformer_lens.utilities package" href="transformer_lens.utilities.html" /><link rel="prev" title="Citation" href="content/citation.html" />

    <link rel="shortcut icon" href="_static/favicon.ico"/><!-- Generated with Sphinx 5.2.3 and Furo 2023.03.27 -->
        <title>transformer_lens package - TransformerLens Documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">TransformerLens Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="_static/transformer_lens_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">TransformerLens Documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="content/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="content/gallery.html">Gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="content/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="content/citation.html">Citation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code</span></p>
<ul class="current">
<li class="toctree-l1 current has-children current-page"><a class="current reference internal" href="#">transformer_lens package</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="transformer_lens.utilities.html">transformer_lens.utilities package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_properties_table.html">Model Properties Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="content/development.html">Local Development</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neelnanda-io/TransformerLens">Github</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="transformer-lens-package">
<h1>transformer_lens package<a class="headerlink" href="#transformer-lens-package" title="Permalink to this heading">#</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="transformer_lens.utilities.html">transformer_lens.utilities package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transformer_lens.utilities.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="transformer_lens.utilities.html#module-transformer_lens.utilities.devices">transformer_lens.utilities.devices module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.utilities.html#transformer_lens.utilities.devices.get_device_for_block_index"><code class="docutils literal notranslate"><span class="pre">get_device_for_block_index()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="transformer_lens.utilities.html#transformer_lens.utilities.devices.move_to_and_update_config"><code class="docutils literal notranslate"><span class="pre">move_to_and_update_config()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transformer_lens.utilities.html#module-transformer_lens.utilities">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">#</a></h2>
</section>
<section id="module-transformer_lens.ActivationCache">
<span id="transformer-lens-activationcache-module"></span><h2>transformer_lens.ActivationCache module<a class="headerlink" href="#module-transformer_lens.ActivationCache" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.ActivationCache.</span></span><span class="sig-name descname"><span class="pre">ActivationCache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_batch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A wrapper around a dictionary of cached activations from a model run, with a variety of helper functions. In general, any utility which is specifically about editing/processing activations should be a method here, while any utility which is more general should be a function in utils.py, and any utility which is specifically about model weights should be in HookedTransformer.py or components.py.</p>
<p>NOTE: This is designed to be used with the HookedTransformer class, and will not work with other models. It’s also designed to be used with all activations of HookedTransformer being cached, and some internal methods will break without that.</p>
<p>WARNING: The biggest footgun and source of bugs in this code will be keeping track of indexes, dimensions, and the numbers of each. There are several kinds of activations:</p>
<p>Internal attn head vectors: q, k, v, z. Shape [batch, pos, head_index, d_head]
Internal attn pattern style results: pattern (post softmax), attn_scores (pre-softmax). Shape [batch, head_index, query_pos, key_pos]
Attn head results: result. Shape [batch, pos, head_index, d_model]
Internal MLP vectors: pre, post, mid (only used for solu_ln - the part between activation + layernorm). Shape [batch, pos, d_mlp]
Residual stream vectors: resid_pre, resid_mid, resid_post, attn_out, mlp_out, embed, pos_embed, normalized (output of each LN or LNPre). Shape [batch, pos, d_model]
LayerNorm Scale: scale. Shape [batch, pos, 1]</p>
<p>Sometimes the batch dimension will be missing because we applied remove_batch_dim (used when batch_size=1), and we need functions to be robust to that. I THINK I’ve got everything working, but could easily be wrong!</p>
<p>Type-Annotations key:
layers_covered is the number of layers queried in functions that stack the residual stream.
batch_and_pos_dims is the set of dimensions from batch and pos - by default this is [“batch”, “pos”], but is only [“pos”] if we’ve removed the batch dimension and is [()] if we’ve removed batch dimension and are applying a pos slice which indexes a specific position.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.accumulated_resid">
<span class="sig-name descname"><span class="pre">accumulated_resid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_mid</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'layers_covered</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.accumulated_resid" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the accumulated residual stream up to a given layer, ie a stack of previous residual streams up to that layer’s input. This can be thought of as a series of partial values of the residual stream, where the model gradually accumulates what it wants.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (int, <em>optional</em>) – The layer to take components up to - by default includes resid_pre for that layer and excludes resid_mid and resid_post for that layer. layer==n_layers, -1 or None means to return all residual streams, including the final one (ie immediately pre logits). The indices are taken such that this gives the accumulated streams up to the input to layer l</p></li>
<li><p><strong>incl_mid</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return resid_mid for all previous layers. Defaults to False.</p></li>
<li><p><strong>mlp_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to include resid_mid for the current layer - essentially giving MLP input rather than Attn input. Defaults to False.</p></li>
<li><p><strong>apply_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply LayerNorm to the stack. Defaults to False.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a>) – A slice object to apply to the pos dimension. Defaults to None, do nothing.</p></li>
<li><p><strong>return_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return a list of labels for the residual stream components. Useful for labelling graphs. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A [num_components, batch_size, pos, d_model] tensor of the accumulated residual streams.
(labels): An optional list of labels for the components.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Components</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.apply_ln_to_stack">
<span class="sig-name descname"><span class="pre">apply_ln_to_stack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">residual_stack</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_batch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.apply_ln_to_stack" title="Permalink to this definition">#</a></dt>
<dd><p>Takes a stack of components of the residual stream (eg outputs of decompose_resid or accumulated_resid), treats them as the input to a specific layer, and applies the layer norm scaling of that layer to them, using the cached scale factors - simulating what that component of the residual stream contributes to that layer’s input.</p>
<p>The layernorm scale is global across the entire residual stream for each layer, batch element and position, which is why we need to use the cached scale factors rather than just applying a new LayerNorm.</p>
<p>If the model does not use LayerNorm, it returns the residual stack unchanged.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>residual_stack</strong> (<em>torch.Tensor</em>) – A tensor, whose final dimension is
d_model. The other trailing dimensions are assumed to be the
same as the stored hook_scale - which may or may not include
batch or position dimensions.</p></li>
<li><p><strong>layer</strong> (<em>int</em>) – The layer we’re taking the input to. In [0, n_layers],
n_layers means the unembed. None maps to the n_layers case, ie
the unembed.</p></li>
<li><p><strong>mlp_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the input is to the MLP or attn
(ie ln2 vs ln1). Defaults to False, ie ln1. If layer==n_layers,
must be False, and we use ln_final</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – The slice to take of positions, if residual_stack is not
over the full context, None means do nothing. It is assumed that
pos_slice has already been applied to residual_stack, and this
is only applied to the scale. See utils.Slice for details.
Defaults to None, do nothing.</p></li>
<li><p><strong>batch_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – The slice to take on the batch dimension.
Defaults to None, do nothing.</p></li>
<li><p><strong>has_batch_dim</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether residual_stack has a batch dimension.
Defaults to True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.apply_slice_to_batch_dim">
<span class="sig-name descname"><span class="pre">apply_slice_to_batch_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.apply_slice_to_batch_dim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.compute_head_results">
<span class="sig-name descname"><span class="pre">compute_head_results</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.compute_head_results" title="Permalink to this definition">#</a></dt>
<dd><p>Computes and caches the results for each attention head, ie the amount contributed to the residual stream from that head. attn_out for a layer is the sum of head results plus b_O. Intended use is to enable use_attn_results when running and caching the model, but this can be useful if you forget.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.decompose_resid">
<span class="sig-name descname"><span class="pre">decompose_resid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">all</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">mlp</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">attn</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_embeds</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'layers_covered</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.decompose_resid" title="Permalink to this definition">#</a></dt>
<dd><p>Decomposes the residual stream input to layer L into a stack of the output of previous layers. The sum of these is the input to layer L (plus embedding and pos embedding). This is useful for attributing model behaviour to different components of the residual stream</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>int</em>) – The layer to take components up to - by default includes
resid_pre for that layer and excludes resid_mid and resid_post for that layer. layer==n_layers means to return all layer outputs incl in the final layer, layer==0 means just embed and pos_embed. The indices are taken such that this gives the accumulated streams up to the input to layer l</p></li>
<li><p><strong>incl_mid</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return resid_mid for all previous
layers. Defaults to False.</p></li>
<li><p><strong>mlp_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to include attn_out for the current
layer - essentially decomposing the residual stream that’s input to the MLP input rather than the Attn input. Defaults to False.</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – Values are “all”, “mlp” or “attn”. “all” returns all
components, “mlp” returns only the MLP components, and “attn” returns only the attention components. Defaults to “all”.</p></li>
<li><p><strong>apply_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply LayerNorm to the stack. Defaults to False.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a>) – A slice object to apply to the pos dimension.
Defaults to None, do nothing.</p></li>
<li><p><strong>incl_embeds</strong> (<em>bool</em>) – Whether to include embed &amp; pos_embed</p></li>
<li><p><strong>return_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return a list of labels for
the residual stream components. Useful for labelling graphs. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A [num_components, batch_size, pos, d_model] tensor of the accumulated residual streams.
(labels): An optional list of labels for the components.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Components</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.get_full_resid_decomposition">
<span class="sig-name descname"><span class="pre">get_full_resid_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expand_neurons</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.get_full_resid_decomposition" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the full decomposition of the residual stream into embed, pos_embed, each head result, each neuron result, and the accumulated biases. We break down the residual stream that is input into some layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>int</em>) – The layer we’re inputting into. layer is in [0, n_layers], if layer==n_layers (or None) we’re inputting into the unembed (the entire stream), if layer==0 then it’s just embed and pos_embed</p></li>
<li><p><strong>mlp_input</strong> (<em>bool</em><em>, </em><em>optional</em>) – Are we inputting to the MLP in that layer or the attn? Must be False for final layer, since that’s the unembed. Defaults to False.</p></li>
<li><p><strong>expand_neurons</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to expand the MLP outputs to give every neuron’s result or just return the MLP layer outputs. Defaults to True.</p></li>
<li><p><strong>apply_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply LayerNorm to the stack. Defaults to False.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – Slice of the positions to take. Defaults to None. See utils.Slice for details.</p></li>
<li><p><strong>return_labels</strong> (<em>bool</em>) – Whether to return the labels. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.get_neuron_results">
<span class="sig-name descname"><span class="pre">get_neuron_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neuron_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*batch_and_pos_dims</span> <span class="pre">num_neurons</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.get_neuron_results" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the results of for neurons in a specific layer (ie, how much each neuron contributes to the residual stream). Does it for the subset of neurons specified by neuron_slice, defaults to all of them. Does <em>not</em> cache these because it’s expensive in space and cheap to compute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>int</em>) – Layer index</p></li>
<li><p><strong>neuron_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – Slice of the neuron. Defaults to None.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – Slice of the positions. Defaults to None. See utils.Slice for details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>[batch_size, pos, d_mlp, d_model] tensor of the results (d_mlp is the neuron index axis)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.items">
<span class="sig-name descname"><span class="pre">items</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.items" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.keys">
<span class="sig-name descname"><span class="pre">keys</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.keys" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.logit_attrs">
<span class="sig-name descname"><span class="pre">logit_attrs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">residual_stack</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">position'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incorrect_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">position'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_batch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.logit_attrs" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the logit attributions for the residual stack on an input of tokens, or the logit difference attributions for the residual stack if incorrect_tokens is provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>residual_stack</strong> (Float[torch.Tensor, “num_components <a href="#id1"><span class="problematic" id="id2">*</span></a>batch_and_pos_dims d_model”]) – stack of components of residual stream to get logit attributions for.</p></li>
<li><p><strong>tokens</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;&quot;</em><em>]</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;batch&quot;</em><em>]</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;batch position&quot;</em><em>]</em><em>]</em>) – tokens to compute logit attributions on.</p></li>
<li><p><strong>incorrect_tokens</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;&quot;</em><em>]</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;batch&quot;</em><em>]</em><em>, </em><em>Int</em><em>[</em><em>torch.Tensor</em><em>, </em><em>&quot;batch position&quot;</em><em>]</em><em>]</em><em>, </em><em>optional</em>) – if provided, compute attributions on logit difference between tokens and incorrect_tokens.
Must have the same shape as tokens.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – The slice to apply layer norm scaling on.
Defaults to None, do nothing.</p></li>
<li><p><strong>batch_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – The slice to take on the batch dimension during layer norm scaling.
Defaults to None, do nothing.</p></li>
<li><p><strong>has_batch_dim</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether residual_stack has a batch dimension.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A [num_components, <a href="#id3"><span class="problematic" id="id4">*</span></a>batch_and_pos_dims] tensor of the logit attributions or logit difference attributions if incorrect_tokens was provided.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Components</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.remove_batch_dim">
<span class="sig-name descname"><span class="pre">remove_batch_dim</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.remove_batch_dim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.stack_activation">
<span class="sig-name descname"><span class="pre">stack_activation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayer_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'layers_covered</span> <span class="pre">...'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.stack_activation" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a stack of all head results (ie residual stream contribution) up to layer L. A good way to decompose the outputs of attention layers into attribution by specific heads. The output shape is exactly the same shape as the activations, just with a leading layers dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation_name</strong> (<em>str</em>) – The name of the activation to be stacked</p></li>
<li><p><strong>layer</strong> (<em>int</em>) – Layer index - heads at all layers strictly before this are included. layer must be in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer</p></li>
<li><p><strong>sublayer_type</strong> (str, <em>optional</em>) – The sub layer type of the activation, passed to utils.get_act_name. Can normally be inferred</p></li>
<li><p><strong>incl_remainder</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return a final term which is “the rest of the residual stream”. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.stack_head_results">
<span class="sig-name descname"><span class="pre">stack_head_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_remainder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.stack_head_results" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a stack of all head results (ie residual stream contribution) up to layer L. A good way to decompose the outputs of attention layers into attribution by specific heads. Note that the num_components axis has length layer x n_heads ((layer head_index) in einops notation)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>int</em>) – Layer index - heads at all layers strictly before this are included. layer must be in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer</p></li>
<li><p><strong>return_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to also return a list of labels of the form “L0H0” for the heads. Defaults to False.</p></li>
<li><p><strong>incl_remainder</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return a final term which is “the rest of the residual stream”. Defaults to False.</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a>) – A slice object to apply to the pos dimension. Defaults to None, do nothing.</p></li>
<li><p><strong>apply_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply LayerNorm to the stack. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.stack_neuron_results">
<span class="sig-name descname"><span class="pre">stack_neuron_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neuron_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><span class="pre">Slice</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_remainder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'num_components</span> <span class="pre">*batch_and_pos_dims</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.stack_neuron_results" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a stack of all neuron results (ie residual stream contribution) up to layer L - ie the amount each individual neuron contributes to the residual stream. Also returns a list of labels of the form “L0N0” for the neurons. A good way to decompose the outputs of MLP layers into attribution by specific neurons.</p>
<p>Note that doing this for all neurons is SUPER expensive on GPU memory and only works for small models or short inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>int</em>) – Layer index - heads at all layers strictly before this are included. layer must be in [1, n_layers]</p></li>
<li><p><strong>pos_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – Slice of the positions. Defaults to None. See utils.Slice for details.</p></li>
<li><p><strong>neuron_slice</strong> (<a class="reference internal" href="#transformer_lens.utils.Slice" title="transformer_lens.utils.Slice"><em>Slice</em></a><em>, </em><em>optional</em>) – Slice of the neurons. Defaults to None. See utils.Slice for details.</p></li>
<li><p><strong>return_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to also return a list of labels of the form “L0H0” for the heads. Defaults to False.</p></li>
<li><p><strong>incl_remainder</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return a final term which is “the rest of the residual stream”. Defaults to False.</p></li>
<li><p><strong>apply_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to apply LayerNorm to the stack. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a></span></span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.to" title="Permalink to this definition">#</a></dt>
<dd><p>Moves the cache to a device - mostly useful for moving it to CPU after model computation finishes to save GPU memory. Matmuls will be much slower on the CPU.</p>
<p>Note that some methods will break unless the model is also moved to the same device, eg compute_head_results</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.toggle_autodiff">
<span class="sig-name descname"><span class="pre">toggle_autodiff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.toggle_autodiff" title="Permalink to this definition">#</a></dt>
<dd><p>Sets autodiff to mode (defaults to turning it off).
WARNING: This is pretty dangerous, since autodiff is global state - this turns off torch’s ability to take gradients completely and it’s easy to get a bunch of errors if you don’t realise what you’re doing.</p>
<p>But autodiff consumes a LOT of GPU memory (since every intermediate activation is cached until all downstream activations are deleted - this means that computing the loss and storing it in a list will keep every activation sticking around!). So often when you’re analysing a model’s activations, and don’t need to do any training, autodiff is more trouble than its worth.</p>
<p>If you don’t want to mess with global state, using torch.inference_mode as a context manager or decorator achieves similar effects :)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.ActivationCache.ActivationCache.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.ActivationCache.ActivationCache.values" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.FactoredMatrix">
<span id="transformer-lens-factoredmatrix-module"></span><h2>transformer_lens.FactoredMatrix module<a class="headerlink" href="#module-transformer_lens.FactoredMatrix" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.FactoredMatrix.</span></span><span class="sig-name descname"><span class="pre">FactoredMatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">A</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">ldim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">B</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">mdim</span> <span class="pre">rdim'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class to represent low rank factored matrices, where the matrix is represented as a product of two matrices. Has utilities for efficient calculation of eigenvalues, norm and SVD.</p>
<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.AB">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AB</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">ldim</span> <span class="pre">rdim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.AB" title="Permalink to this definition">#</a></dt>
<dd><p>The product matrix - expensive to compute, and can consume a lot of GPU memory</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.BA">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">BA</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">rdim</span> <span class="pre">ldim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.BA" title="Permalink to this definition">#</a></dt>
<dd><p>The reverse product. Only makes sense when ldim==rdim</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.S">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">S</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.S" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.T">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">T</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.T" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.U">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">U</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">ldim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.U" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.Vh">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Vh</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">rdim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.Vh" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.collapse_l">
<span class="sig-name descname"><span class="pre">collapse_l</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">mdim</span> <span class="pre">rdim'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.collapse_l" title="Permalink to this definition">#</a></dt>
<dd><p>Collapses the left side of the factorization by removing the orthogonal factor (given by self.U). Returns a (…, mdim, rdim) tensor</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.collapse_r">
<span class="sig-name descname"><span class="pre">collapse_r</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">ldim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.collapse_r" title="Permalink to this definition">#</a></dt>
<dd><p>Analogous to collapse_l, returns a (…, ldim, mdim) tensor</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.eigenvalues">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">eigenvalues</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.eigenvalues" title="Permalink to this definition">#</a></dt>
<dd><p>Eigenvalues of AB are the same as for BA (apart from trailing zeros), because if BAv=kv ABAv = A(BAv)=kAv, so Av is an eigenvector of AB with eigenvalue k.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.get_corner">
<span class="sig-name descname"><span class="pre">get_corner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.get_corner" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.make_even">
<span class="sig-name descname"><span class="pre">make_even</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.make_even" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the factored form of (U &#64; S.sqrt().diag(), S.sqrt().diag() &#64; Vh) where U, S, Vh are the SVD of the matrix. This is an equivalent factorisation, but more even - each half has half the singular values, and orthogonal rows/cols</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.ndim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ndim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.ndim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.norm">
<span class="sig-name descname"><span class="pre">norm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.norm" title="Permalink to this definition">#</a></dt>
<dd><p>Frobenius norm is sqrt(sum of squared singular values)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.pair">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">pair</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">ldim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">mdim</span> <span class="pre">rdim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.pair" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.svd">
<span class="sig-name descname"><span class="pre">svd</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">ldim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims</span> <span class="pre">rdim</span> <span class="pre">mdim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.svd" title="Permalink to this definition">#</a></dt>
<dd><p>Efficient algorithm for finding Singular Value Decomposition, a tuple (U, S, Vh) for matrix M st S is a vector and U, Vh are orthogonal matrices, and U &#64; S.diag() &#64; Vh.T == M</p>
<p>(Note that Vh is given as the transpose of the obvious thing)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.FactoredMatrix.FactoredMatrix.unsqueeze">
<span class="sig-name descname"><span class="pre">unsqueeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></span></span><a class="headerlink" href="#transformer_lens.FactoredMatrix.FactoredMatrix.unsqueeze" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.HookedEncoder">
<span id="transformer-lens-hookedencoder-module"></span><h2>transformer_lens.HookedEncoder module<a class="headerlink" href="#module-transformer_lens.HookedEncoder" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.HookedEncoder.</span></span><span class="sig-name descname"><span class="pre">HookedEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule" title="transformer_lens.hook_points.HookedRootModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">HookedRootModule</span></code></a></p>
<p>This class implements a BERT-style encoder using the components in ./components.py, with HookPoints on every interesting activation. It inherits from HookedRootModule.</p>
<p>Limitations:
- The current MVP implementation supports only the masked language modelling (MLM) task. Next sentence prediction (NSP), causal language modelling, and other tasks are not yet supported.
- Also note that model does not include dropouts, which may lead to inconsistent results from training or fine-tuning.</p>
<dl class="simple">
<dt>Like HookedTransformer, it can have a pretrained Transformer’s weights loaded via <cite>.from_pretrained</cite>. There are a few features you might know from HookedTransformer which are not yet supported:</dt><dd><ul class="simple">
<li><p>There is no preprocessing (e.g. LayerNorm folding) when loading a pretrained model</p></li>
<li><p>The model only accepts tokens as inputs, and not strings, or lists of strings</p></li>
</ul>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.OV">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">OV</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.OV" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.QK">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">QK</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.QK" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_E">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_E</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_E" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience to get the embedding matrix</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_E_pos">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_E_pos</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab+n_ctx</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_E_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Concatenated W_E and W_pos. Used as a full (overcomplete) basis of the input space, useful for full QK and full OV circuits.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_K">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_K</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_K" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the key weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_O">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_O</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_O" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the attn output weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_Q">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_Q</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_Q" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the query weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_U">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_U</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_model</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_U" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience to get the unembedding matrix (ie the linear map from the final residual stream to the output logits)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_V">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_V</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_V" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the value weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_in">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_in" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP input weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_out">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_out</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_mlp</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_out" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP output weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.W_pos">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_pos</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_ctx</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.W_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience function to get the positional embedding. Only works on models with absolute positional embeddings!</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.all_head_labels">
<span class="sig-name descname"><span class="pre">all_head_labels</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.all_head_labels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_K">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_K</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_K" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the key biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_O">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_O</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_O" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the attn output biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_Q">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_Q</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_Q" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the query biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_U">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_U</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_U" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_V">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_V</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_V" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the value biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_in">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_in" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP input biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.b_out">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_out</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.b_out" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP output biases across all layers</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.cpu" title="Permalink to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.cuda" title="Permalink to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">logits</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">one_zero_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.forward" title="Permalink to this definition">#</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">one_zero_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dd><p>Input must be a batch of tokens. Strings and lists of strings are not yet supported.</p>
<p>return_type Optional[str]: The type of output to return. Can be one of: None (return nothing, don’t calculate logits), or ‘logits’ (return logits).</p>
<p>token_type_ids Optional[torch.Tensor]: Binary ids indicating whether a token belongs to sequence A or B. For example, for two sentences: “[CLS] Sentence A [SEP] Sentence B [SEP]”, token_type_ids would be [0, 0, …, 0, 1, …, 1, 1]. <cite>0</cite> represents tokens from Sentence A, <cite>1</cite> from Sentence B. If not provided, BERT assumes a single sequence input. Typically, shape is (batch_size, sequence_length).</p>
<p>one_zero_attention_mask: Optional[torch.Tensor]: A binary mask which indicates which tokens should be attended to (1) and which should be ignored (0). Primarily used for padding variable-length sentences in a batch. For instance, in a batch with sentences of differing lengths, shorter sentences are padded with 0s on the right. If not provided, the model assumes all tokens should be attended to.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">from_pretrained_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder" title="transformer_lens.HookedEncoder.HookedEncoder"><span class="pre">HookedEncoder</span></a></span></span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.from_pretrained" title="Permalink to this definition">#</a></dt>
<dd><p>Loads in the pretrained weights from huggingface. Currently supports loading weight from HuggingFace BertForMaskedLM. Unlike HookedTransformer, this does not yet do any preprocessing on the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.run_with_cache">
<span class="sig-name descname"><span class="pre">run_with_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_cache_object</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">True</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.run_with_cache" title="Permalink to this definition">#</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">run_with_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_cache_object</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">False</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dd><p>Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule. This function was copied directly from HookedTransformer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_or_dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_details</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.to" title="Permalink to this definition">#</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.channels_last</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point or complex <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>s. In addition, this method will
only cast the floating point or complex parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point or complex dtype of
the parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
<li><p><strong>memory_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>) – the desired memory
format for 4D parameters and buffers in this module (keyword
only argument)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="go">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">))</span>
<span class="go">tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedEncoder.HookedEncoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.HookedEncoder.HookedEncoder.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.HookedTransformer">
<span id="transformer-lens-hookedtransformer-module"></span><h2>transformer_lens.HookedTransformer module<a class="headerlink" href="#module-transformer_lens.HookedTransformer" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.HookedTransformer.</span></span><span class="sig-name descname"><span class="pre">HookedTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule" title="transformer_lens.hook_points.HookedRootModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">HookedRootModule</span></code></a></p>
<p>This class implements a full Transformer using the components in ./components.py, with
HookPoints on every interesting activation. It inherits from HookedRootModule.</p>
<p>It can have a pretrained Transformer’s weights automatically loaded in via the HookedTransformer.from_pretrained
class method. It can also be instantiated with randomly initialized weights via __init__ and being passed a dict or
HookedTransformerConfig object.</p>
<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.OV">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">OV</span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.OV" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.QK">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">QK</span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.QK" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_E">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_E</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_E" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience to get the embedding matrix</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_E_pos">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_E_pos</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab+n_ctx</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_E_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Concatenated W_E and W_pos. Used as a full (overcomplete) basis of the input space, useful for full QK and full OV circuits.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_K">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_K</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_K" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the key weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_O">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_O</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_O" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the attn output weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_Q">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_Q</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_Q" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the query weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_U">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_U</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_model</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_U" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience to get the unembedding matrix (ie the linear map from the final residual stream to the output logits)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_V">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_V</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_model</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_V" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the value weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_in">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_in" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP input weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_out">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_out</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_mlp</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_out" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP output weights across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.W_pos">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">W_pos</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_ctx</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.W_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Convenience function to get the positional embedding. Only works on models with absolute positional embeddings!</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.accumulated_bias">
<span class="sig-name descname"><span class="pre">accumulated_bias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_mlp_biases</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'layers_accumulated_over</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.accumulated_bias" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the accumulated bias from all layer outputs (ie the b_Os and b_outs), up to the input of layer L.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>from the attention output</em><em> of </em><em>the current</em>) – Layer number, in [0, n_layers]. layer==0 means no layers, layer==n_layers means all layers.</p></li>
<li><p><strong>mlp_input</strong> (<em>bool</em>) – If True, we take the bias up to the input of the MLP of layer L (ie we include the bias</p></li>
<li><p><strong>layer</strong> – </p></li>
<li><p><strong>layers</strong><strong>)</strong> (<em>otherwise just biases from previous</em>) – </p></li>
<li><p><strong>include_mlp_biases</strong> (<em>bool</em>) – Whether to include the biases of MLP layers. Often useful to have as False if</p></li>
<li><p><strong>heads</strong> (<em>we're expanding attn_out into individual</em>) – </p></li>
<li><p><strong>is.</strong> (<em>but keeping mlp_out as</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>[d_model], accumulated bias</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bias (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.all_composition_scores">
<span class="sig-name descname"><span class="pre">all_composition_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">n_layers</span> <span class="pre">n_heads'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.all_composition_scores" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the Composition scores for all pairs of heads, as a L1, H1, L2, H2 tensor (which is upper triangular
on the first and third axes)</p>
<p>mode is one of [“Q”, “K”, “V”]</p>
<p>See <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition">https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition</a>
for three metrics used</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.all_head_labels">
<span class="sig-name descname"><span class="pre">all_head_labels</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.all_head_labels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_K">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_K</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_K" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the key biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_O">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_O</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_O" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the attn output biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_Q">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_Q</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_Q" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the query biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_U">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_U</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_vocab'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_U" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_V">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_V</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_V" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the value biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_in">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_in" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP input biases across all layers</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.b_out">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">b_out</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_layers</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.b_out" title="Permalink to this definition">#</a></dt>
<dd><p>Stacks the MLP output biases across all layers</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.center_unembed">
<span class="sig-name descname"><span class="pre">center_unembed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.center_unembed" title="Permalink to this definition">#</a></dt>
<dd><p>Centers the unembedding weights W_U. This is done by subtracting the mean of the weights from the weights
themselves. This is done in-place. As softmax is translation invariant, this changes the logits but not the
log probs, and makes the model logits (slightly) more interpretable - when trying to understand how components
contribute to the logits, we’ll be less misled by components that just add something to every logit.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.center_writing_weights">
<span class="sig-name descname"><span class="pre">center_writing_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.center_writing_weights" title="Permalink to this definition">#</a></dt>
<dd><p>Centers the weights of the model that write to the residual stream - W_out, W_E, W_pos and W_out. This is
done by subtracting the mean of the weights from the weights themselves. This is done in-place. See
fold_layer_norm for more details.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.check_hooks_to_add">
<span class="sig-name descname"><span class="pre">check_hooks_to_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook_point</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook_point_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.check_hooks_to_add" title="Permalink to this definition">#</a></dt>
<dd><p>Override this function to add checks on which hooks should be added</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.cpu" title="Permalink to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.cuda" title="Permalink to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.fold_layer_norm">
<span class="sig-name descname"><span class="pre">fold_layer_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.fold_layer_norm" title="Permalink to this definition">#</a></dt>
<dd><p>Takes in a state dict from a pretrained model, formatted to be consistent with HookedTransformer but with
LayerNorm weights and biases. Folds these into the neighbouring weights. See further_comments.md for more details</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – State dict of pretrained model</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.fold_value_biases">
<span class="sig-name descname"><span class="pre">fold_value_biases</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.fold_value_biases" title="Permalink to this definition">#</a></dt>
<dd><p>Fold the value biases into the output bias. Because attention patterns add up to 1, the value biases always
have a constant effect on a head’s output. Further, as the outputs of each head in a layer add together, each
head’s value bias has a constant effect on the <em>layer’s</em> output, which can make it harder to interpret the
effect of any given head, and it doesn’t matter which head a bias is associated with.
We can factor this all into a single output bias to the layer, and make it easier to interpret the head’s output.
Formally, we take b_O_new = b_O_original + sum_head(b_V_head &#64; W_O_head)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">logits</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_at_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><span class="pre">HookedTransformerKeyValueCache</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos-1'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.forward" title="Permalink to this definition">#</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">loss</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_at_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><span class="pre">HookedTransformerKeyValueCache</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos-1'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">both</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_at_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><span class="pre">HookedTransformerKeyValueCache</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos-1'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_at_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><span class="pre">HookedTransformerKeyValueCache</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span></dt>
<dd><p>Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically tokenized to a
batch of a single element. The prepend_bos flag only applies when inputting a text string.</p>
<dl class="simple">
<dt>return_type Optional[str]: The type of output to return. Can be one of: None (return nothing, don’t calculate</dt><dd><p>logits), ‘logits’ (return logits), ‘loss’ (return cross-entropy loss), ‘both’ (return logits and loss)</p>
</dd>
<dt>loss_per_token bool: Whether to return the (next token prediction) loss per token (True) or average (False).</dt><dd><p>Average loss is a scalar (averaged over position <em>and</em> batch), per-token loss is a tensor ([batch, position-1])
- position-1 because we’re predicting the next token, and there’s no specified next token for the final
token. Defaults to False.</p>
</dd>
<dt>prepend_bos bool: Whether to prepend the BOS token to the input. Only applies when input is a string. Defaults</dt><dd><p>to True (unlike to_tokens) - even for models not explicitly trained with this, heads often use the first
position as a resting position and accordingly lose information from the first token, so this empirically
seems to give better results.</p>
</dd>
</dl>
<p>stop_at_layer Optional[int]: If not None, stop the forward pass at the specified layer. Exclusive - ie,
stop_at_layer = 0 will only run the embedding layer, stop_at_layer = 1 will run the embedding layer and the
first transformer block, etc. Supports negative indexing. Useful for analysis of intermediate layers, eg finding
neuron activations in layer 3 of a 24 layer model. Defaults to None (run the full model).</p>
<p>Note that loss is the standard “predict the next token” cross-entropy loss for GPT-2 style language models -
if you want a custom loss function, the recommended behaviour is returning the logits and then applying your
custom loss function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_writing_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_unembed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refactor_factored_attn_matrices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hf_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">from_pretrained_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer" title="transformer_lens.HookedTransformer.HookedTransformer"><span class="pre">HookedTransformer</span></a></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.from_pretrained" title="Permalink to this definition">#</a></dt>
<dd><p>Class method to load in a pretrained model weights to the HookedTransformer format and optionally to do some
processing to make the model easier to interpret. Currently supports loading from most autoregressive
HuggingFace models (GPT2, GPTNeo, GPTJ, OPT) and from a range of toy models and SoLU models trained by me (Neel Nanda).</p>
<p>Also supports loading from a checkpoint for checkpointed models (currently, models trained by me (NeelNanda) and
the stanford-crfm models). These can either be determined by the checkpoint index (the index of the checkpoint
in the checkpoint list) or by the checkpoint value (the value of the checkpoint, eg 1000 for a checkpoint taken
at step 1000 or after 1000 tokens. Each model has checkpoints labelled with exactly one of labels and steps).
If neither is specified the final model is loaded. If both are specified, the checkpoint index is used.</p>
<p>See load_and_process_state_dict for details on the processing (folding layer norm, centering the unembedding and
centering the writing weights)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_name</strong> (<em>str</em>) – The model name - must be an element of OFFICIAL_MODEL_NAMES or an alias of one.</p></li>
<li><p><strong>fold_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to fold in the LayerNorm weights to the
subsequent linear layer. This does not change the computation.
Defaults to True.</p></li>
<li><p><strong>center_writing_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to center weights</p></li>
<li><p><strong>to</strong> (<em>writing</em>) – the residual stream (ie set mean to be zero). Due to LayerNorm
this doesn’t change the computation. Defaults to True.</p></li>
<li><p><strong>center_unembed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to center W_U (ie set mean</p></li>
<li><p><strong>zero</strong><strong>)</strong><strong>.</strong> (<em>to be</em>) – Softmax is translation invariant so this doesn’t affect log
probs or loss, but does change logits. Defaults to True.</p></li>
<li><p><strong>refactor_factored_attn_matrices</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to convert the factored
matrices (W_Q &amp; W_K, and W_O &amp; W_V) to be “even”. Defaults to False</p></li>
<li><p><strong>checkpoint_index</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a checkpoint, the index of
the checkpoint to load. Defaults to None.</p></li>
<li><p><strong>checkpoint_value</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a checkpoint, the value of
the checkpoint to load, ie the step or token number (each model
has checkpoints labelled with exactly one of these). Defaults to
None.</p></li>
<li><p><strong>hf_model</strong> (<em>AutoModelForCausalLM</em><em>, </em><em>optional</em>) – If you have already loaded in the
HuggingFace model, you can pass it in here rather than needing
to recreate the object. Defaults to None.</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – The device to load the model onto. By
default will load to CUDA if available, else CPU.</p></li>
<li><p><strong>n_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of devices to split the model
across. Defaults to 1. If greater than 1, <cite>device</cite> must be cuda.</p></li>
<li><p><strong>tokenizer</strong> (<a href="#id5"><span class="problematic" id="id6">*</span></a>optional) – The tokenizer to use for the model. If not
provided, it is inferred from cfg.tokenizer_name or initialized to None.
If None, then the model cannot be passed strings, and d_vocab must be explicitly set.</p></li>
<li><p><strong>move_to_device</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to move the model to the device specified in cfg.
device. Must be true if <cite>n_devices</cite> in the config is greater than 1, since the model’s layers
will be split across multiple devices.</p></li>
<li><p><strong>from_pretrained_kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Any other optional argument passed to HuggingFace’s
from_pretrained (e.g. “cache_dir” or “torch_dtype”). Also passed to other HuggingFace
functions when compatible. For some models or arguments it doesn’t work, especially for
models that are not internally loaded with HuggingFace’s from_pretrained (e.g. SoLU models).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.from_pretrained_no_processing">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained_no_processing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_writing_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_unembed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refactor_factored_attn_matrices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">from_pretrained_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.from_pretrained_no_processing" title="Permalink to this definition">#</a></dt>
<dd><p>Wrapper for from_pretrained with all boolean flags related to simplifying the model set to False. Refer to
from_pretrained for details.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_new_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_at_eos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_return_sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past_kv_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'input'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos_plus_new_tokens'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.generate" title="Permalink to this definition">#</a></dt>
<dd><p>Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.</p>
<p>To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish (by producing an
EOT token), we keep running the model on the entire batch, but throw away the output for a finished sequence
and just keep adding EOTs to pad.</p>
<p>This supports entering a single string, but not a list of strings - if the strings don’t tokenize to exactly the
same length, this gets messy. If that functionality is needed, convert them to a batch of tokens and input that
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>int</em>) – Either a batch of tokens ([batch, pos]) or a text string (this will be converted to a batch of tokens with batch size 1)</p></li>
<li><p><strong>max_new_tokens</strong> (<em>int</em>) – Maximum number of tokens to generate</p></li>
<li><p><strong>stop_at_eos</strong> (<em>bool</em>) – If True, stop generating tokens when the model outputs eos_token</p></li>
<li><p><strong>eos_token_id</strong> (int, <em>optional</em>) – The token ID to use for end of sentence. If None, use the tokenizer’s eos_token_id - required if using stop_at_eos</p></li>
<li><p><strong>do_sample</strong> (<em>bool</em>) – If True, sample from the model’s output distribution. Otherwise, use greedy search (take the max logit each time).</p></li>
<li><p><strong>top_k</strong> (<em>int</em>) – Number of tokens to sample from. If None, sample from all tokens</p></li>
<li><p><strong>top_p</strong> (<em>float</em>) – Probability mass to sample from. If 1.0, sample from all tokens. If &lt;1.0, we take the top tokens with cumulative probability &gt;= top_p</p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – Temperature for sampling. Higher values will make the model more random (limit of temp -&gt; 0 is just taking the top token, limit of temp -&gt; inf is sampling from a uniform distribution)</p></li>
<li><p><strong>freq_penalty</strong> (<em>float</em>) – Frequency penalty for sampling - how much to penalise previous tokens. Higher values will make the model more random</p></li>
<li><p><strong>use_past_kv_cache</strong> (<em>bool</em>) – If True, create and use cache to speed up generation</p></li>
<li><p><strong>prepend_bos</strong> (<em>bool</em>) – If True, prepend the model’s bos_token_id to the input, if it’s a string. Irrelevant if input is a tensor.</p></li>
<li><p><strong>return_type</strong> (str, <em>optional</em>) – The type of the output to return - either a string (str), a tensor of tokens (tensor) or whatever the format of the input was (input).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – If True, show tqdm progress bars for generation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>[batch, pos + max_new_tokens], generated sequence of new tokens - by default returns same type as input</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>outputs (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.get_token_position">
<span class="sig-name descname"><span class="pre">get_token_position</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">single_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'first'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.get_token_position" title="Permalink to this definition">#</a></dt>
<dd><p>Get the position of a single_token in a string or sequence of tokens. Raises an error if the token is not
present.</p>
<p>Gotcha: If you’re inputting a string, it’ll automatically be tokenized. Be careful about prepend_bos is true or
false! When a string is input to the model, a BOS (beginning of sequence) token is prepended by default when the
string is tokenized. But this should only be done at the START of the input, not when inputting part of the
prompt. If you’re getting weird off-by-one errors, check carefully for what the setting should be!</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>single_token</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – The token to search for. Can
be a token index, or a string (but the string must correspond to a
single token)</p></li>
<li><p><strong>input</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The sequence to
search in. Can be a string or a rank 1 tensor of tokens or a rank 2 tensor of tokens with a dummy batch
dimension.</p></li>
<li><p><strong>mode</strong> (<em>str</em><em>, </em><em>optional</em>) – If there are multiple matches, which match to return. Supports “first” or “last”.
Defaults to “first”.</p></li>
<li><p><strong>prepend_bos</strong> (<em>bool</em>) – Prepends a BOS (beginning of sequence) token when tokenizing a string. Only matters when
inputting a string to the function, otherwise ignored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.init_weights">
<span class="sig-name descname"><span class="pre">init_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.init_weights" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize weights matrices with a normal of std=initializer_range (default=0.02). This roughly follows the
GPT-2 paper’s scheme (but with truncation, and not halving the std for W_pos).</p>
<p>LayerNorm weights are already initialized to 1.0, and all biases are initialized to 0.0 (including LayerNorm),
so this just initializes weight matrices.</p>
<p>Weight matrices are set to empty by default (to save space + compute, since they’re the bulk of the parameters),
so it is important to call this if you are not loading in pretrained weights! Note that this function assumes that weight names being with <a href="#id9"><span class="problematic" id="id10">W_</span></a></p>
<p>Set seed here to ensure determinism.</p>
<p>This does NOT follow the PyTorch scheme, which as far as I can tell is super out of date but no one has gotten
round to updating it?
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/18182">https://github.com/pytorch/pytorch/issues/18182</a></p>
<p>PyTorch Transformers are especially bad - TransformerEncoder initializes all layers to the exact same weights?!
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/72253">https://github.com/pytorch/pytorch/issues/72253</a></p>
<p>The best paper I’ve found on transformer initialization is the muP paper, but haven’t integrated those ideas yet:
<a class="reference external" href="https://arxiv.org/abs/2203.03466">https://arxiv.org/abs/2203.03466</a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.load_and_process_state_dict">
<span class="sig-name descname"><span class="pre">load_and_process_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_writing_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_unembed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_value_biases</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refactor_factored_attn_matrices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.load_and_process_state_dict" title="Permalink to this definition">#</a></dt>
<dd><p>Method to load a state dict into the model, and to apply processing to simplify it. The state dict is assumed
to be in the HookedTransformer format.</p>
<p>See the relevant method (same name as the flag) for more details on the folding, centering and processing flags.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – The state dict of the model, in HookedTransformer format</p></li>
<li><p><strong>fold_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to fold in the LayerNorm weights to the
subsequent linear layer. This does not change the computation. Defaults to True.</p></li>
<li><p><strong>center_writing_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to center weights writing to the
residual stream (ie set mean to be zero). Due to LayerNorm this doesn’t change the computation.
Defaults to True.</p></li>
<li><p><strong>center_unembed</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to center W_U (ie set mean to be zero).
Softmax is translation invariant so this doesn’t affect log probs or loss, but does change logits.
Defaults to True.</p></li>
<li><p><strong>fold_value_biases</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to fold the value biases into the output bias.
Because attention patterns add up to 1, the value biases always have a constant effect on a layer’s
output, and it doesn’t matter which head a bias is associated with. We can factor this all into a single
output bias to the layer, and make it easier to interpret the head’s output.</p></li>
<li><p><strong>refactor_factored_attn_matrices</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to convert the factored
matrices (W_Q &amp; W_K, and W_O &amp; W_V) to be “even”. Defaults to False</p></li>
<li><p><strong>model_name</strong> (<em>str</em><em>, </em><em>optional</em>) – checks the model name for special cases of state dict loading. Only used for
Redwood 2L model currently</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.load_sample_training_dataset">
<span class="sig-name descname"><span class="pre">load_sample_training_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.load_sample_training_dataset" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to load in a 10K-20K dataset of elements from the model’s training data distribution.</p>
<p>Wrapper around utils.get_dataset, which identifies the appropriate dataset the pretrained models. Each dataset
has a ‘text’ field, which contains the relevant info, some have several meta data fields.</p>
<p>Kwargs will be passed to utils.get_dataset (e.g. cache_dir to set download location)</p>
<p>Notes:
* GPT-2’s training data is not open source. OpenWebText is a replication (links with &gt;3 karma on Reddit)
* OPT’s training data is not open source, and is a mess of different things that is hard to replicate. I default
to the Pile, which covers some of it, but imperfectly.</p>
<p>(Some models will have actually been trained on the data supplied here, for some it’s from the validation set)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.loss_fn">
<span class="sig-name descname"><span class="pre">loss_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.loss_fn" title="Permalink to this definition">#</a></dt>
<dd><p>Wrapper around utils.lm_cross_entropy_loss, used in forward() with return_type==”loss” or “both”.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.move_model_modules_to_device">
<span class="sig-name descname"><span class="pre">move_model_modules_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.move_model_modules_to_device" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.process_weights_">
<span class="sig-name descname"><span class="pre">process_weights_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_writing_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_unembed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refactor_factored_attn_matrices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.process_weights_" title="Permalink to this definition">#</a></dt>
<dd><p>Wrapper around load_and_process_state_dict to allow for in-place processing of the weights. This is useful if
using HookedTransformer for training, if we then want to analyse a cleaner version of the same model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.refactor_factored_attn_matrices">
<span class="sig-name descname"><span class="pre">refactor_factored_attn_matrices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.refactor_factored_attn_matrices" title="Permalink to this definition">#</a></dt>
<dd><p>Experimental method for managing queries, keys and values. As argued in [A Mathematical Framework for Transformer
Circuits](<a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">https://transformer-circuits.pub/2021/framework/index.html</a>), queries, keys and values are somewhat
arbitrary intermediate terms when computing with the low rank factored matrices W_QK = W_Q &#64; W_K.T and W_OV = W_V &#64; W_O,
and these matrices are the only thing determining head behaviour. But there are many ways to find a low rank
factorization to a given matrix, and hopefully some of these are more interpretable than others! This method is
one attempt, which makes all of the matrices have orthogonal rows or columns, W_O into a rotation and W_Q and W_K
having the nth column in each having the same norm. The formula is $W_V = U &#64; S,W_O=Vh.T,W_Q=U&#64;S.sqrt(),W_K=Vh&#64;S.sqrt()$.</p>
<p>More details:</p>
<p>If W_OV = U &#64; S &#64; Vh.T in its singular value decomposition, (where S is in R^d_head not R^d_model, as W_OV is low rank),
W_OV = (U &#64; S) &#64; (Vh.T) is an equivalent low rank factorisation, where rows/columns of each matrix are orthogonal!
So setting $W_V=US$ and $W_O=Vh.T$ works just as well. I <em>think</em> this is a more interpretable setup, because now
$W_O$ is just a rotation, and doesn’t change the norm, so $z$ has the same norm as the result of the head.</p>
<p>For $W_QK = W_Q &#64; W_K.T$ we use the refactor $W_Q = U &#64; S.sqrt()$ and $W_K = Vh &#64; S.sqrt()$, which is also
equivalent ($S==S.sqrt() &#64; S.sqrt()$ as $S$ is diagonal). Here we keep the matrices as having the same norm,
since there’s not an obvious asymmetry between the keys and queries.</p>
<p>Biases are more fiddly to deal with. For OV it’s pretty easy - we just need (x &#64; W_V + b_V) &#64; W_O + b_O to be
preserved, so we can set b_V’ = 0. and b_O’ = b_V &#64; W_O + b_O (note that b_V in R^{head_index x d_head} while b_O in R^{d_model},
so we need to sum b_V &#64; W_O along the head_index dimension too).</p>
<p>For QK it’s messy - we need to preserve the bilinear form of (x &#64; W_Q +
b_Q) * (y &#64; W_K + b_K), which is fairly messy. To deal with the biases,
we concatenate them to W_Q and W_K to simulate a d_model+1 dimensional
input (whose final coordinate is always 1), do the SVD factorization on
this effective matrix, then separate out into final weights and biases</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.run_with_cache">
<span class="sig-name descname"><span class="pre">run_with_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_cache_object</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">True</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.HookedTransformer.Output" title="transformer_lens.HookedTransformer.Output"><span class="pre">Output</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.run_with_cache" title="Permalink to this definition">#</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">run_with_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_cache_object</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">False</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.HookedTransformer.Output" title="transformer_lens.HookedTransformer.Output"><span class="pre">Output</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dd><p>Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an
ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a
dictionary of activations as in HookedRootModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.sample_datapoint">
<span class="sig-name descname"><span class="pre">sample_datapoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.sample_datapoint" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to randomly sample a data point from self.dataset, a small dataset from the data distribution
the model was trained on.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenize</strong> (<em>bool</em>) – Whether to return tokens (instead of text). Defaults to False. Note that the returned tokens</p></li>
<li><p><strong>size.</strong> (<em>will be automatically truncated to the model's max context</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Implicitly calls self.load_sample_training_dataset if it hasn’t already been called. Only works for pretrained
models with an associated dataset. But you can manually replace self.dataset with a dataset of your choice if you want.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_tokenizer">
<span class="sig-name descname"><span class="pre">set_tokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_tokenizer" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the tokenizer to use for this model.
tokenizer (PreTrainedTokenizer): a pretrained HuggingFace tokenizer</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_in">
<span class="sig-name descname"><span class="pre">set_use_attn_in</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_attn_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_in" title="Permalink to this definition">#</a></dt>
<dd><p>Toggles whether to allow editing of inputs to each attention head.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_result">
<span class="sig-name descname"><span class="pre">set_use_attn_result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_attn_result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_result" title="Permalink to this definition">#</a></dt>
<dd><p>Toggles whether to explicitly calculate and expose the result for each attention head - useful for
interpretability but can easily burn through GPU memory.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_use_hook_mlp_in">
<span class="sig-name descname"><span class="pre">set_use_hook_mlp_in</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_hook_mlp_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_hook_mlp_in" title="Permalink to this definition">#</a></dt>
<dd><p>Toggles whether to allow storing and editing inputs to each MLP layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_input">
<span class="sig-name descname"><span class="pre">set_use_split_qkv_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_split_qkv_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_input" title="Permalink to this definition">#</a></dt>
<dd><p>Toggles whether to allow editing of inputs to each attention head.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_normalized_input">
<span class="sig-name descname"><span class="pre">set_use_split_qkv_normalized_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_split_qkv_normalized_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_normalized_input" title="Permalink to this definition">#</a></dt>
<dd><p>Toggles whether to allow editing of inputs to each attention head AFTER LAYER NORM.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_or_dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dtype</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_details</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to" title="Permalink to this definition">#</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.channels_last</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point or complex <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>s. In addition, this method will
only cast the floating point or complex parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point or complex dtype of
the parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
<li><p><strong>memory_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>) – the desired memory
format for 4D parameters and buffers in this module (keyword
only argument)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="go">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">))</span>
<span class="go">tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to_single_str_token">
<span class="sig-name descname"><span class="pre">to_single_str_token</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">int_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to_single_str_token" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to_single_token">
<span class="sig-name descname"><span class="pre">to_single_token</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">string</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to_single_token" title="Permalink to this definition">#</a></dt>
<dd><p>Maps a string that makes up a single token to the id for that token. Raises an error for strings that are
not a single token! If uncertain use to_tokens</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to_str_tokens">
<span class="sig-name descname"><span class="pre">to_str_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to_str_tokens" title="Permalink to this definition">#</a></dt>
<dd><p>Method to map text, a list of text or tokens to a list of tokens as strings</p>
<p>Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when inputting a prompt
to the model as the first token is often treated weirdly, but should only be done at the START of the prompt.
Make sure to turn it off if you’re looking at the tokenization of part of the prompt!
(Note: some models eg GPT-2 were not trained with a BOS token, others (OPT and my models) were)</p>
<p>Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether the first letter is
capitalized. It’s easy to shoot yourself in the foot here if you’re not careful!</p>
<p>Gotcha3: If passing a string that exceeds the model’s context length (model.cfg.n_ctx), it will be truncated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>list</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The input - either a string or a tensor of tokens. If tokens, should</p></li>
<li><p><strong>[</strong><strong>1</strong> (<em>be a tensor</em><em> of </em><em>shape</em><em> [</em><em>pos</em><em>] </em><em>or</em>) – </p></li>
<li><p><strong>pos</strong><strong>]</strong> – </p></li>
<li><p><strong>prepend_bos</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to prepend a BOS token. Only applies if input is a string. Defaults to</p></li>
<li><p><strong>True.</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of individual tokens as strings</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str_tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to_string">
<span class="sig-name descname"><span class="pre">to_string</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to_string" title="Permalink to this definition">#</a></dt>
<dd><p>Converts a tensor of tokens to a string (if rank 1) or a list of strings (if rank 2).</p>
<p>Accepts lists of tokens and numpy arrays as inputs too (and converts to tensors internally)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.to_tokens">
<span class="sig-name descname"><span class="pre">to_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.to_tokens" title="Permalink to this definition">#</a></dt>
<dd><p>Converts a string to a tensor of tokens. If prepend_bos is True, prepends the BOS token to the input - this is
recommended when creating a sequence of tokens to be input to a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – </p></li>
<li><p><strong>prepend_bos</strong> (<em>bool</em>) – Whether to prepend a beginning of sequence token. Defaults to True</p></li>
<li><p><strong>move_to_device</strong> (<em>bool</em>) – Whether to move the output tensor of tokens to the device the model lives on.</p></li>
<li><p><strong>True</strong> (<em>Defaults to</em>) – </p></li>
<li><p><strong>truncate</strong> (<em>bool</em>) – If the output tokens are too long, whether to truncate the output tokens to the model’s</p></li>
<li><p><strong>True.</strong> (<em>max context window. Does nothing for shorter inputs. Defaults to</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when inputting a prompt
to the model as the first token is often treated weirdly, but should only be done at the START of the prompt.
Make sure to turn it off if you’re looking at the tokenization of part of the prompt!
(Note: some models eg GPT-2 were not trained with a BOS token, others (OPT and my models) were)</p>
<p>Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether the first letter is
capitalized. It’s easy to shoot yourself in the foot here if you’re not careful!</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.tokens_to_residual_directions">
<span class="sig-name descname"><span class="pre">tokens_to_residual_directions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.tokens_to_residual_directions" title="Permalink to this definition">#</a></dt>
<dd><p>Maps tokens to a tensor with the unembedding vector for those tokens, ie the vector in the residual stream
that we dot with to the get the logit for that token.</p>
<p>WARNING: If you use this without folding in LayerNorm, the results will be misleading and may be incorrect, as
the LN weights change the unembed map. This is done automatically with the fold_ln flag on from_pretrained</p>
<p>WARNING 2: LayerNorm scaling will scale up or down the effective direction in the residual stream for each
output token on any given input token position. ActivationCache.apply_ln_to_stack will apply the appropriate
scaling to these directions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>, </em><em>torch.Tensor</em><em>]</em>) – The token(s). If a single token, can be a single element tensor, an
integer, or string. If string, will be mapped to a single token using to_single_token, and an error
raised if it’s multiple tokens. The method also works for a batch of input tokens</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The unembedding vector for the token(s), a stack of [d_model] tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>residual_direction torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.HookedTransformer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.HookedTransformer.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.Output">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.HookedTransformer.</span></span><span class="sig-name descname"><span class="pre">Output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformer.Output" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">NamedTuple</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.Output.logits">
<span class="sig-name descname"><span class="pre">logits</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.Output.logits" title="Permalink to this definition">#</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformer.Output.loss">
<span class="sig-name descname"><span class="pre">loss</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos-1'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.HookedTransformer.Output.loss" title="Permalink to this definition">#</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.HookedTransformerConfig">
<span id="transformer-lens-hookedtransformerconfig-module"></span><h2>transformer_lens.HookedTransformerConfig module<a class="headerlink" href="#module-transformer_lens.HookedTransformerConfig" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.HookedTransformerConfig.</span></span><span class="sig-name descname"><span class="pre">HookedTransformerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'custom'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_vocab</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_attn_result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_attn_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_split_qkv_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_split_qkv_normalized_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_hook_mlp_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_attn_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_local_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">original_architecture</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">from_checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_label_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_types</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gpt2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalization_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'causal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_attn_by_inverse_layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_embedding_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'standard'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_rms</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_vocab_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_attn_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rotary_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_hook_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gated_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class to store the configuration of a HookedTransformer model.</p>
<p>See further_comments.md for more details on the more complex arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – The dimensionality of the embeddings.</p></li>
<li><p><strong>d_head</strong> (<em>int</em>) – The dimensionality of each attention head.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em>) – The number of transformer blocks (one block = one attn layer AND one MLP layer).</p></li>
<li><p><strong>n_ctx</strong> (<em>int</em>) – The maximum sequence length.</p></li>
<li><p><strong>n_heads</strong> (<em>int</em>) – The number of attention heads. If not
specified, will be set to d_model // d_head. (This is represented by a default value of -1)</p></li>
<li><p><strong>d_mlp</strong> (int, <em>optional</em>) – The dimensionality of the feedforward mlp
network. Defaults to 4 * d_model, and in an attn-only model is None.</p></li>
<li><p><strong>d_vocab</strong> (<em>int</em>) – The size of the vocabulary. Defaults to -1, which means not set. If not set, will be
automatically set from the tokenizer’s vocab size.</p></li>
<li><p><strong>act_fn</strong> (str, <em>optional</em>) – The activation function to use. Always
lowercase. Supports [‘relu’, ‘gelu’, ‘silu’, ‘gelu_new’, ‘solu_ln’,
‘gelu_fast’]. Must be set unless using an attn-only model.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – The epsilon value to use for layer normalization. Defaults
to 1e-5</p></li>
<li><p><strong>use_attn_result</strong> (<em>bool</em>) – whether to explicitly calculate the amount
each head adds to the residual stream (with a hook) and THEN add it
up, vs just calculating the sum. This can be very memory intensive
for large models, so defaults to False</p></li>
<li><p><strong>use_split_qkv_input</strong> (<em>bool</em>) – whether to explicitly calculate the input of
each head separately, with a hook. Defaults to false to save memory.</p></li>
<li><p><strong>use_split_qkv_normalized_input</strong> (<em>bool</em>) – whether to explicitly calculate
the layer-normalized-input to each head and QKV input separately</p></li>
<li><p><strong>use_hook_mlp_in</strong> (<em>bool</em>) – whether to use a hook to get the input to the
MLP layer. Defaults to false to save memory.</p></li>
<li><p><strong>use_attn_in</strong> (<em>bool</em>) – whether to explicitly calculate the input of each
attention head separately, with a hook. Defaults to false to save memory</p></li>
<li><p><strong>use_attn_scale</strong> (<em>bool</em>) – whether to scale the attention weights by
1/sqrt(d_head)</p></li>
<li><p><strong>model_name</strong> (<em>str</em>) – the name of the model, used to load
weights from HuggingFace or initialized to “custom” if not passed</p></li>
<li><p><strong>original_architecture</strong> (str, <em>optional</em>) – the family of the model, used</p></li>
<li><p><strong>load</strong> (<em>to help</em>) – weights from HuggingFace or initialized to “custom” if not passed</p></li>
<li><p><strong>from_checkpoint</strong> (<em>bool</em>) – Whether the model weights were
loaded from a checkpoint (only applies to pretrained models)</p></li>
<li><p><strong>checkpoint_index</strong> (int, <em>optional</em>) – The index of the
checkpoint loaded (only applies to pretrained models).</p></li>
<li><p><strong>checkpoint_label_type</strong> (str, <em>optional</em>) – Whether
checkpoints are labelled by the number of steps or number of tokens.</p></li>
<li><p><strong>checkpoint_value</strong> (int, <em>optional</em>) – The value of the
checkpoint label (whether of steps or tokens).</p></li>
<li><p><strong>tokenizer_name</strong> (str, <em>optional</em>) – the full name of the model, passed into
HuggingFace to access the tokenizer. Only used when passing in
custom config, if loading from pretrained then this is not needed.</p></li>
<li><p><strong>use_local_attn</strong> (<em>bool</em>) – whether to use local attention - ie each
destination token can only attend to source tokens a certain distance back.</p></li>
<li><p><strong>window_size</strong> (int, <em>optional</em>) – the size of the window for local
attention</p></li>
<li><p><strong>attn_types</strong> (List[str], <em>optional</em>) – the types of attention to use for
local attention</p></li>
<li><p><strong>weight_init_mode</strong> (<em>str</em>) – the initialization mode to use for the
weights. Only relevant for custom models, ignored for pre-trained.
Currently the only supported mode is ‘gpt2’, where biases are
initialized to 0 and weights are standard normals of range
initializer_range.</p></li>
<li><p><strong>normalization_type</strong> (str, <em>optional</em>) – the type of normalization to use.
Options are None (no normalization), ‘LN’ (use LayerNorm, including weights
&amp; biases) and ‘LNPre’ (use LayerNorm, but no weights &amp; biases).
Defaults to LN</p></li>
<li><p><strong>device</strong> (<em>str</em>) – The device to use for the model. Defaults to ‘cuda’ if
available, else ‘cpu’. Must be ‘cuda’ if <cite>n_devices</cite> &gt; 1.</p></li>
<li><p><strong>n_devices</strong> (<em>int</em>) – The number of devices to use for the model. Defaults to 1. Layers are loaded
to support “pipeline parallelism”, where each device is responsible for a subset of the layers.</p></li>
<li><p><strong>attention_dir</strong> (<em>str</em>) – Whether to use causal (aka unidirectional aka GPT-2
style) or bidirectional attention. Options are ‘causal’ and
‘bidirectional’. Defaults to ‘causal’</p></li>
<li><p><strong>attn_only</strong> (<em>bool</em>) – Whether to only use attention layers, no feedforward
layers. Defaults to False</p></li>
<li><p><strong>seed</strong> (int, <em>optional</em>) – The seed to use for the model.
Used to set sources of randomness (Python, PyTorch and
NumPy) and to initialize weights. Defaults to None. We recommend setting a seed, so your experiments are reproducible.</p></li>
<li><p><strong>initializer_range</strong> (<em>float</em>) – The standard deviation of the normal used to
initialise the weights, initialized to 0.8 / sqrt(d_model) .</p></li>
<li><p><strong>init_weights</strong> (<em>bool</em>) – Whether to initialize the weights. Defaults to
True. If False, does not initialize weights.</p></li>
<li><p><strong>scale_attn_by_inverse_layer_idx</strong> (<em>bool</em>) – Whether to scale the attention
weights by 1/(layer_id+1), used by Mistral (Stanford) models for numerical stability when
training in FP16. Defaults to False.</p></li>
<li><p><strong>positional_embedding_type</strong> (<em>str</em>) – The positional embedding used. Options
are ‘standard’ (ie GPT-2 style, absolute, randomly initialized learned positional
embeddings, directly added to the residual stream), ‘rotary’
(described here: <a class="reference external" href="https://blog.eleuther.ai/rotary-embeddings/">https://blog.eleuther.ai/rotary-embeddings/</a> ) and
‘shortformer’ (GPT-2 style absolute &amp; learned, but rather than being
added to the residual stream they’re only added to the inputs to the
keys and the queries (ie key = W_K(res_stream + pos_embed), but
values and MLPs don’t get any positional info)). Sinusoidal are not
currently supported. Defaults to ‘standard’.</p></li>
<li><p><strong>final_rms</strong> (<em>bool</em>) – Whether to replace the final normalization (just
before the unembed) with RMSNorm (ie no centering or bias, just
scaling + weights). Only included because of a dumb bug in my
original SoLU code. Defaults to False.</p></li>
<li><p><strong>d_vocab_out</strong> (int, <em>optional</em>) – The size of the output vocabulary. Defaults to -1, which means not set. If not
set, will be equal to d_vocab. Mainly useful for algorithmic tasks
where the input and output vocabularies may be different.</p></li>
<li><p><strong>parallel_attn_mlp</strong> (<em>bool</em>) – Whether to parallelize the attention and MLP
layers - a weird cursed thing done by GPT-J. Means that
mlp_out=MLP(ln1(resid_pre)) and resid_post=resid_pre+attn_out+mlp_out. Defaults to False.</p></li>
<li><p><strong>rotary_dim</strong> (int, <em>optional</em>) – The dimensionality of the rotary
embeddings, may be d_head in which case only the first rotary_dim
dimensions of each head are rotated. Defaults to None, if
positional_embedding_type==”rotary” it defaults to d_head.</p></li>
<li><p><strong>n_params</strong> (int, <em>optional</em>) – The number of (hidden weight)
parameters in the model. This is automatically calculated and not
intended to be set by the user. (Non embedding parameters, because
the [scaling laws paper](<a class="reference external" href="https://arxiv.org/pdf/2001.08361.pdf">https://arxiv.org/pdf/2001.08361.pdf</a>) found
that that was a more meaningful number. Ignoring biases and layer
norms, for convenience)</p></li>
<li><p><strong>use_hook_tokens</strong> (<em>bool</em>) – Will add a hook point on the token input to
HookedTransformer.forward, which lets you cache or intervene on the tokens.
Defaults to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.act_fn">
<span class="sig-name descname"><span class="pre">act_fn</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.act_fn" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attention_dir">
<span class="sig-name descname"><span class="pre">attention_dir</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'causal'</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attention_dir" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_only">
<span class="sig-name descname"><span class="pre">attn_only</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_only" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_types">
<span class="sig-name descname"><span class="pre">attn_types</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_types" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_index">
<span class="sig-name descname"><span class="pre">checkpoint_index</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_index" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_label_type">
<span class="sig-name descname"><span class="pre">checkpoint_label_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_label_type" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_value">
<span class="sig-name descname"><span class="pre">checkpoint_value</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_value" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_head">
<span class="sig-name descname"><span class="pre">d_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_head" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_mlp">
<span class="sig-name descname"><span class="pre">d_mlp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_mlp" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_model">
<span class="sig-name descname"><span class="pre">d_model</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_model" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab">
<span class="sig-name descname"><span class="pre">d_vocab</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab_out">
<span class="sig-name descname"><span class="pre">d_vocab_out</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab_out" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.device">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.device" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1e-05</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.eps" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.final_rms">
<span class="sig-name descname"><span class="pre">final_rms</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.final_rms" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_checkpoint">
<span class="sig-name descname"><span class="pre">from_checkpoint</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_checkpoint" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_dict">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a></span></span><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_dict" title="Permalink to this definition">#</a></dt>
<dd><p>Instantiates a <cite>HookedTransformerConfig</cite> from a Python dictionary of
parameters.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.gated_mlp">
<span class="sig-name descname"><span class="pre">gated_mlp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.gated_mlp" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_mode">
<span class="sig-name descname"><span class="pre">init_mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'gpt2'</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_weights">
<span class="sig-name descname"><span class="pre">init_weights</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_weights" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.initializer_range">
<span class="sig-name descname"><span class="pre">initializer_range</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1.0</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.initializer_range" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.model_name">
<span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'custom'</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.model_name" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_ctx">
<span class="sig-name descname"><span class="pre">n_ctx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_ctx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_devices">
<span class="sig-name descname"><span class="pre">n_devices</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_devices" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_heads">
<span class="sig-name descname"><span class="pre">n_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">-1</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_heads" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_layers">
<span class="sig-name descname"><span class="pre">n_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_layers" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_params">
<span class="sig-name descname"><span class="pre">n_params</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_params" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.normalization_type">
<span class="sig-name descname"><span class="pre">normalization_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'LN'</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.normalization_type" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.original_architecture">
<span class="sig-name descname"><span class="pre">original_architecture</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.original_architecture" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.parallel_attn_mlp">
<span class="sig-name descname"><span class="pre">parallel_attn_mlp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.parallel_attn_mlp" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.positional_embedding_type">
<span class="sig-name descname"><span class="pre">positional_embedding_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'standard'</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.positional_embedding_type" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.rotary_dim">
<span class="sig-name descname"><span class="pre">rotary_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.rotary_dim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.scale_attn_by_inverse_layer_idx">
<span class="sig-name descname"><span class="pre">scale_attn_by_inverse_layer_idx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.scale_attn_by_inverse_layer_idx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.seed">
<span class="sig-name descname"><span class="pre">seed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.seed" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.set_seed_everywhere">
<span class="sig-name descname"><span class="pre">set_seed_everywhere</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.set_seed_everywhere" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.to_dict" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.tokenizer_name">
<span class="sig-name descname"><span class="pre">tokenizer_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.tokenizer_name" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_in">
<span class="sig-name descname"><span class="pre">use_attn_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_in" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_result">
<span class="sig-name descname"><span class="pre">use_attn_result</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_result" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_scale">
<span class="sig-name descname"><span class="pre">use_attn_scale</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_scale" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_mlp_in">
<span class="sig-name descname"><span class="pre">use_hook_mlp_in</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_mlp_in" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_tokens">
<span class="sig-name descname"><span class="pre">use_hook_tokens</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_tokens" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_local_attn">
<span class="sig-name descname"><span class="pre">use_local_attn</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_local_attn" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_input">
<span class="sig-name descname"><span class="pre">use_split_qkv_input</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_input" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_normalized_input">
<span class="sig-name descname"><span class="pre">use_split_qkv_normalized_input</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_normalized_input" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.HookedTransformerConfig.HookedTransformerConfig.window_size">
<span class="sig-name descname"><span class="pre">window_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.window_size" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.components">
<span id="transformer-lens-components-module"></span><h2>transformer_lens.components module<a class="headerlink" href="#module-transformer_lens.components" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.Attention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">Attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'global'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.Attention" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.OV">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">OV</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></em><a class="headerlink" href="#transformer_lens.components.Attention.OV" title="Permalink to this definition">#</a></dt>
<dd><p>OV-Circuit, as defined in A Mathematical Framework. Because there’s no non-linearity between the value vector and the output of the layer, the output is purely determined by the matrix W_OV = W_V &#64; W_O, and not W_V or W_O individually. (Mathematically, for a single head, output == pattern &#64; residual &#64; W_V &#64; W_O, see the glossary for more)</p>
<p>Done in the order W_V, W_O because the paper uses left-multiplying weight matrices, and TransformerLens uses right-multiplying, sorry!</p>
<p>lru_cache says “compute this the first time a user runs attn.OV, and then cache it”. By not defining this in __init__, this means it’s only computed and only consumes memory for investigations that need it.</p>
<p>Returns a FactoredMatrix, with left matrix W_V [head_index, d_model, d_head] and right matrix W_O [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model]. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the OV circuit of a head k, attn.OV[k] works.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.QK">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">QK</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></em><a class="headerlink" href="#transformer_lens.components.Attention.QK" title="Permalink to this definition">#</a></dt>
<dd><p>QK-Circuit, as defined in A Mathematical Framework. Because there’s no non-linearity in the key-query dot product, the output is purely determined by the matrix W_QK = W_Q.T &#64; W_K, and not W_Q or W_K individually. (Mathematically, for a single head, pattern = destination_residual.T &#64; W_Q.T &#64; W_K &#64; source-residual, see the glossary for more).</p>
<p>Done in the order Q on the left, K on the right, because the pattern has dimensions [destination_pos, source_pos]</p>
<p>lru_cache says “compute this the first time a user runs attn.QK, and then cache it”. By not defining this in __init__, this means it’s only computed and only consumes memory for investigations that need it.</p>
<p>Returns a FactoredMatrix, with left matrix W_Q [head_index, d_model, d_head] and right matrix W_K.T [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model] matrix. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the QK circuit of a head k, attn.QK[k] works.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.apply_causal_mask">
<span class="sig-name descname"><span class="pre">apply_causal_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attn_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">head_index</span> <span class="pre">pos</span> <span class="pre">pos_plus_past_kv_pos_offset'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_pos_offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.Attention.apply_causal_mask" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.apply_rotary">
<span class="sig-name descname"><span class="pre">apply_rotary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_pos_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Attention.apply_rotary" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.calculate_sin_cos_rotary">
<span class="sig-name descname"><span class="pre">calculate_sin_cos_rotary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rotary_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10000</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_ctx</span> <span class="pre">rotary_dim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'n_ctx</span> <span class="pre">rotary_dim'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Attention.calculate_sin_cos_rotary" title="Permalink to this definition">#</a></dt>
<dd><p>Calculate the sine and cosine waves to use in a rotary embedding. See <a class="reference external" href="https://blog.eleuther.ai/rotary-embeddings/">https://blog.eleuther.ai/rotary-embeddings/</a> for details</p>
<p>Note: For some inexplicable reason, in GPT-J each ADJACENT pair of elements in k and q are rotated, in GPT-NeoX the pair of elements at k and k+n//2 are rotated (ie folding the full length in half, and then looking at pairs accordingly). I have absolutely no clue why, it should be completely equivalent.
To resolve this, I’ve coded it to default to the GPT-J mode, but to explicitly check whether it’s GPT-NeoX and then do the GPT-NeoX thing if it is.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache_entry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry"><span class="pre">HookedTransformerKeyValueCacheEntry</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additive_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Attention.forward" title="Permalink to this definition">#</a></dt>
<dd><p>shortformer_pos_embed is only used if self.cfg.positional_embedding_type == “shortformer”, else defaults to None and is irrelevant. See HookedTransformerConfig for more details
past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None
additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.rotary_rotate_qk">
<span class="sig-name descname"><span class="pre">rotary_rotate_qk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_pos_offset</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Attention.rotary_rotate_qk" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.rotate_every_two">
<span class="sig-name descname"><span class="pre">rotate_every_two</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">rotary_dim'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">rotary_dim'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Attention.rotate_every_two" title="Permalink to this definition">#</a></dt>
<dd><p>Rotary helper function, splits x into blocks of size 2 along the final axis and maps [x0, x1] to [-x1, x0]</p>
<p>The final axis of x must have even length.</p>
<p>GPT-NeoX and GPT-J do rotary subtly differently, see calculate_sin_cos_rotary for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.Attention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.Attention.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.BertBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">BertBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.BertBlock" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>BERT Block. Similar to the TransformerBlock, except that the LayerNorms are applied after the attention and MLP, rather than before.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.BertBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">resid_pre</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additive_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.BertBlock.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.BertBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.BertBlock.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.BertEmbed">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">BertEmbed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.BertEmbed" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Custom embedding layer for a BERT-like model. This module computes the sum of the token, positional and token-type embeddings and takes the layer norm of the result.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.BertEmbed.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.BertEmbed.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.BertEmbed.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.BertEmbed.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.BertMLMHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">BertMLMHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.BertMLMHead" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Transforms BERT embeddings into logits. The purpose of this module is to predict masked tokens in a sentence.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.BertMLMHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">resid</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.components.BertMLMHead.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.BertMLMHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.BertMLMHead.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.Embed">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">Embed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.Embed" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Embed.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Embed.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.Embed.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.Embed.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.GatedMLP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">GatedMLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.GatedMLP" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.GatedMLP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.GatedMLP.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.GatedMLP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.GatedMLP.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.LayerNorm" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.LayerNorm.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.LayerNorm.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNormPre">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">LayerNormPre</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.LayerNormPre" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNormPre.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">head_index</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.LayerNormPre.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.LayerNormPre.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.LayerNormPre.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.MLP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">MLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.MLP" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.MLP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.MLP.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.MLP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.MLP.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.PosEmbed">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">PosEmbed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.PosEmbed" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.PosEmbed.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_pos_offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.PosEmbed.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Tokens have shape [batch, pos]
past_kv_pos_offset is the length of tokens in the past_kv_cache (if used, defaults to zero if unused)
Output shape [pos, d_model] - will be broadcast along batch dim</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.PosEmbed.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.PosEmbed.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">RMSNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.RMSNorm" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">length'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">length'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.RMSNorm.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.RMSNorm.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNormPre">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">RMSNormPre</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.RMSNormPre" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNormPre.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">length'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">length'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.RMSNormPre.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.RMSNormPre.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.RMSNormPre.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.TokenTypeEmbed">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">TokenTypeEmbed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.TokenTypeEmbed" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The token-type embed is a binary ids indicating whether a token belongs to sequence A or B. For example, for two sentences: “[CLS] Sentence A [SEP] Sentence B [SEP]”, token_type_ids would be [0, 0, …, 0, 1, …, 1, 1]. <cite>0</cite> represents tokens from Sentence A, <cite>1</cite> from Sentence B. If not provided, BERT assumes a single sequence input. Typically, shape is (batch_size, sequence_length).</p>
<p>See the BERT paper for more information: <a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.TokenTypeEmbed.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.TokenTypeEmbed.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.TokenTypeEmbed.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.TokenTypeEmbed.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.TransformerBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">TransformerBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_index</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.TransformerBlock" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.TransformerBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">resid_pre</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shortformer_pos_embed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_kv_cache_entry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry"><span class="pre">HookedTransformerKeyValueCacheEntry</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.TransformerBlock.forward" title="Permalink to this definition">#</a></dt>
<dd><p>A single Transformer block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>resid_pre</strong> (<em>torch.Tensor</em>) – The residual stream - shape [batch, pos, d_model]</p></li>
<li><p><strong>cache</strong> (<a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><em>HookedTransformerKeyValueCache</em></a>) – A cache of previous keys and values, used only when generating text. Defaults to None.</p></li>
<li><p><strong>shortformer_pos_embed</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Only used for positional_embeddings_type == “shortformer”. The positional embeddings. See HookedTransformerConfig for details. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>_description_</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>_type_</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.TransformerBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.TransformerBlock.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.components.Unembed">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.components.</span></span><span class="sig-name descname"><span class="pre">Unembed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.components.Unembed" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.components.Unembed.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">residual</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_model'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab_out'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.components.Unembed.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.components.Unembed.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.components.Unembed.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.evals">
<span id="transformer-lens-evals-module"></span><h2>transformer_lens.evals module<a class="headerlink" href="#module-transformer_lens.evals" title="Permalink to this heading">#</a></h2>
<p>A file with some rough evals for models - I expect you to be likely better off using the HuggingFace evaluate library if you want to do anything properly, but this is here if you want it and want to eg cheaply and roughly compare models you’ve trained to baselines.</p>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.evals.IOIDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">IOIDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">templates</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nouns</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.IOIDataset" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>Dataset for Indirect Object Identification tasks.
Paper: <a class="reference external" href="https://arxiv.org/pdf/2211.00593.pdf">https://arxiv.org/pdf/2211.00593.pdf</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformer_lens.evals</span> <span class="kn">import</span> <span class="n">ioi_eval</span><span class="p">,</span> <span class="n">IOIDataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformer_lens.HookedTransformer</span> <span class="kn">import</span> <span class="n">HookedTransformer</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2-small&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Eval like this</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ioi_eval</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="go">{&#39;Logit Difference&#39;: 3.655226745605469, &#39;Accuracy&#39;: 1.0}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Can use custom dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ds</span> <span class="o">=</span> <span class="n">IOIDataset</span><span class="p">(</span>
<span class="go">    tokenizer=model.tokenizer,</span>
<span class="go">    num_samples=100,</span>
<span class="go">    templates=[&#39;[A] met with [B]. [B] gave the [OBJECT] to [A]&#39;],</span>
<span class="go">    names=[&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charlie&#39;],</span>
<span class="go">    nouns={&#39;OBJECT&#39;: [&#39;ball&#39;, &#39;book&#39;]},</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ioi_eval</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">ds</span><span class="p">))</span>
<span class="go">{&#39;Logit Difference&#39;: 3.7498160457611083, &#39;Accuracy&#39;: 1.0}</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.evals.IOIDataset.get_default_names">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_default_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.IOIDataset.get_default_names" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.evals.IOIDataset.get_default_nouns">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_default_nouns</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.IOIDataset.get_default_nouns" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.evals.IOIDataset.get_default_templates">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_default_templates</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.IOIDataset.get_default_templates" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.evals.IOIDataset.get_sample">
<span class="sig-name descname"><span class="pre">get_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.evals.IOIDataset.get_sample" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.evaluate">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.evaluate" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.evaluate_on_dataset">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">evaluate_on_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.evaluate_on_dataset" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.induction_loss">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">induction_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subseq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">384</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.induction_loss" title="Permalink to this definition">#</a></dt>
<dd><p>Generates a batch of random sequences repeated twice, and measures model performance on the second half. Tests whether a model has induction heads.</p>
<p>By default, prepends a beginning of string token (prepend_bos flag), which is useful to give models a resting position, and sometimes models were trained with this.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.ioi_eval">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">ioi_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.ioi_eval" title="Permalink to this definition">#</a></dt>
<dd><p>Evaluates the model on the Indirect Object Identification task.</p>
<dl>
<dt>dataset must be a torch Dataset that returns a dict:</dt><dd><dl class="simple">
<dt>{</dt><dd><p>‘prompt’: torch.LongTensor,
‘IO’: torch.LongTensor,
‘S’: torch.LongTensor</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>Returns average logit difference and accuracy.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.make_code_data_loader">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">make_code_data_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.make_code_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Evaluate on the CodeParrot dataset, a dump of Python code.</p>
<p>All models seem to get significantly lower loss here (even non-code trained models like GPT-2),
presumably code is much easier to predict than natural language?</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.make_owt_data_loader">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">make_owt_data_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.make_owt_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Evaluate on OpenWebText an open source replication of the GPT-2 training corpus (Reddit links with &gt;3 karma)</p>
<p>I think the Mistral models were trained on this dataset, so they get very good performance.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.make_pile_data_loader">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">make_pile_data_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.make_pile_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Evaluate on the first 10k texts from The Pile.</p>
<p>The Pile is EleutherAI’s general-purpose english dataset, made of 22 subsets
including academic papers, books, internet content…</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.make_wiki_data_loader">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">make_wiki_data_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.make_wiki_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Evaluate on Wikitext 2, a dump of Wikipedia articles. (Using the train set because it’s larger, I don’t really expect anyone to bother with quarantining the validation set nowadays.)</p>
<p>Note there’s likely to be dataset leakage into training data (though I believe GPT-2 was explicitly trained on non-Wikipedia data)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.evals.sanity_check">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.evals.</span></span><span class="sig-name descname"><span class="pre">sanity_check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.evals.sanity_check" title="Permalink to this definition">#</a></dt>
<dd><p>Very basic eval - just feeds a string into the model (in this case, the first paragraph of Circuits: Zoom In), and returns the loss. It’s a rough and quick sanity check - if the loss is &lt;5 the model is probably OK, if the loss is &gt;7 something’s gone wrong.</p>
<p>Note that this is a very basic eval, and doesn’t really tell you much about the model’s performance.</p>
</dd></dl>

</section>
<section id="module-transformer_lens.head_detector">
<span id="transformer-lens-head-detector-module"></span><h2>transformer_lens.head_detector module<a class="headerlink" href="#module-transformer_lens.head_detector" title="Permalink to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.compute_head_attention_similarity_score">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">compute_head_attention_similarity_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_pattern</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detection_pattern</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_current_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">error_measure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">abs</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">mul</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.compute_head_attention_similarity_score" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the similarity between <cite>attention_pattern</cite> and <cite>detection_pattern</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>attention_pattern</strong> – Lower triangular matrix (Tensor) representing the attention pattern of a particular attention head.</p></li>
<li><p><strong>detection_pattern</strong> – Lower triangular matrix (Tensor) representing the attention pattern we are looking for.</p></li>
<li><p><strong>exclude_bos</strong> – <cite>True</cite> if the beginning-of-sentence (BOS) token should be omitted from comparison. <cite>False</cite> otherwise.</p></li>
<li><p><strong>exclude_bcurrent_token</strong> – <cite>True</cite> if the current token at each position should be omitted from comparison. <cite>False</cite> otherwise.</p></li>
<li><p><strong>error_measure</strong> – “abs” for using absolute values of element-wise differences as the error measure. “mul” for using element-wise multiplication (legacy code).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.detect_head">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">detect_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer" title="transformer_lens.HookedTransformer.HookedTransformer"><span class="pre">HookedTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detection_pattern</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">previous_token_head</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">duplicate_token_head</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">induction_head</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_current_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">error_measure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">abs</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">mul</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mul'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.detect_head" title="Permalink to this definition">#</a></dt>
<dd><p>Searches the model (or a set of specific heads, for circuit analysis) for a particular type of attention head.
This head is specified by a detection pattern, a (sequence_length, sequence_length) tensor representing the attention pattern we expect that type of attention head to show.
The detection pattern can be also passed not as a tensor, but as a name of one of pre-specified types of attention head (see <cite>HeadName</cite> for available patterns), in which case the tensor is computed within the function itself.</p>
<p>There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern.</p>
<p>1. <cite>“mul”</cite> (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.
Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score:
how big fraction of this head’s attention is allocated to these specific query-key pairs?
Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).
2. <cite>“abs”</cite> calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.
The “raw result” ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval,
with 1 being perfect match and -1 perfect mismatch.</p>
<p><strong>Which one should you use?</strong> <cite>“mul”</cite> is likely better for quick or exploratory investigations. For precise examinations where you’re trying to
reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to <cite>“abs”</cite>.</p>
<p>The advantage of <cite>“abs”</cite> is that you can make more precise predictions, and have that measured in the score.
You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer.
The “mul” metric does not allow this, you’ll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.</p>
<blockquote>
<div><p>model: Model being used.
seq: String or list of strings being fed to the model.
head_name: Name of an existing head in HEAD_NAMES we want to check. Must pass either a head_name or a detection_pattern, but not both!
detection_pattern: (sequence_length, sequence_length) Tensor representing what attention pattern corresponds to the head we’re looking for <strong>or</strong> the name of a pre-specified head. Currently available heads are: <cite>[“previous_token_head”, “duplicate_token_head”, “induction_head”]</cite>.
heads: If specific attention heads is given here, all other heads’ score is set to -1. Useful for IOI-style circuit analysis. Heads can be spacified as a list tuples (layer, head) or a dictionary mapping a layer to heads within that layer that we want to analyze.
cache: Include the cache to save time if you want.
exclude_bos: Exclude attention paid to the beginning of sequence token.
exclude_current_token: Exclude attention paid to the current token.
error_measure: <cite>“mul”</cite> for using element-wise multiplication (default). <cite>“abs”</cite> for using absolute values of element-wise differences as the error measure.</p>
</div></blockquote>
<p>A (n_layers, n_heads) Tensor representing the score for each attention head.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformer_lens</span> <span class="kn">import</span> <span class="n">HookedTransformer</span><span class="p">,</span>  <span class="n">utils</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformer_lens.head_detector</span> <span class="kn">import</span> <span class="n">detect_head</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">renderer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">px</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">color_continuous_midpoint</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">color_continuous_scale</span><span class="o">=</span><span class="s2">&quot;RdBu&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="n">xaxis</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">yaxis</span><span class="p">},</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">renderer</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2-small&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;This is a test sequence. This is a test sequence.&quot;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_score</span> <span class="o">=</span> <span class="n">detect_head</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sequence</span><span class="p">,</span> <span class="s2">&quot;previous_token_head&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_score</span><span class="p">,</span> <span class="n">zmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">zmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">xaxis</span><span class="o">=</span><span class="s2">&quot;Head&quot;</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Previous Head Matches&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.get_duplicate_token_head_detection_pattern">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">get_duplicate_token_head_detection_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.get_duplicate_token_head_detection_pattern" title="Permalink to this definition">#</a></dt>
<dd><p>Outputs a detection score for [duplicate token heads](<a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo">https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sequence</strong> – String being fed to the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.get_induction_head_detection_pattern">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">get_induction_head_detection_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.get_induction_head_detection_pattern" title="Permalink to this definition">#</a></dt>
<dd><p>Outputs a detection score for [induction heads](<a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY">https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sequence</strong> – String being fed to the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.get_previous_token_head_detection_pattern">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">get_previous_token_head_detection_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.get_previous_token_head_detection_pattern" title="Permalink to this definition">#</a></dt>
<dd><p>Outputs a detection score for [previous token heads](<a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc">https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tokens</strong> – Tokens being fed to the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.head_detector.get_supported_heads">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.head_detector.</span></span><span class="sig-name descname"><span class="pre">get_supported_heads</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.head_detector.get_supported_heads" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a list of supported heads.</p>
</dd></dl>

</section>
<section id="module-transformer_lens.hook_points">
<span id="transformer-lens-hook-points-module"></span><h2>transformer_lens.hook_points module<a class="headerlink" href="#module-transformer_lens.hook_points" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.hook_points.</span></span><span class="sig-name descname"><span class="pre">HookPoint</span></span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A helper class to access intermediate activations in a PyTorch model (inspired by Garcon).</p>
<p>HookPoint is a dummy module that acts as an identity function by default. By wrapping any
intermediate activation in a HookPoint, it provides a convenient way to add PyTorch hooks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.add_hook">
<span class="sig-name descname"><span class="pre">add_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.add_hook" title="Permalink to this definition">#</a></dt>
<dd><p>Hook format is fn(activation, hook_name)
Change it into PyTorch hook format (this includes input and output,
which are the same for a HookPoint)
If prepend is True, add this hook before all other hooks</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.add_perma_hook">
<span class="sig-name descname"><span class="pre">add_perma_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.add_perma_hook" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.clear_context">
<span class="sig-name descname"><span class="pre">clear_context</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.clear_context" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.layer">
<span class="sig-name descname"><span class="pre">layer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.layer" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.remove_hooks">
<span class="sig-name descname"><span class="pre">remove_hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">including_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.remove_hooks" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookPoint.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.hook_points.HookPoint.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.hook_points.</span></span><span class="sig-name descname"><span class="pre">HookedRootModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A class building on nn.Module to interface nicely with HookPoints
Adds various nice utilities, most notably run_with_hooks to run the model with temporary hooks, and run_with_cache to run the model on some input and return a cache of all activations</p>
<p>WARNING: The main footgun with PyTorch hooking is that hooks are GLOBAL state. If you add a hook to the module, and then run it a bunch of times, the hooks persist. If you debug a broken hook and add the fixed version, the broken one is still there. To solve this, run_with_hooks will remove hooks at the end by default, and I recommend using the API of this and run_with_cache. If you want to add hooks into global state, I recommend being intentional about this, and I recommend using reset_hooks liberally in your code to remove any accidentally remaining global state.</p>
<p>The main time this goes wrong is when you want to use backward hooks (to cache or intervene on gradients). In this case, you need to keep the hooks around as global state until you’ve run loss.backward() (and so need to disable the reset_hooks_end flag on run_with_hooks)</p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.add_caching_hooks">
<span class="sig-name descname"><span class="pre">add_caching_hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">names_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_bwd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_batch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.add_caching_hooks" title="Permalink to this definition">#</a></dt>
<dd><p>Adds hooks to the model to cache activations. Note: It does NOT actually run the model to get activations, that must be done separately.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>names_filter</strong> (<em>NamesFilter</em><em>, </em><em>optional</em>) – Which activations to cache. Can be a list of strings (hook names) or a filter function mapping hook names to booleans. Defaults to lambda name: True.</p></li>
<li><p><strong>incl_bwd</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to also do backwards hooks. Defaults to False.</p></li>
<li><p><strong>device</strong> (<em>_type_</em><em>, </em><em>optional</em>) – The device to store on. Defaults to same device as model.</p></li>
<li><p><strong>remove_batch_dim</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.</p></li>
<li><p><strong>cache</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em><em>, </em><em>optional</em>) – The cache to store activations in, a new dict is created by default. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The cache where activations will be stored.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>cache (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.add_hook">
<span class="sig-name descname"><span class="pre">add_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.add_hook" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.add_perma_hook">
<span class="sig-name descname"><span class="pre">add_perma_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.add_perma_hook" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.cache_all">
<span class="sig-name descname"><span class="pre">cache_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_bwd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_batch_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.cache_all" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.cache_some">
<span class="sig-name descname"><span class="pre">cache_some</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_bwd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_batch_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.cache_some" title="Permalink to this definition">#</a></dt>
<dd><p>Cache a list of hook provided by names, Boolean function on names</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.check_and_add_hook">
<span class="sig-name descname"><span class="pre">check_and_add_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook_point</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook_point_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.check_and_add_hook" title="Permalink to this definition">#</a></dt>
<dd><p>Runs checks on the hook, and then adds it to the hook point</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.check_hooks_to_add">
<span class="sig-name descname"><span class="pre">check_hooks_to_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook_point</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook_point_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fwd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.check_hooks_to_add" title="Permalink to this definition">#</a></dt>
<dd><p>Override this function to add checks on which hooks should be added</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.clear_contexts">
<span class="sig-name descname"><span class="pre">clear_contexts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.clear_contexts" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.get_caching_hooks">
<span class="sig-name descname"><span class="pre">get_caching_hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">names_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_bwd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_batch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.get_caching_hooks" title="Permalink to this definition">#</a></dt>
<dd><p>Creates hooks to cache activations. Note: It does not add the hooks to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>names_filter</strong> (<em>NamesFilter</em><em>, </em><em>optional</em>) – Which activations to cache. Can be a list of strings (hook names) or a filter function mapping hook names to booleans. Defaults to lambda name: True.</p></li>
<li><p><strong>incl_bwd</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to also do backwards hooks. Defaults to False.</p></li>
<li><p><strong>device</strong> (<em>_type_</em><em>, </em><em>optional</em>) – The device to store on. Keeps on the same device as the layer if None.</p></li>
<li><p><strong>remove_batch_dim</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.</p></li>
<li><p><strong>cache</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em><em>, </em><em>optional</em>) – The cache to store activations in, a new dict is created by default. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The cache where activations will be stored.
fwd_hooks (list): The forward hooks.
bwd_hooks (list): The backward hooks. Empty if incl_bwd is False.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>cache (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.hook_points">
<span class="sig-name descname"><span class="pre">hook_points</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.hook_points" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.hooks">
<span class="sig-name descname"><span class="pre">hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_hooks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_hooks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_hooks_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clear_contexts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.hooks" title="Permalink to this definition">#</a></dt>
<dd><p>A context manager for adding temporary hooks to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_hooks</strong> – List[Tuple[name, hook]], where name is either the name of a hook point or a
Boolean function on hook names and hook is the function to add to that hook point.</p></li>
<li><p><strong>bwd_hooks</strong> – Same as fwd_hooks, but for the backward pass.</p></li>
<li><p><strong>reset_hooks_end</strong> (<em>bool</em>) – If True, removes all hooks added by this context manager when the context manager exits.</p></li>
<li><p><strong>clear_contexts</strong> (<em>bool</em>) – If True, clears hook contexts whenever hooks are reset.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">hooks</span><span class="p">(</span><span class="n">fwd_hooks</span><span class="o">=</span><span class="n">my_hooks</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">hooked_loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.remove_all_hook_fns">
<span class="sig-name descname"><span class="pre">remove_all_hook_fns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">direction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'both'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">including_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.remove_all_hook_fns" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.reset_hooks">
<span class="sig-name descname"><span class="pre">reset_hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">clear_contexts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'both'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">including_permanent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.reset_hooks" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.run_with_cache">
<span class="sig-name descname"><span class="pre">run_with_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">names_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_batch_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incl_bwd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_hooks_end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clear_contexts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.run_with_cache" title="Permalink to this definition">#</a></dt>
<dd><p>Runs the model and returns the model output and a Cache object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*model_args</strong> – Positional arguments for the model.</p></li>
<li><p><strong>names_filter</strong> (<em>NamesFilter</em><em>, </em><em>optional</em>) – A filter for which activations to cache. Accepts None, str,
list of str, or a function that takes a string and returns a bool. Defaults to None, which
means cache everything.</p></li>
<li><p><strong>device</strong> (<em>str</em><em> or </em><em>torch.Device</em><em>, </em><em>optional</em>) – The device to cache activations on. Defaults to the
model device. WARNING: Setting a different device than the one used by the model leads to
significant performance degradation.</p></li>
<li><p><strong>remove_batch_dim</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, removes the batch dimension when caching. Only
makes sense with batch_size=1 inputs. Defaults to False.</p></li>
<li><p><strong>incl_bwd</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, calls backward on the model output and caches gradients
as well. Assumes that the model outputs a scalar (e.g., return_type=”loss”). Custom loss
functions are not supported. Defaults to False.</p></li>
<li><p><strong>reset_hooks_end</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, removes all hooks added by this function at the
end of the run. Defaults to True.</p></li>
<li><p><strong>clear_contexts</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, clears hook contexts whenever hooks are reset.
Defaults to False.</p></li>
<li><p><strong>**model_kwargs</strong> – Keyword arguments for the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple containing the model output and a Cache object.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.run_with_hooks">
<span class="sig-name descname"><span class="pre">run_with_hooks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_hooks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_hooks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_hooks_end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clear_contexts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.run_with_hooks" title="Permalink to this definition">#</a></dt>
<dd><p>Runs the model with specified forward and backward hooks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_hooks</strong> (<em>List</em><em>[</em><em>Tuple</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>Callable</em><em>]</em><em>]</em>) – A list of (name, hook), where name is
either the name of a hook point or a boolean function on hook names, and hook is the
function to add to that hook point. Hooks with names that evaluate to True are added
respectively.</p></li>
<li><p><strong>bwd_hooks</strong> (<em>List</em><em>[</em><em>Tuple</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>]</em><em>, </em><em>Callable</em><em>]</em><em>]</em>) – Same as fwd_hooks, but for the
backward pass.</p></li>
<li><p><strong>reset_hooks_end</strong> (<em>bool</em>) – If True, all hooks are removed at the end, including those added
during this run. Default is True.</p></li>
<li><p><strong>clear_contexts</strong> (<em>bool</em>) – If True, clears hook contexts whenever hooks are reset. Default is
False.</p></li>
<li><p><strong>*model_args</strong> – Positional arguments for the model.</p></li>
<li><p><strong>**model_kwargs</strong> – Keyword arguments for the model.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to use backward hooks, set <cite>reset_hooks_end</cite> to False, so the backward hooks
remain active. This function only runs a forward pass.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.setup" title="Permalink to this definition">#</a></dt>
<dd><p>Sets up model.</p>
<p>This function must be called in the model’s <cite>__init__</cite> method AFTER defining all layers. It
adds a parameter to each module containing its name, and builds a dictionary mapping module
names to the module instances. It also initializes a hook dictionary for modules of type
“HookPoint”.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.hook_points.HookedRootModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#transformer_lens.hook_points.HookedRootModule.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.hook_points.LensHandle">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.hook_points.</span></span><span class="sig-name descname"><span class="pre">LensHandle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RemovableHandle</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_permanent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_level</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.hook_points.LensHandle" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A dataclass that holds information about a PyTorch hook.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.hook_points.LensHandle.hook">
<span class="sig-name descname"><span class="pre">hook</span></span><a class="headerlink" href="#transformer_lens.hook_points.LensHandle.hook" title="Permalink to this definition">#</a></dt>
<dd><p>Reference to the hook’s RemovableHandle.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>hooks.RemovableHandle</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.hook_points.LensHandle.is_permanent">
<span class="sig-name descname"><span class="pre">is_permanent</span></span><a class="headerlink" href="#transformer_lens.hook_points.LensHandle.is_permanent" title="Permalink to this definition">#</a></dt>
<dd><p>Indicates if the hook is permanent. Defaults to False.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool, optional</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.hook_points.LensHandle.context_level">
<span class="sig-name descname"><span class="pre">context_level</span></span><a class="headerlink" href="#transformer_lens.hook_points.LensHandle.context_level" title="Permalink to this definition">#</a></dt>
<dd><p>Context level associated with the hooks context
manager for the given hook. Defaults to None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[int], optional</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">context_level</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#id0" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id7">
<span class="sig-name descname"><span class="pre">hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">RemovableHandle</span></em><a class="headerlink" href="#id7" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id8">
<span class="sig-name descname"><span class="pre">is_permanent</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#id8" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.loading_from_pretrained">
<span id="transformer-lens-loading-from-pretrained-module"></span><h2>transformer_lens.loading_from_pretrained module<a class="headerlink" href="#module-transformer_lens.loading_from_pretrained" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">Config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm_eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_vocab</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">50257</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.02</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_head</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_head">
<span class="sig-name descname"><span class="pre">d_head</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">64</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_head" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_mlp">
<span class="sig-name descname"><span class="pre">d_mlp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">3072</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_mlp" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_model">
<span class="sig-name descname"><span class="pre">d_model</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">768</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_model" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.d_vocab">
<span class="sig-name descname"><span class="pre">d_vocab</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">50257</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.d_vocab" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.debug">
<span class="sig-name descname"><span class="pre">debug</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.debug" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.init_range">
<span class="sig-name descname"><span class="pre">init_range</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.02</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.init_range" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.layer_norm_eps">
<span class="sig-name descname"><span class="pre">layer_norm_eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1e-05</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.layer_norm_eps" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_ctx">
<span class="sig-name descname"><span class="pre">n_ctx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1024</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_ctx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_heads">
<span class="sig-name descname"><span class="pre">n_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_heads" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.Config.n_layers">
<span class="sig-name descname"><span class="pre">n_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">12</span></em><a class="headerlink" href="#transformer_lens.loading_from_pretrained.Config.n_layers" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_checkpoint_labels">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_checkpoint_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_checkpoint_labels" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the checkpoint labels for a given model, and the label_type
(step or token). Raises an error for models that are not checkpointed.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_num_params_of_pretrained">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_num_params_of_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_num_params_of_pretrained" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the number of parameters of a pretrained model, used to filter to only run code for sufficiently small models.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.loading_from_pretrained.get_pretrained_model_config">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.loading_from_pretrained.</span></span><span class="sig-name descname"><span class="pre">get_pretrained_model_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fold_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.loading_from_pretrained.get_pretrained_model_config" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the pretrained model config as an HookedTransformerConfig object.</p>
<p>There are two types of pretrained models: HuggingFace models (where
AutoModel and AutoConfig work), and models trained by me (NeelNanda) which
aren’t as integrated with HuggingFace infrastructure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_name</strong> – The name of the model. This can be either the official
HuggingFace model name, or the name of a model trained by me
(NeelNanda).</p></li>
<li><p><strong>checkpoint_index</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a
checkpoint, the index of the checkpoint to load. Defaults to None.</p></li>
<li><p><strong>checkpoint_value</strong> (<em>int</em><em>, </em><em>optional</em>) – If loading from a checkpoint, the</p></li>
<li><p><strong>of</strong> (<em>value</em>) – the checkpoint to load, ie the step or token number (each model has
checkpoints labelled with exactly one of these). Defaults to None.</p></li>
<li><p><strong>fold_ln</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to fold the layer norm into the
subsequent linear layers (see HookedTransformer.fold_layer_norm for
details). Defaults to False.</p></li>
<li><p><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – The device to load the model onto. By
default will load to CUDA if available, else CPU.</p></li>
<li><p><strong>n_devices</strong> (<em>int</em>) – The number of devices to split the model across. Defaults to 1.</p></li>
<li><p><strong>kwargs</strong> – Other optional arguments passed to HuggingFace’s from_pretrained.
Also given to other HuggingFace functions when compatible.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-transformer_lens.make_docs">
<span id="transformer-lens-make-docs-module"></span><h2>transformer_lens.make_docs module<a class="headerlink" href="#module-transformer_lens.make_docs" title="Permalink to this heading">#</a></h2>
</section>
<section id="module-transformer_lens.past_key_value_caching">
<span id="transformer-lens-past-key-value-caching-module"></span><h2>transformer_lens.past_key_value_caching module<a class="headerlink" href="#module-transformer_lens.past_key_value_caching" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.past_key_value_caching.</span></span><span class="sig-name descname"><span class="pre">HookedTransformerKeyValueCache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">entries</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry"><span class="pre">HookedTransformerKeyValueCacheEntry</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A cache for storing past keys and values for the Transformer. This is important for generating text - we can cache a lot of past computation and avoid repeating ourselves!</p>
<p>This cache is a list of HookedTransformerKeyValueCacheEntry objects, one for each layer in the Transformer. Each object stores a [batch, pos_so_far, n_heads, d_head] tensor for both keys and values, and each entry has an append method to add a single new key and value.</p>
<p>Generation is assumed to be done by initializing with some prompt and then continuing iteratively one token at a time. So append only works for adding a single token’s worth of keys and values, and but the cache can be initialized with many.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.entries">
<span class="sig-name descname"><span class="pre">entries</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry" title="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry"><span class="pre">HookedTransformerKeyValueCacheEntry</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.entries" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.init_cache">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">init_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.init_cache" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.past_key_value_caching.</span></span><span class="sig-name descname"><span class="pre">HookedTransformerKeyValueCacheEntry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">jaxtyping.Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos_so_far</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">jaxtyping.Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos_so_far</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.append">
<span class="sig-name descname"><span class="pre">append</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">new_tokens</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">new_tokens</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.append" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.init_cache_entry">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">init_cache_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig" title="transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><span class="pre">HookedTransformerConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.init_cache_entry" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_keys">
<span class="sig-name descname"><span class="pre">past_keys</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos_so_far</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_keys" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_values">
<span class="sig-name descname"><span class="pre">past_values</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos_so_far</span> <span class="pre">n_heads</span> <span class="pre">d_head'</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_values" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-transformer_lens.patching">
<span id="transformer-lens-patching-module"></span><h2>transformer_lens.patching module<a class="headerlink" href="#module-transformer_lens.patching" title="Permalink to this heading">#</a></h2>
<p>A module for patching activations in a transformer model, and measuring the effect of the patch on the output.
This implements the activation patching technique for a range of types of activation.
The structure is to have a single generic_activation_patch function that does everything, and to have a range of specialised functions for specific types of activation.</p>
<p>See this explanation for more <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx">https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx</a>
And check out the Activation Patching in TransformerLens Demo notebook for a demo of how to use this module.</p>
<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.generic_activation_patch">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">generic_activation_patch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer" title="transformer_lens.HookedTransformer.HookedTransformer"><span class="pre">HookedTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">corrupted_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">patching_metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_setter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><span class="pre">ActivationCache</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_axis_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">typing_extensions.Literal</span><span class="p"><span class="pre">[</span></span><span class="pre">layer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pos</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">head_index</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">head</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">src_pos</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dest_pos</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_df</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">DataFrame</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_index_df</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.generic_activation_patch" title="Permalink to this definition">#</a></dt>
<dd><p>A generic function to do activation patching, will be specialised to specific use cases.</p>
<p>Activation patching is about studying the counterfactual effect of a specific activation between a clean run and a corrupted run. The idea is have two inputs, clean and corrupted, which have two different outputs, and differ in some key detail. Eg “The Eiffel Tower is in” vs “The Colosseum is in”. Then to take a cached set of activations from the “clean” run, and a set of corrupted.</p>
<p>Internally, the key function comes from three things: A list of tuples of indices (eg (layer, position, head_index)), a index_to_act_name function which identifies the right activation for each index, a patch_setter function which takes the corrupted activation, the index and the clean cache, and a metric for how well the patched model has recovered.</p>
<p>The indices can either be given explicitly as a pandas dataframe, or by listing the relevant axis names and having them inferred from the tokens and the model config. It is assumed that the first column is always layer.</p>
<p>This function then iterates over every tuple of indices, does the relevant patch, and stores it</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> – The input tokens for the corrupted run</p></li>
<li><p><strong>clean_cache</strong> – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
<li><p><strong>patch_setter</strong> – A function which acts on (corrupted_activation, index, clean_cache) to edit the activation and patch in the relevant chunk of the clean activation</p></li>
<li><p><strong>activation_name</strong> – The name of the activation being patched</p></li>
<li><p><strong>index_axis_names</strong> – The names of the axes to (fully) iterate over, implicitly fills in index_df</p></li>
<li><p><strong>index_df</strong> – The dataframe of indices, columns are axis names and each row is a tuple of indices. Will be inferred from index_axis_names if not given. When this is input, the output will be a flattened tensor with an element per row of index_df</p></li>
<li><p><strong>return_index_df</strong> – A Boolean flag for whether to return the dataframe of indices too</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. By default it has one dimension for each index dimension, via index_df set explicitly it is flattened with one element per row.
index_df <em>optional</em>: The dataframe of indices</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_all_pos_every">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_all_pos_every</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corrupted_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_cache</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'patch_type</span> <span class="pre">layer</span> <span class="pre">head'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_all_pos_every" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to get activation patching results for every head (across all positions) for every act type (output, query, key, value, pattern). Wrapper around each’s patching function, returns a stacked tensor of shape [5, n_layers, n_heads]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [5, n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_by_pos_every">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_by_pos_every</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corrupted_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_cache</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'patch_type</span> <span class="pre">layer</span> <span class="pre">pos</span> <span class="pre">head'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_by_pos_every" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to get activation patching results for every head (by position) for every act type (output, query, key, value, pattern). Wrapper around each’s patching function, returns a stacked tensor of shape [5, n_layers, pos, n_heads]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [5, n_layers, pos, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_k_all_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_k_all_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'k',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_k_all_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the keys of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_k_by_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_k_by_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'k',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_k_by_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the keys of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_out_all_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_out_all_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'z',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_out_all_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the outputs of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_out_by_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_out_by_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'z',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_out_by_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the output of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_pattern_all_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_pattern_all_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_pattern_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'pattern',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head_index'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_all_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the attention pattern of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_pattern_by_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_pattern_by_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_pos_pattern_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'pattern',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head_index',</span> <span class="pre">'dest_pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_by_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the attention pattern of each Attention Head (by destination position). Returns a tensor of shape [n_layers, n_heads, dest_pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads, dest_pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_pattern_dest_src_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_pattern_dest_src_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_dest_src_pos_pattern_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'pattern',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head_index',</span> <span class="pre">'dest_pos',</span> <span class="pre">'src_pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_dest_src_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for each destination, source entry of the attention pattern for each Attention Head. Returns a tensor of shape [n_layers, n_heads, dest_pos, src_pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads, dest_pos, src_pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_q_all_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_q_all_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'q',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_q_all_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the queries of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_q_by_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_q_by_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'q',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_q_by_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the queries of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_v_all_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_v_all_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'v',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_v_all_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the values of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_head_v_by_pos">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_head_v_by_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_head_vector_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'v',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos',</span> <span class="pre">'head'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_head_v_by_pos" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the values of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_attn_out">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_attn_out</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'attn_out',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_attn_out" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the output of each Attention layer (by position). Returns a tensor of shape [n_layers, pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_block_every">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_block_every</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corrupted_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_cache</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'patch_type</span> <span class="pre">layer</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_block_every" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to get activation patching results for the residual stream (at the start of each block), output of each Attention layer and output of each MLP layer. Wrapper around each’s patching function, returns a stacked tensor of shape [3, n_layers, pos]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [3, n_layers, pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_mlp_out">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_mlp_out</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'mlp_out',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_mlp_out" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the output of each MLP layer (by position). Returns a tensor of shape [n_layers, pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_resid_mid">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_resid_mid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'resid_mid',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_resid_mid" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the residual stream (between the attn and MLP layer of each block) (by position). Returns a tensor of shape [n_layers, pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each patch. Has shape [n_layers, pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.get_act_patch_resid_pre">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">get_act_patch_resid_pre</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">HookedTransformer,</span> <span class="pre">corrupted_tokens:</span> <span class="pre">Int[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos'],</span> <span class="pre">clean_cache:</span> <span class="pre">ActivationCache,</span> <span class="pre">patching_metric:</span> <span class="pre">Callable[[Float[torch.Tensor,</span> <span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab']],</span> <span class="pre">Float[torch.Tensor,</span> <span class="pre">'']],</span> <span class="pre">*,</span> <span class="pre">patch_setter:</span> <span class="pre">Callable[[CorruptedActivation,</span> <span class="pre">Sequence[int],</span> <span class="pre">ActivationCache],</span> <span class="pre">PatchedActivation]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">layer_pos_patch_setter&gt;,</span> <span class="pre">activation_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'resid_pre',</span> <span class="pre">index_axis_names:</span> <span class="pre">Optional[Sequence[AxisNames]]</span> <span class="pre">=</span> <span class="pre">('layer',</span> <span class="pre">'pos'),</span> <span class="pre">index_df:</span> <span class="pre">Optional[pd.DataFrame]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">return_index_df:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pd.DataFrame</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.patching.get_act_patch_resid_pre" title="Permalink to this definition">#</a></dt>
<dd><p>Function to get activation patching results for the residual stream (at the start of each block) (by position). Returns a tensor of shape [n_layers, pos]</p>
<p>See generic_activation_patch for a more detailed explanation of activation patching</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The relevant model</p></li>
<li><p><strong>corrupted_tokens</strong> (<em>torch.Tensor</em>) – The input tokens for the corrupted run. Has shape [batch, pos]</p></li>
<li><p><strong>clean_cache</strong> (<a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache" title="transformer_lens.ActivationCache.ActivationCache"><em>ActivationCache</em></a>) – The cached activations from the clean run</p></li>
<li><p><strong>patching_metric</strong> – A function from the model’s output logits to some metric (eg loss, logit diff, etc)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tensor of the patching metric for each resid_pre patch. Has shape [n_layers, pos]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>patched_output (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_head_dest_src_pos_pattern_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_head_dest_src_pos_pattern_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_head_dest_src_pos_pattern_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer,  head_index, dest_pos, src_pos]</p>
<p>Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_head_pattern_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_head_pattern_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_head_pattern_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer,  head_index]</p>
<p>Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_head_pos_pattern_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_head_pos_pattern_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_head_pos_pattern_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer,  head_index, dest_pos]</p>
<p>Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_head_vector_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_head_vector_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_head_vector_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer,  head_index]</p>
<p>Implicitly assumes that the activation axis order is [batch, pos, head_index, …], which is true of all attention head vector activations (q, k, v, z, result) but <em>not</em> of attention patterns.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_pos_head_vector_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_pos_head_vector_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_pos_head_vector_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer, pos, head_index]</p>
<p>Implicitly assumes that the activation axis order is [batch, pos, head_index, …], which is true of all attention head vector activations (q, k, v, z, result) but <em>not</em> of attention patterns.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.patching.layer_pos_patch_setter">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.patching.</span></span><span class="sig-name descname"><span class="pre">layer_pos_patch_setter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corrupted_activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.patching.layer_pos_patch_setter" title="Permalink to this definition">#</a></dt>
<dd><p>Applies the activation patch where index = [layer, pos]</p>
<p>Implicitly assumes that the activation axis order is [batch, pos, …], which is true of everything that is not an attention pattern shaped tensor.</p>
</dd></dl>

</section>
<section id="module-transformer_lens.train">
<span id="transformer-lens-train-module"></span><h2>transformer_lens.train module<a class="headerlink" href="#module-transformer_lens.train" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.train.</span></span><span class="sig-name descname"><span class="pre">HookedTransformerTrainConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_every</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wandb</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wandb_project_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_every</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class to store training hyperparameters for a training run of
an HookedTransformer model.
:param num_epochs: Number of epochs to train for
:type num_epochs: int
:param batch_size: Size of batches to use for training
:type batch_size: int
:param lr: Learning rate to use for training
:type lr: float
:param seed: Random seed to use for training
:type seed: int
:param momentum: Momentum to use for training
:type momentum: float
:param max_grad_norm: Maximum gradient norm to use for
:type max_grad_norm: float, <em>optional</em>
:param weight_decay: Weight decay to use for training
:type weight_decay: float, <em>optional</em>
:param optimizer_name: The name of the optimizer to use
:type optimizer_name: str
:param device: Device to use for training
:type device: str, <em>optional</em>
:param warmup_steps: Number of warmup steps to use for training
:type warmup_steps: int, <em>optional</em>
:param save_every: After how many batches should a checkpoint be saved
:type save_every: int, <em>optional</em>
:param save_dir: Where to save checkpoints
:type save_dir: str, <em>optional</em>
:param : Where to save checkpoints
:type : str, <em>optional</em>
:param wandb: Whether to use Weights and Biases for logging
:type wandb: bool
:param wandb_project: Name of the Weights and Biases project to use
:type wandb_project: str, <em>optional</em>
:param print_every: Print the loss every n steps
:type print_every: int, <em>optional</em>
:param max_steps: Terminate the epoch after this many steps. Used for debugging.
:type max_steps: int, <em>optional</em></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.batch_size" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.device">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.device" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.lr">
<span class="sig-name descname"><span class="pre">lr</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.001</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.lr" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.max_grad_norm">
<span class="sig-name descname"><span class="pre">max_grad_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.max_grad_norm" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.max_steps">
<span class="sig-name descname"><span class="pre">max_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.max_steps" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.momentum">
<span class="sig-name descname"><span class="pre">momentum</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.0</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.momentum" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.num_epochs">
<span class="sig-name descname"><span class="pre">num_epochs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.num_epochs" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.optimizer_name">
<span class="sig-name descname"><span class="pre">optimizer_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'Adam'</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.optimizer_name" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.print_every">
<span class="sig-name descname"><span class="pre">print_every</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">50</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.print_every" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.save_dir">
<span class="sig-name descname"><span class="pre">save_dir</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.save_dir" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.save_every">
<span class="sig-name descname"><span class="pre">save_every</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.save_every" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.seed">
<span class="sig-name descname"><span class="pre">seed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.seed" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.wandb">
<span class="sig-name descname"><span class="pre">wandb</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.wandb" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.wandb_project_name">
<span class="sig-name descname"><span class="pre">wandb_project_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.wandb_project_name" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.warmup_steps">
<span class="sig-name descname"><span class="pre">warmup_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.warmup_steps" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="transformer_lens.train.HookedTransformerTrainConfig.weight_decay">
<span class="sig-name descname"><span class="pre">weight_decay</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#transformer_lens.train.HookedTransformerTrainConfig.weight_decay" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.train.train">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.train.</span></span><span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer" title="transformer_lens.HookedTransformer.HookedTransformer"><span class="pre">HookedTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig" title="transformer_lens.train.HookedTransformerTrainConfig"><span class="pre">HookedTransformerTrainConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer" title="transformer_lens.HookedTransformer.HookedTransformer"><span class="pre">HookedTransformer</span></a></span></span><a class="headerlink" href="#transformer_lens.train.train" title="Permalink to this definition">#</a></dt>
<dd><p>Trains an HookedTransformer model on an autoregressive language modeling task.
:param model: The model to train
:param config: The training configuration
:param dataset: The dataset to train on - this function assumes the dataset is set up for autoregressive language modeling.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The trained model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-transformer_lens.utils">
<span id="transformer-lens-utils-module"></span><h2>transformer_lens.utils module<a class="headerlink" href="#module-transformer_lens.utils" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="transformer_lens.utils.Slice">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">Slice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_slice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.Slice" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>We use a custom slice syntax because Python/Torch’s don’t let us reduce the number of dimensions:</p>
<p>Note that slicing with input_slice=None means do nothing, NOT add an extra dimension (use unsqueeze for that)</p>
<p>There are several modes:
int - just index with that integer (decreases number of dimensions)
slice - Input is a tuple converted to a slice ((k,) means :k, (k, m) means m:k, (k, m, n) means m:k:n)
array - Input is a list or tensor or numpy array, converted to a numpy array, and we take the stack of values at those indices
identity - Input is None, leave it unchanged.</p>
<p>Examples for dim=0:
if input_slice=0, tensor -&gt; tensor[0]
elif input_slice = (1, 5), tensor -&gt; tensor[1:5]
elif input_slice = (1, 5, 2), tensor -&gt; tensor[1:5:2] (ie indexing with [1, 3])
elif input_slice = [1, 4, 5], tensor -&gt; tensor[[1, 4, 5]] (ie changing the first axis to have length 3, and taking the indices 1, 4, 5 out).
elif input_slice is a Tensor, same as list - Tensor is assumed to be a 1D list of indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Class<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>Slice</cite>
An object that represents a slice input. It can be a tuple of integers or a slice object.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.utils.Slice.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#transformer_lens.utils.Slice.apply" title="Permalink to this definition">#</a></dt>
<dd><p>Takes in a tensor and a slice, and applies the slice to the given dimension (supports positive and negative dimension syntax). Returns the sliced tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – The tensor to slice.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – The dimension to slice along. Supports positive and negative dimension syntax.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The sliced tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="transformer_lens.utils.Slice.indices">
<span class="sig-name descname"><span class="pre">indices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int64</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.Slice.indices" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the indices when this slice is applied to an axis of size max_ctx. Returns them as a numpy array, for integer slicing it is eg array([4])</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>max_ctx</strong> (<em>int</em><em>, </em><em>optional</em>) – The size of the axis to slice. Only used if the slice is not an integer.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The indices that this slice will select.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the slice is not an integer and max_ctx is not specified.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="transformer_lens.utils.SliceInput">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">SliceInput</span></span><a class="headerlink" href="#transformer_lens.utils.SliceInput" title="Permalink to this definition">#</a></dt>
<dd><p>An optional type alias for a slice input used in the <cite>ActivationCache</cite> module.</p>
<dl class="simple">
<dt>A <cite>SliceInput</cite> can be one of the following types:</dt><dd><ul class="simple">
<li><p><cite>int</cite>: an integer representing a single position</p></li>
<li><p><cite>Tuple[int, int]</cite>: a tuple of two integers representing a range of positions</p></li>
<li><p><cite>Tuple[int, int, int]</cite>: a tuple of three integers representing a range of positions with a step size</p></li>
<li><p><cite>List[int]</cite>: a list of integers representing multiple positions</p></li>
<li><p><cite>torch.Tensor</cite>: a tensor containing a boolean mask or a list of indices to be selected from the input tensor.</p></li>
</ul>
</dd>
</dl>
<p><cite>SliceInput</cite> is used in the <cite>apply_ln_to_stack</cite> method in the <cite>ActivationCache</cite> module.</p>
<dl class="simple">
<dt><a class="reference internal" href="#transformer_lens.utils.SliceInput" title="transformer_lens.utils.SliceInput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SliceInput</span></code></a></dt><dd><p>An object that represents a slice input. It can be a tuple of integers or a slice object.</p>
</dd>
</dl>
<p>alias of <code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]]</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.composition_scores">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">composition_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">left</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">right</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix" title="transformer_lens.FactoredMatrix.FactoredMatrix"><span class="pre">FactoredMatrix</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'*leading_dims_left</span> <span class="pre">*T.leading_dims_right'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.composition_scores" title="Permalink to this definition">#</a></dt>
<dd><p>See <cite>HookedTransformer.all_composition_scores</cite> for documentation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.download_file_from_hf">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">download_file_from_hf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subfolder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'/home/runner/.cache/huggingface/hub'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_is_torch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.download_file_from_hf" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to download files from the HuggingFace Hub, from subfolder/file_name in repo_name, saving locally to cache_dir and returning the loaded file (if a json or Torch object) and the file path otherwise.</p>
<p>If it’s a Torch file without the “.pth” extension, set force_is_torch=True to load it as a Torch object.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.gelu_fast">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">gelu_fast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.gelu_fast" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.gelu_new">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">gelu_new</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.gelu_new" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.get_act_name">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">get_act_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.get_act_name" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to convert shorthand to an activation name. Pretty hacky, intended to be useful for short feedback
loop hacking stuff together, more so than writing good, readable code. But it is deterministic!</p>
<p>Returns a name corresponding to an activation point in a TransformerLens model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – Takes in the name of the activation. This can be used to specify any activation name by itself.</p></li>
<li><p><strong>it</strong> (<em>The code assumes the first sequence</em><em> of </em><em>digits passed to</em>) – </p></li>
<li><p><strong>type.</strong> (<em>that is the layer</em>) – </p></li>
<li><p><strong>number</strong> (<em>Given only a word and</em>) – </p></li>
<li><p><strong>is.</strong> (<em>it leaves layer and layer_type as</em>) – </p></li>
<li><p><strong>word</strong> (<em>Given only a</em>) – </p></li>
<li><p><strong>is.</strong> – </p></li>
<li><p><strong>Examples</strong> – get_act_name(‘embed’) = get_act_name(‘embed’, None, None)
get_act_name(‘k6’) = get_act_name(‘k’, 6, None)
get_act_name(‘scale4ln1’) = get_act_name(‘scale’, 4, ‘ln1’)</p></li>
<li><p><strong>layer</strong> (<em>int</em><em>, </em><em>optional</em>) – Takes in the layer number. Used for activations that appear in every block.</p></li>
<li><p><strong>layer_type</strong> (<em>string</em><em>, </em><em>optional</em>) – Used to distinguish between activations that appear multiple times in one block.</p></li>
</ul>
</dd>
</dl>
<p>Full Examples:</p>
<p>get_act_name(‘k’, 6, ‘a’)==’blocks.6.attn.hook_k’
get_act_name(‘pre’, 2)==’blocks.2.mlp.hook_pre’
get_act_name(‘embed’)==’hook_embed’
get_act_name(‘normalized’, 27, ‘ln2’)==’blocks.27.ln2.hook_normalized’
get_act_name(‘k6’)==’blocks.6.attn.hook_k’
get_act_name(‘scale4ln1’)==’blocks.4.ln1.hook_scale’
get_act_name(‘pre5’)==’blocks.5.mlp.hook_pre’</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.get_corner">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">get_corner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.get_corner" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.get_dataset">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">get_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dataset</span></span></span><a class="headerlink" href="#transformer_lens.utils.get_dataset" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a small HuggingFace dataset, for easy testing and exploration. Accesses several convenience datasets with 10,000 elements (dealing with the enormous 100GB - 2TB datasets is a lot of effort!). Note that it returns a dataset (ie a dictionary containing all the data), <em>not</em> a DataLoader (iterator over the data + some fancy features). But you can easily convert it to a DataLoader.</p>
<p>Each dataset has a ‘text’ field, which contains the relevant info, some also have several meta data fields</p>
<p>Kwargs will be passed to the huggingface dataset loading function, e.g. “data_dir”</p>
<p>Possible inputs:
* openwebtext (approx the GPT-2 training data <a class="reference external" href="https://huggingface.co/datasets/openwebtext">https://huggingface.co/datasets/openwebtext</a>)
* pile (The Pile, a big mess of tons of diverse data <a class="reference external" href="https://pile.eleuther.ai/">https://pile.eleuther.ai/</a>)
* c4 (Colossal, Cleaned, Common Crawl - basically openwebtext but bigger <a class="reference external" href="https://huggingface.co/datasets/c4">https://huggingface.co/datasets/c4</a>)
* code (Codeparrot Clean, a Python code dataset <a class="reference external" href="https://huggingface.co/datasets/codeparrot/codeparrot-clean">https://huggingface.co/datasets/codeparrot/codeparrot-clean</a> )
* c4_code (c4 + code - the 20K data points from c4-10k and code-10k. This is the mix of datasets used to train my interpretability-friendly models, though note that they are <em>not</em> in the correct ratio! There’s 10K texts for each, but about 22M tokens of code and 5M tokens of C4)
* wiki (Wikipedia, generated from the 20220301.en split of <a class="reference external" href="https://huggingface.co/datasets/wikipedia">https://huggingface.co/datasets/wikipedia</a> )</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.is_lower_triangular">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">is_lower_triangular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#transformer_lens.utils.is_lower_triangular" title="Permalink to this definition">#</a></dt>
<dd><p>Checks if <cite>x</cite> is a lower triangular matrix.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.is_square">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">is_square</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#transformer_lens.utils.is_square" title="Permalink to this definition">#</a></dt>
<dd><p>Checks if <cite>x</cite> is a square matrix.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.keep_single_column">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">keep_single_column</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.keep_single_column" title="Permalink to this definition">#</a></dt>
<dd><p>Acts on a HuggingFace dataset to delete all columns apart from a single column name - useful when we want to tokenize and mix together different strings</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.lm_accuracy">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">lm_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.lm_accuracy" title="Permalink to this definition">#</a></dt>
<dd><p>Cross-Entropy Accuracy for Language Modelling. We measure the accuracy on the logits for predicting the NEXT token.</p>
<p>If per_token is True, returns the boolean for top 1 accuracy for each token in the batch. Note that this has size [batch, seq_len-1], as we cannot predict the first token.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.lm_cross_entropy_loss">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">lm_cross_entropy_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">''</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.lm_cross_entropy_loss" title="Permalink to this definition">#</a></dt>
<dd><p>Cross entropy loss for the language model, gives the loss for predicting the NEXT token.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<em>torch.Tensor</em>) – Logits. Shape [batch, pos, d_vocab]</p></li>
<li><p><strong>tokens</strong> (<em>torch.Tensor</em><em>[</em><em>int64</em><em>]</em>) – Input tokens. Shape [batch, pos]</p></li>
<li><p><strong>per_token</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return the log probs predicted for the correct token, or the loss (ie mean of the predicted log probs). Note that the returned array has shape [batch, seq-1] as we cannot predict the first token (alternately, we ignore the final logit). Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.print_gpu_mem">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">print_gpu_mem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.print_gpu_mem" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.remove_batch_dim">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">remove_batch_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'1</span> <span class="pre">...'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.remove_batch_dim" title="Permalink to this definition">#</a></dt>
<dd><p>Removes the first dimension of a tensor if it is size 1, otherwise returns the tensor unchanged</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.sample_logits">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">sample_logits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">final_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">d_vocab'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Int</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.sample_logits" title="Permalink to this definition">#</a></dt>
<dd><p>Sample from the logits, in order to generate text</p>
<p>final_logits has shape [batch, vocab_size]
We divide the logits by temperature before softmaxing and sampling - high temperature = more uniform, low = more argmaxy. Temp = 0.0 is greedy sampling
We apply top_k and top_p filtering to the logits, to encourage diversity. top_k = 10 means we only sample from the 10 most likely tokens. top_p = 0.9 means we only sample from the top 90% of tokens, and then renormalise the distribution. top_k and top_p are mutually exclusive. By default we apply neither and just sample from the full distribution.</p>
<p>Frequency penalty is a penalty on the probability of a token, proportional to the number of times it has been generated so far. This encourages the model to generate new tokens, rather than repeating itself. It is a hyperparameter, and should be tuned. It is applied to the logits before sampling. If this is non-zero it is required to input the input_tokens</p>
<p>#! TODO: Finish testing all the edge cases here. Useful testing code:
logits = torch.randn(4)
print(logits)
np.unique(np.array([sample_logits(logits, top_k=2).item() for i in range(1000)]), return_counts=True)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.solu">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">solu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'batch</span> <span class="pre">pos</span> <span class="pre">d_mlp'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.solu" title="Permalink to this definition">#</a></dt>
<dd><p>SoLU activation function as described by
<a class="reference external" href="https://transformer-circuits.pub/2022/solu/index.html">https://transformer-circuits.pub/2022/solu/index.html</a>.</p>
<p>LayerNorm implemented by the MLP class.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.test_prompt">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">test_prompt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">answer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_space_to_answer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_details</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.test_prompt" title="Permalink to this definition">#</a></dt>
<dd><p>Function to test whether a model can give the correct answer to a prompt. Intended for exploratory analysis, so it prints things out rather than returning things.</p>
<p>Works for multi-token answers and multi-token prompts.</p>
<p>Will always print the ranks of the answer tokens, and if print_details will print the logit and prob for the answer tokens and the top k tokens returned for each answer position.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.to_numpy">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">to_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformer_lens.utils.to_numpy" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to convert a tensor to a numpy array. Also works on lists, tuples, and numpy arrays.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.tokenize_and_concatenate">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">tokenize_and_concatenate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AutoTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">column_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'text'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_bos_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proc</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dataset</span></span></span><a class="headerlink" href="#transformer_lens.utils.tokenize_and_concatenate" title="Permalink to this definition">#</a></dt>
<dd><p>Helper function to tokenizer and concatenate a dataset of text. This converts the text to tokens, concatenates them (separated by EOS tokens) and then reshapes them into a 2D array of shape (____, sequence_length), dropping the last batch. Tokenizers are much faster if parallelised, so we chop the string into 20, feed it into the tokenizer, in parallel with padding, then remove padding at the end.</p>
<p>This tokenization is useful for training language models, as it allows us to efficiently train on a large corpus of text of varying lengths (without, eg, a lot of truncation or padding). Further, for models with absolute positional encodings, this avoids privileging early tokens (eg, news articles often begin with CNN, and models may learn to use early positional encodings to predict these)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>Dataset</em>) – The dataset to tokenize, assumed to be a HuggingFace text dataset.</p></li>
<li><p><strong>tokenizer</strong> (<em>AutoTokenizer</em>) – The tokenizer. Assumed to have a bos_token_id and an eos_token_id.</p></li>
<li><p><strong>streaming</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – The length of the context window of the sequence. Defaults to 1024.</p></li>
<li><p><strong>column_name</strong> (<em>str</em><em>, </em><em>optional</em>) – The name of the text column in the dataset. Defaults to ‘text’.</p></li>
<li><p><strong>add_bos_token</strong> (<em>bool</em><em>, </em><em>optional</em>) – . Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Returns the tokenized dataset, as a dataset of tensors, with a single column called “tokens”</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dataset</p>
</dd>
</dl>
<p>Note: There is a bug when inputting very small datasets (eg, &lt;1 batch per process) where it just outputs nothing. I’m not super sure why</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="transformer_lens.utils.transpose">
<span class="sig-prename descclassname"><span class="pre">transformer_lens.utils.</span></span><span class="sig-name descname"><span class="pre">transpose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">a</span> <span class="pre">b'</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Float</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'...</span> <span class="pre">b</span> <span class="pre">a'</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#transformer_lens.utils.transpose" title="Permalink to this definition">#</a></dt>
<dd><p>Utility to swap the last two dimensions of a tensor, regardless of the number of leading dimensions</p>
</dd></dl>

</section>
<section id="module-transformer_lens">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-transformer_lens" title="Permalink to this heading">#</a></h2>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="transformer_lens.utilities.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">transformer_lens.utilities package</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="content/citation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Citation</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Neel Nanda
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">transformer_lens package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-transformer_lens.ActivationCache">transformer_lens.ActivationCache module</a><ul>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache"><code class="docutils literal notranslate"><span class="pre">ActivationCache</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.accumulated_resid"><code class="docutils literal notranslate"><span class="pre">ActivationCache.accumulated_resid()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.apply_ln_to_stack"><code class="docutils literal notranslate"><span class="pre">ActivationCache.apply_ln_to_stack()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.apply_slice_to_batch_dim"><code class="docutils literal notranslate"><span class="pre">ActivationCache.apply_slice_to_batch_dim()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.compute_head_results"><code class="docutils literal notranslate"><span class="pre">ActivationCache.compute_head_results()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.decompose_resid"><code class="docutils literal notranslate"><span class="pre">ActivationCache.decompose_resid()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.get_full_resid_decomposition"><code class="docutils literal notranslate"><span class="pre">ActivationCache.get_full_resid_decomposition()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.get_neuron_results"><code class="docutils literal notranslate"><span class="pre">ActivationCache.get_neuron_results()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.items"><code class="docutils literal notranslate"><span class="pre">ActivationCache.items()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.keys"><code class="docutils literal notranslate"><span class="pre">ActivationCache.keys()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.logit_attrs"><code class="docutils literal notranslate"><span class="pre">ActivationCache.logit_attrs()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.remove_batch_dim"><code class="docutils literal notranslate"><span class="pre">ActivationCache.remove_batch_dim()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.stack_activation"><code class="docutils literal notranslate"><span class="pre">ActivationCache.stack_activation()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.stack_head_results"><code class="docutils literal notranslate"><span class="pre">ActivationCache.stack_head_results()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.stack_neuron_results"><code class="docutils literal notranslate"><span class="pre">ActivationCache.stack_neuron_results()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.to"><code class="docutils literal notranslate"><span class="pre">ActivationCache.to()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.toggle_autodiff"><code class="docutils literal notranslate"><span class="pre">ActivationCache.toggle_autodiff()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.ActivationCache.ActivationCache.values"><code class="docutils literal notranslate"><span class="pre">ActivationCache.values()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.FactoredMatrix">transformer_lens.FactoredMatrix module</a><ul>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.AB"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.AB</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.BA"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.BA</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.S"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.S</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.T"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.T</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.U"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.U</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.Vh"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.Vh</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.collapse_l"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.collapse_l()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.collapse_r"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.collapse_r()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.eigenvalues"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.eigenvalues</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.get_corner"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.get_corner()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.make_even"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.make_even()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.ndim"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.ndim</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.norm"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.norm()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.pair"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.pair</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.svd"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.svd()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.FactoredMatrix.FactoredMatrix.unsqueeze"><code class="docutils literal notranslate"><span class="pre">FactoredMatrix.unsqueeze()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.HookedEncoder">transformer_lens.HookedEncoder module</a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder"><code class="docutils literal notranslate"><span class="pre">HookedEncoder</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.OV"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.OV</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.QK"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.QK</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_E"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_E</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_E_pos"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_E_pos</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_K"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_K</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_O"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_O</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_Q"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_Q</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_U"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_U</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_V"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_V</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_in"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_out"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_out</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.W_pos"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.W_pos</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.all_head_labels"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.all_head_labels()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_K"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_K</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_O"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_O</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_Q"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_Q</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_U"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_U</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_V"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_V</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_in"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.b_out"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.b_out</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.cpu"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.cpu()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.cuda"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.cuda()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.forward"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.from_pretrained"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.from_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.run_with_cache"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.run_with_cache()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.to"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.to()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedEncoder.HookedEncoder.training"><code class="docutils literal notranslate"><span class="pre">HookedEncoder.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.HookedTransformer">transformer_lens.HookedTransformer module</a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer"><code class="docutils literal notranslate"><span class="pre">HookedTransformer</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.OV"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.OV</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.QK"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.QK</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_E"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_E</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_E_pos"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_E_pos</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_K"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_K</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_O"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_O</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_Q"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_Q</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_U"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_U</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_V"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_V</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_out"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_out</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.W_pos"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.W_pos</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.accumulated_bias"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.accumulated_bias()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.all_composition_scores"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.all_composition_scores()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.all_head_labels"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.all_head_labels()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_K"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_K</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_O"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_O</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_Q"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_Q</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_U"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_U</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_V"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_V</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.b_out"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.b_out</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.center_unembed"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.center_unembed()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.center_writing_weights"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.center_writing_weights()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.check_hooks_to_add"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.check_hooks_to_add()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.cpu"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.cpu()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.cuda"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.cuda()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.fold_layer_norm"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.fold_layer_norm()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.fold_value_biases"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.fold_value_biases()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.forward"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.from_pretrained"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.from_pretrained_no_processing"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.from_pretrained_no_processing()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.generate"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.generate()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.get_token_position"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.get_token_position()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.init_weights"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.init_weights()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.load_and_process_state_dict"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.load_and_process_state_dict()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.load_sample_training_dataset"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.load_sample_training_dataset()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.loss_fn"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.loss_fn()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.move_model_modules_to_device"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.move_model_modules_to_device()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.process_weights_"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.process_weights_()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.refactor_factored_attn_matrices"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.refactor_factored_attn_matrices()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.run_with_cache"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.run_with_cache()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.sample_datapoint"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.sample_datapoint()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_tokenizer"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_tokenizer()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_use_attn_in()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_attn_result"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_use_attn_result()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_hook_mlp_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_use_hook_mlp_in()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_input"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_use_split_qkv_input()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.set_use_split_qkv_normalized_input"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.set_use_split_qkv_normalized_input()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to_single_str_token"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to_single_str_token()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to_single_token"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to_single_token()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to_str_tokens"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to_str_tokens()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to_string"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to_string()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.to_tokens"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.to_tokens()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.tokens_to_residual_directions"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.tokens_to_residual_directions()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.HookedTransformer.training"><code class="docutils literal notranslate"><span class="pre">HookedTransformer.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.Output"><code class="docutils literal notranslate"><span class="pre">Output</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.Output.logits"><code class="docutils literal notranslate"><span class="pre">Output.logits</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformer.Output.loss"><code class="docutils literal notranslate"><span class="pre">Output.loss</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.HookedTransformerConfig">transformer_lens.HookedTransformerConfig module</a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.act_fn"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.act_fn</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attention_dir"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.attention_dir</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_only"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.attn_only</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.attn_types"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.attn_types</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_index"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.checkpoint_index</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_label_type"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.checkpoint_label_type</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.checkpoint_value"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.checkpoint_value</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_head"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.d_head</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_mlp"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.d_mlp</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_model"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.d_model</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.d_vocab</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.d_vocab_out"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.d_vocab_out</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.device"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.device</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.eps"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.eps</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.final_rms"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.final_rms</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_checkpoint"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.from_checkpoint</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.from_dict"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.from_dict()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.gated_mlp"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.gated_mlp</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_mode"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.init_mode</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.init_weights"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.init_weights</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.initializer_range"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.initializer_range</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.model_name"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.model_name</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_ctx"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.n_ctx</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_devices"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.n_devices</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_heads"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.n_heads</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_layers"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.n_layers</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.n_params"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.n_params</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.normalization_type"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.normalization_type</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.original_architecture"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.original_architecture</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.parallel_attn_mlp"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.parallel_attn_mlp</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.positional_embedding_type"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.positional_embedding_type</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.rotary_dim"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.rotary_dim</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.scale_attn_by_inverse_layer_idx"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.scale_attn_by_inverse_layer_idx</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.seed"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.seed</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.set_seed_everywhere"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.set_seed_everywhere()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.to_dict"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.to_dict()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.tokenizer_name"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.tokenizer_name</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_attn_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_result"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_attn_result</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_attn_scale"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_attn_scale</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_mlp_in"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_hook_mlp_in</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_hook_tokens"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_hook_tokens</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_local_attn"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_local_attn</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_input"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_split_qkv_input</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.use_split_qkv_normalized_input"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.use_split_qkv_normalized_input</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.HookedTransformerConfig.HookedTransformerConfig.window_size"><code class="docutils literal notranslate"><span class="pre">HookedTransformerConfig.window_size</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.components">transformer_lens.components module</a><ul>
<li><a class="reference internal" href="#transformer_lens.components.Attention"><code class="docutils literal notranslate"><span class="pre">Attention</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.Attention.OV"><code class="docutils literal notranslate"><span class="pre">Attention.OV</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.QK"><code class="docutils literal notranslate"><span class="pre">Attention.QK</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.apply_causal_mask"><code class="docutils literal notranslate"><span class="pre">Attention.apply_causal_mask()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.apply_rotary"><code class="docutils literal notranslate"><span class="pre">Attention.apply_rotary()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.calculate_sin_cos_rotary"><code class="docutils literal notranslate"><span class="pre">Attention.calculate_sin_cos_rotary()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.forward"><code class="docutils literal notranslate"><span class="pre">Attention.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.rotary_rotate_qk"><code class="docutils literal notranslate"><span class="pre">Attention.rotary_rotate_qk()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.rotate_every_two"><code class="docutils literal notranslate"><span class="pre">Attention.rotate_every_two()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Attention.training"><code class="docutils literal notranslate"><span class="pre">Attention.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.BertBlock"><code class="docutils literal notranslate"><span class="pre">BertBlock</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.BertBlock.forward"><code class="docutils literal notranslate"><span class="pre">BertBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.BertBlock.training"><code class="docutils literal notranslate"><span class="pre">BertBlock.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.BertEmbed"><code class="docutils literal notranslate"><span class="pre">BertEmbed</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.BertEmbed.forward"><code class="docutils literal notranslate"><span class="pre">BertEmbed.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.BertEmbed.training"><code class="docutils literal notranslate"><span class="pre">BertEmbed.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.BertMLMHead"><code class="docutils literal notranslate"><span class="pre">BertMLMHead</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.BertMLMHead.forward"><code class="docutils literal notranslate"><span class="pre">BertMLMHead.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.BertMLMHead.training"><code class="docutils literal notranslate"><span class="pre">BertMLMHead.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.Embed"><code class="docutils literal notranslate"><span class="pre">Embed</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.Embed.forward"><code class="docutils literal notranslate"><span class="pre">Embed.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Embed.training"><code class="docutils literal notranslate"><span class="pre">Embed.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.GatedMLP"><code class="docutils literal notranslate"><span class="pre">GatedMLP</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.GatedMLP.forward"><code class="docutils literal notranslate"><span class="pre">GatedMLP.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.GatedMLP.training"><code class="docutils literal notranslate"><span class="pre">GatedMLP.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.LayerNorm.forward"><code class="docutils literal notranslate"><span class="pre">LayerNorm.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.LayerNorm.training"><code class="docutils literal notranslate"><span class="pre">LayerNorm.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.LayerNormPre"><code class="docutils literal notranslate"><span class="pre">LayerNormPre</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.LayerNormPre.forward"><code class="docutils literal notranslate"><span class="pre">LayerNormPre.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.LayerNormPre.training"><code class="docutils literal notranslate"><span class="pre">LayerNormPre.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.MLP.forward"><code class="docutils literal notranslate"><span class="pre">MLP.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.MLP.training"><code class="docutils literal notranslate"><span class="pre">MLP.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.PosEmbed"><code class="docutils literal notranslate"><span class="pre">PosEmbed</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.PosEmbed.forward"><code class="docutils literal notranslate"><span class="pre">PosEmbed.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.PosEmbed.training"><code class="docutils literal notranslate"><span class="pre">PosEmbed.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.RMSNorm"><code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.RMSNorm.forward"><code class="docutils literal notranslate"><span class="pre">RMSNorm.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.RMSNorm.training"><code class="docutils literal notranslate"><span class="pre">RMSNorm.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.RMSNormPre"><code class="docutils literal notranslate"><span class="pre">RMSNormPre</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.RMSNormPre.forward"><code class="docutils literal notranslate"><span class="pre">RMSNormPre.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.RMSNormPre.training"><code class="docutils literal notranslate"><span class="pre">RMSNormPre.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.TokenTypeEmbed"><code class="docutils literal notranslate"><span class="pre">TokenTypeEmbed</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.TokenTypeEmbed.forward"><code class="docutils literal notranslate"><span class="pre">TokenTypeEmbed.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.TokenTypeEmbed.training"><code class="docutils literal notranslate"><span class="pre">TokenTypeEmbed.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.TransformerBlock"><code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.TransformerBlock.forward"><code class="docutils literal notranslate"><span class="pre">TransformerBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.TransformerBlock.training"><code class="docutils literal notranslate"><span class="pre">TransformerBlock.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.components.Unembed"><code class="docutils literal notranslate"><span class="pre">Unembed</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.components.Unembed.forward"><code class="docutils literal notranslate"><span class="pre">Unembed.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.components.Unembed.training"><code class="docutils literal notranslate"><span class="pre">Unembed.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.evals">transformer_lens.evals module</a><ul>
<li><a class="reference internal" href="#transformer_lens.evals.IOIDataset"><code class="docutils literal notranslate"><span class="pre">IOIDataset</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.evals.IOIDataset.get_default_names"><code class="docutils literal notranslate"><span class="pre">IOIDataset.get_default_names()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.IOIDataset.get_default_nouns"><code class="docutils literal notranslate"><span class="pre">IOIDataset.get_default_nouns()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.IOIDataset.get_default_templates"><code class="docutils literal notranslate"><span class="pre">IOIDataset.get_default_templates()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.IOIDataset.get_sample"><code class="docutils literal notranslate"><span class="pre">IOIDataset.get_sample()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.evals.evaluate"><code class="docutils literal notranslate"><span class="pre">evaluate()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.evaluate_on_dataset"><code class="docutils literal notranslate"><span class="pre">evaluate_on_dataset()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.induction_loss"><code class="docutils literal notranslate"><span class="pre">induction_loss()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.ioi_eval"><code class="docutils literal notranslate"><span class="pre">ioi_eval()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.make_code_data_loader"><code class="docutils literal notranslate"><span class="pre">make_code_data_loader()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.make_owt_data_loader"><code class="docutils literal notranslate"><span class="pre">make_owt_data_loader()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.make_pile_data_loader"><code class="docutils literal notranslate"><span class="pre">make_pile_data_loader()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.make_wiki_data_loader"><code class="docutils literal notranslate"><span class="pre">make_wiki_data_loader()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.evals.sanity_check"><code class="docutils literal notranslate"><span class="pre">sanity_check()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.head_detector">transformer_lens.head_detector module</a><ul>
<li><a class="reference internal" href="#transformer_lens.head_detector.compute_head_attention_similarity_score"><code class="docutils literal notranslate"><span class="pre">compute_head_attention_similarity_score()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.head_detector.detect_head"><code class="docutils literal notranslate"><span class="pre">detect_head()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.head_detector.get_duplicate_token_head_detection_pattern"><code class="docutils literal notranslate"><span class="pre">get_duplicate_token_head_detection_pattern()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.head_detector.get_induction_head_detection_pattern"><code class="docutils literal notranslate"><span class="pre">get_induction_head_detection_pattern()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.head_detector.get_previous_token_head_detection_pattern"><code class="docutils literal notranslate"><span class="pre">get_previous_token_head_detection_pattern()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.head_detector.get_supported_heads"><code class="docutils literal notranslate"><span class="pre">get_supported_heads()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.hook_points">transformer_lens.hook_points module</a><ul>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint"><code class="docutils literal notranslate"><span class="pre">HookPoint</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.add_hook"><code class="docutils literal notranslate"><span class="pre">HookPoint.add_hook()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.add_perma_hook"><code class="docutils literal notranslate"><span class="pre">HookPoint.add_perma_hook()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.clear_context"><code class="docutils literal notranslate"><span class="pre">HookPoint.clear_context()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.forward"><code class="docutils literal notranslate"><span class="pre">HookPoint.forward()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.layer"><code class="docutils literal notranslate"><span class="pre">HookPoint.layer()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.remove_hooks"><code class="docutils literal notranslate"><span class="pre">HookPoint.remove_hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookPoint.training"><code class="docutils literal notranslate"><span class="pre">HookPoint.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule"><code class="docutils literal notranslate"><span class="pre">HookedRootModule</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.add_caching_hooks"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.add_caching_hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.add_hook"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.add_hook()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.add_perma_hook"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.add_perma_hook()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.cache_all"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.cache_all()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.cache_some"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.cache_some()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.check_and_add_hook"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.check_and_add_hook()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.check_hooks_to_add"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.check_hooks_to_add()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.clear_contexts"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.clear_contexts()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.get_caching_hooks"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.get_caching_hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.hook_points"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.hook_points()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.hooks"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.remove_all_hook_fns"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.remove_all_hook_fns()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.reset_hooks"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.reset_hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.run_with_cache"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.run_with_cache()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.run_with_hooks"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.run_with_hooks()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.setup"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.setup()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.HookedRootModule.training"><code class="docutils literal notranslate"><span class="pre">HookedRootModule.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.hook_points.LensHandle"><code class="docutils literal notranslate"><span class="pre">LensHandle</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.hook_points.LensHandle.hook"><code class="docutils literal notranslate"><span class="pre">LensHandle.hook</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.LensHandle.is_permanent"><code class="docutils literal notranslate"><span class="pre">LensHandle.is_permanent</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.hook_points.LensHandle.context_level"><code class="docutils literal notranslate"><span class="pre">LensHandle.context_level</span></code></a></li>
<li><a class="reference internal" href="#id0"><code class="docutils literal notranslate"><span class="pre">LensHandle.context_level</span></code></a></li>
<li><a class="reference internal" href="#id7"><code class="docutils literal notranslate"><span class="pre">LensHandle.hook</span></code></a></li>
<li><a class="reference internal" href="#id8"><code class="docutils literal notranslate"><span class="pre">LensHandle.is_permanent</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.loading_from_pretrained">transformer_lens.loading_from_pretrained module</a><ul>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config"><code class="docutils literal notranslate"><span class="pre">Config</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_head"><code class="docutils literal notranslate"><span class="pre">Config.d_head</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_mlp"><code class="docutils literal notranslate"><span class="pre">Config.d_mlp</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_model"><code class="docutils literal notranslate"><span class="pre">Config.d_model</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.d_vocab"><code class="docutils literal notranslate"><span class="pre">Config.d_vocab</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.debug"><code class="docutils literal notranslate"><span class="pre">Config.debug</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.init_range"><code class="docutils literal notranslate"><span class="pre">Config.init_range</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.layer_norm_eps"><code class="docutils literal notranslate"><span class="pre">Config.layer_norm_eps</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_ctx"><code class="docutils literal notranslate"><span class="pre">Config.n_ctx</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_heads"><code class="docutils literal notranslate"><span class="pre">Config.n_heads</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.Config.n_layers"><code class="docutils literal notranslate"><span class="pre">Config.n_layers</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_checkpoint_labels"><code class="docutils literal notranslate"><span class="pre">get_checkpoint_labels()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_num_params_of_pretrained"><code class="docutils literal notranslate"><span class="pre">get_num_params_of_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.loading_from_pretrained.get_pretrained_model_config"><code class="docutils literal notranslate"><span class="pre">get_pretrained_model_config()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.make_docs">transformer_lens.make_docs module</a></li>
<li><a class="reference internal" href="#module-transformer_lens.past_key_value_caching">transformer_lens.past_key_value_caching module</a><ul>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCache</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.entries"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCache.entries</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCache.init_cache"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCache.init_cache()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCacheEntry</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.append"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCacheEntry.append()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.init_cache_entry"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCacheEntry.init_cache_entry()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_keys"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCacheEntry.past_keys</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.past_key_value_caching.HookedTransformerKeyValueCacheEntry.past_values"><code class="docutils literal notranslate"><span class="pre">HookedTransformerKeyValueCacheEntry.past_values</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.patching">transformer_lens.patching module</a><ul>
<li><a class="reference internal" href="#transformer_lens.patching.generic_activation_patch"><code class="docutils literal notranslate"><span class="pre">generic_activation_patch()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_all_pos_every"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_all_pos_every()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_by_pos_every"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_by_pos_every()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_k_all_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_k_all_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_k_by_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_k_by_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_out_all_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_out_all_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_out_by_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_out_by_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_all_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_pattern_all_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_by_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_pattern_by_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_pattern_dest_src_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_pattern_dest_src_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_q_all_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_q_all_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_q_by_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_q_by_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_v_all_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_v_all_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_head_v_by_pos"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_head_v_by_pos()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_attn_out"><code class="docutils literal notranslate"><span class="pre">get_act_patch_attn_out()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_block_every"><code class="docutils literal notranslate"><span class="pre">get_act_patch_block_every()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_mlp_out"><code class="docutils literal notranslate"><span class="pre">get_act_patch_mlp_out()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_resid_mid"><code class="docutils literal notranslate"><span class="pre">get_act_patch_resid_mid()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.get_act_patch_resid_pre"><code class="docutils literal notranslate"><span class="pre">get_act_patch_resid_pre()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_head_dest_src_pos_pattern_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_head_dest_src_pos_pattern_patch_setter()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_head_pattern_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_head_pattern_patch_setter()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_head_pos_pattern_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_head_pos_pattern_patch_setter()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_head_vector_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_head_vector_patch_setter()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_pos_head_vector_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_pos_head_vector_patch_setter()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.patching.layer_pos_patch_setter"><code class="docutils literal notranslate"><span class="pre">layer_pos_patch_setter()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.train">transformer_lens.train module</a><ul>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.batch_size"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.batch_size</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.device"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.device</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.lr"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.lr</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.max_grad_norm"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.max_grad_norm</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.max_steps"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.max_steps</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.momentum"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.momentum</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.num_epochs"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.num_epochs</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.optimizer_name"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.optimizer_name</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.print_every"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.print_every</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.save_dir"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.save_dir</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.save_every"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.save_every</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.seed"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.seed</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.wandb"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.wandb</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.wandb_project_name"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.wandb_project_name</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.warmup_steps"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.warmup_steps</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.train.HookedTransformerTrainConfig.weight_decay"><code class="docutils literal notranslate"><span class="pre">HookedTransformerTrainConfig.weight_decay</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.train.train"><code class="docutils literal notranslate"><span class="pre">train()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens.utils">transformer_lens.utils module</a><ul>
<li><a class="reference internal" href="#transformer_lens.utils.Slice"><code class="docutils literal notranslate"><span class="pre">Slice</span></code></a><ul>
<li><a class="reference internal" href="#transformer_lens.utils.Slice.apply"><code class="docutils literal notranslate"><span class="pre">Slice.apply()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.Slice.indices"><code class="docutils literal notranslate"><span class="pre">Slice.indices()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformer_lens.utils.SliceInput"><code class="docutils literal notranslate"><span class="pre">SliceInput</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.composition_scores"><code class="docutils literal notranslate"><span class="pre">composition_scores()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.download_file_from_hf"><code class="docutils literal notranslate"><span class="pre">download_file_from_hf()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.gelu_fast"><code class="docutils literal notranslate"><span class="pre">gelu_fast()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.gelu_new"><code class="docutils literal notranslate"><span class="pre">gelu_new()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.get_act_name"><code class="docutils literal notranslate"><span class="pre">get_act_name()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.get_corner"><code class="docutils literal notranslate"><span class="pre">get_corner()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.get_dataset"><code class="docutils literal notranslate"><span class="pre">get_dataset()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.is_lower_triangular"><code class="docutils literal notranslate"><span class="pre">is_lower_triangular()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.is_square"><code class="docutils literal notranslate"><span class="pre">is_square()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.keep_single_column"><code class="docutils literal notranslate"><span class="pre">keep_single_column()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.lm_accuracy"><code class="docutils literal notranslate"><span class="pre">lm_accuracy()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.lm_cross_entropy_loss"><code class="docutils literal notranslate"><span class="pre">lm_cross_entropy_loss()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.print_gpu_mem"><code class="docutils literal notranslate"><span class="pre">print_gpu_mem()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.remove_batch_dim"><code class="docutils literal notranslate"><span class="pre">remove_batch_dim()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.sample_logits"><code class="docutils literal notranslate"><span class="pre">sample_logits()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.solu"><code class="docutils literal notranslate"><span class="pre">solu()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.test_prompt"><code class="docutils literal notranslate"><span class="pre">test_prompt()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.to_numpy"><code class="docutils literal notranslate"><span class="pre">to_numpy()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.tokenize_and_concatenate"><code class="docutils literal notranslate"><span class="pre">tokenize_and_concatenate()</span></code></a></li>
<li><a class="reference internal" href="#transformer_lens.utils.transpose"><code class="docutils literal notranslate"><span class="pre">transpose()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-transformer_lens">Module contents</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    </body>
</html>